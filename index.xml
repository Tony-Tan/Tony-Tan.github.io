<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Anthony&#39;s Blogs</title>
    <link>https://anthony-tan.com/</link>
    <description>Recent content on Anthony&#39;s Blogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 07 Mar 2020 15:40:46 +0000</lastBuildDate><atom:link href="https://anthony-tan.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Boosting and AdaBoost</title>
      <link>https://anthony-tan.com/Boosting-and-AdaBoost/</link>
      <pubDate>Sat, 07 Mar 2020 15:40:46 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Boosting-and-AdaBoost/</guid>
      <description>Preliminaries Committee  Boosting1 The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:
performances better than any of the base classifiers and each base classifier has a different weight factor  Adaboost Adaboost is short for adaptive boosting.</description>
    </item>
    
    <item>
      <title>Committees</title>
      <link>https://anthony-tan.com/Committees/</link>
      <pubDate>Sat, 07 Mar 2020 13:55:21 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Committees/</guid>
      <description>Preliminaries Basic machine learning concepts Probability Theory concepts   expectation correlated random variable  Analysis of Committees1 The committee is a native inspiration for how to combine several models(or we can say how to combine the outputs of several models). For example, we can combine all the models by:
\[ y_{COM}(X)=\frac{1}{M}\sum_{m=1}^My_m(X)\tag{1} \]
Then we want to find out whether this average prediction of models is better than every one of them.</description>
    </item>
    
    <item>
      <title>Bayesian Model Averaging(BMA) and Combining Models</title>
      <link>https://anthony-tan.com/Bayesian-Model-Averaging-and-Combining-Models/</link>
      <pubDate>Sat, 07 Mar 2020 13:10:39 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Bayesian-Model-Averaging-and-Combining-Models/</guid>
      <description>Preliminaries Bayesian Theorem  Bayesian Model Averaging(BMA)1 Bayesian model averaging(BMA) is another wildly used method that is very like a combining model. However, the difference between BMA and combining models is also significant.
A Bayesian model averaging is a Bayesian formula in which the random variable are models(hypothesizes) \(h=1,2,\cdots,H\) with prior probability \(\Pr(h)\), then the marginal distribution over data \(X\) is:
\[ \Pr(X)=\sum_{h=1}^{H}\Pr(X|h)\Pr(h) \]
And the MBA is used to select a model(hypothesis) that can model the data best through Bayesian theory.</description>
    </item>
    
    <item>
      <title>An Introduction to Combining Models</title>
      <link>https://anthony-tan.com/An-Introduction-to-Combining-Models/</link>
      <pubDate>Sat, 07 Mar 2020 12:04:00 +0000</pubDate>
      
      <guid>https://anthony-tan.com/An-Introduction-to-Combining-Models/</guid>
      <description>Preliminaries ‘Mixtures of Gaussians’ Basic machine learning concepts  Combining Models1 The mixture of Gaussians had been discussed in the post ‘Mixtures of Gaussians’. It was used to introduce the ‘EM algorithm’ but it gave us the inspiration of improving model performance.
All models we have studied, besides neural networks, are all single-distribution models. That is just like that, to solve a problem we invite an expert who is very good at this kind of problem, then we just do whatever the expert said.</description>
    </item>
    
    <item>
      <title>EM Algorithm</title>
      <link>https://anthony-tan.com/EM-Algorithm/</link>
      <pubDate>Thu, 05 Mar 2020 20:04:15 +0000</pubDate>
      
      <guid>https://anthony-tan.com/EM-Algorithm/</guid>
      <description>Preliminaries Gaussian distribution log-likelihood Calculus  partial derivative Lagrange multiplier   EM Algorithm for Gaussian Mixture1 Analysis Maximizing likelihood could not be used in the Gaussian mixture model directly, because of its severe defects which we have come across at ‘Maximum Likelihood of Gaussian Mixtures’. With the inspiration of K-means, a two-step algorithm was developed.
The objective function is the log-likelihood function:
\[ \begin{aligned} \ln \Pr(\mathbf{x}|\mathbf{\pi},\mathbf{\mu},\Sigma)&amp;amp;=\ln (\Pi_{n=1}^N\sum_{j=1}^{K}\pi_k\mathcal{N}(\mathbf{x}|\mathbf{\mu}_k,\Sigma_k))\\ &amp;amp;=\sum_{n=1}^{N}\ln \sum_{j=1}^{K}\pi_j\mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_j,\Sigma_j)\\ \end{aligned}\tag{1} \] ### \(\mu_k\)</description>
    </item>
    
    <item>
      <title>Maximum Likelihood of Gaussian Mixtures</title>
      <link>https://anthony-tan.com/Maximum-Likelihood-of-Gaussian-Mixtures/</link>
      <pubDate>Thu, 05 Mar 2020 18:54:20 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Maximum-Likelihood-of-Gaussian-Mixtures/</guid>
      <description>Preliminaries Probability Theory   multiplication principle joint distribution the Bayesian theory Gaussian distribution log-likelihood function  ‘Maximum Likelihood Estimation’  Maximum Likelihood1 Gaussian mixtures had been discussed in ‘Mixtures of Gaussians’. And once we have a training data set and a certain hypothesis, what we should do next is estimate the parameters of the model. Both kinds of parameters from a mixture of Gaussians \(\Pr(\mathbf{x})= \sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x}|\mathbf{\mu}_k,\Sigma_k)\): - the parameters of Gaussian: \(\mathbf{\mu}_k,\Sigma_k\) - and latent variables: \(\mathbf{z}\)</description>
    </item>
    
    <item>
      <title>Mixtures of Gaussians</title>
      <link>https://anthony-tan.com/Mixtures-of-Gaussians/</link>
      <pubDate>Thu, 05 Mar 2020 16:05:50 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Mixtures-of-Gaussians/</guid>
      <description>Preliminaries Probability Theory   multiplication principle joint distribution the Bayesian theory Gaussian distribution  Calculus 1,2  A Formal Introduction to Mixtures of Gaussians1 We have introduced a mixture distribution in the post ‘An Introduction to Mixture Models’. And the example in that post was just two components Gaussian Mixture. However, in this post, we would like to talk about Gaussian mixtures formally. And it severs to motivate the development of the expectation-maximization(EM) algorithm.</description>
    </item>
    
    <item>
      <title>K-means Clustering</title>
      <link>https://anthony-tan.com/K-means-Clustering/</link>
      <pubDate>Wed, 04 Mar 2020 22:08:03 +0000</pubDate>
      
      <guid>https://anthony-tan.com/K-means-Clustering/</guid>
      <description>Preliminaries Numerical Optimization  necessary conditions for maximum  K-means algorithm Fisher Linear Discriminant  Clustering Problem1 The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.
Clustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete.</description>
    </item>
    
    <item>
      <title>An Introduction to Mixture Models</title>
      <link>https://anthony-tan.com/An-Introduction-to-Mixture-Models/</link>
      <pubDate>Wed, 04 Mar 2020 19:30:08 +0000</pubDate>
      
      <guid>https://anthony-tan.com/An-Introduction-to-Mixture-Models/</guid>
      <description>Preliminaries linear regression Maximum Likelihood Estimation Gaussian Distribution Conditional Distribution  From Supervised to Unsupervised Learning1 We have discussed many machine learning algorithms, including linear regression, linear classification, neural network models, and e.t.c, till now. However, most of them are supervised learning, which means a teacher is leading the models to bias toward a certain task. In these problems our attention was on the probability distribution of parameters given inputs, outputs, and models:</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://anthony-tan.com/Logistic-Regression/</link>
      <pubDate>Thu, 20 Feb 2020 21:02:47 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Logistic-Regression/</guid>
      <description>Preliminaries ‘An Introduction to Probabilistic Generative Models for Linear Classification’  Idea of logistic regression1 Logistic sigmoid function(logistic function for short) had been introduced in post ‘An Introduction to Probabilistic Generative Models for Linear Classification’. It has an elegant form:
\[ \delta(a)=\frac{1}{1+e^{-a}}\tag{1} \]
and when \(a=0\), \(\delta(a)=\frac{1}{2}\) and this is just the half of the range of logistic function. This gives us a strong implication that we can set \(a\) equals to some functions \(y(\mathbf{x})\), and then</description>
    </item>
    
    <item>
      <title>An Introduction to Probabilistic Generative Models</title>
      <link>https://anthony-tan.com/An-Introduction-to-Probabilistic-Generative-Models/</link>
      <pubDate>Thu, 20 Feb 2020 16:13:30 +0000</pubDate>
      
      <guid>https://anthony-tan.com/An-Introduction-to-Probabilistic-Generative-Models/</guid>
      <description>Preliminaries Probability   Bayesian Formular  Calculus  Probabilistic Generative Models1 The generative model used for making decisions contains an inference step and a decision step:
Inference step is to calculate \(\Pr(\mathcal{C}_k|\mathbf{x})\) which means the probability of \(\mathbf{x}\) belonging to the class \(\mathcal{C}_k\) given \(\mathbf{x}\) Decision step is to make a decision based on \(\Pr(\mathcal{C}_k|\mathbf{x})\) which was calculated in step 1  In this post, we just give an introduction and a framework for the probabilistic generative model in classification.</description>
    </item>
    
    <item>
      <title>Fisher Linear Discriminant(LDA)</title>
      <link>https://anthony-tan.com/Fisher-Linear-Discriminant/</link>
      <pubDate>Wed, 19 Feb 2020 17:01:38 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Fisher-Linear-Discriminant/</guid>
      <description>Preliminaries linear algebra   inner multiplication projection  Idea of Fisher linear discriminant1 ‘Least-square method’ in classification can only deal with a small set of tasks. That is because it was designed for the regression task. Then we come to the famous Fisher linear discriminant. This method is also discriminative for it gives directly the class to which the input \(\mathbf{x}\) belongs. Assuming that the linear function
\[ y=\mathbf{w}^T\mathbf{x}+w_0\tag{1} \]</description>
    </item>
    
    <item>
      <title>Discriminant Functions and Decision Boundary</title>
      <link>https://anthony-tan.com/Discriminant-Functions-and-Decision-Boundary/</link>
      <pubDate>Mon, 17 Feb 2020 16:15:28 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Discriminant-Functions-and-Decision-Boundary/</guid>
      <description>Preliminaries convex definition linear algebra   vector length vector direction  Discriminant Function in Classification The discriminant function or discriminant model is on the other side of the generative model. And we, here, have a look at the behavior of the discriminant function in linear classification.1
In the post ‘Least Squares Classification’, we have seen, in a linear classification task, the decision boundary is a line or hyperplane by which we separate two classes.</description>
    </item>
    
    <item>
      <title>Least Squares in Classification</title>
      <link>https://anthony-tan.com/Least-Squares-in-Classification/</link>
      <pubDate>Mon, 17 Feb 2020 12:39:31 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Least-Squares-in-Classification/</guid>
      <description>Preliminaries A Simple Linear Regression Least Squares Estimation From Linear Regression to Linear Classification pseudo-inverse  Least Squares for Classification1 Least-squares for linear regression had been talked about in ‘Simple Linear Regression’. And in this post, we want to find out whether this powerful algorithm can be used in classification.
Recalling the distinction between the properties of classification and regression, two points need to be emphasized again(‘From Linear Regression to Linear Classification’):</description>
    </item>
    
    <item>
      <title>From Linear Regression to Linear Classification</title>
      <link>https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/</link>
      <pubDate>Mon, 17 Feb 2020 11:20:11 +0000</pubDate>
      
      <guid>https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/</guid>
      <description>Preliminaries An Introduction to Linear Regression A Simple Linear Regression Bayesian theorem Feature extraction  Recall Linear Regression The goal of a regression problem is to find out a function or hypothesis that given an input \(\mathbf{x}\), it can make a prediction \(\hat{y}\) to estimate the target. Both the target \(y\) and prediction \(\hat{y}\) here are continuous. They have the properties of numbers1:
 Consider 3 inputs \(\mathbf{x}_1\), \(\mathbf{x}_2\) and \(\mathbf{x}_3\) and their coresponding targets are \(y_1=0\), \(y_2=1\) and \(y_3=2\).</description>
    </item>
    
    <item>
      <title>Polynomial Regression and Features-Extension of Linear Regression</title>
      <link>https://anthony-tan.com/Polynomial-Regression-and-Features-Extension-of-Linear-Regression/</link>
      <pubDate>Sat, 15 Feb 2020 22:00:40 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Polynomial-Regression-and-Features-Extension-of-Linear-Regression/</guid>
      <description>Priliminaries A Simple Linear Regression Least Squares Estimation  Extending Linear Regression with Features1 The original linear regression is in the form:
\[ \begin{aligned} y(\mathbf{x})&amp;amp;= b + \mathbf{w}^T \mathbf{x}\\ &amp;amp;=w_01 + w_1x_1+ w_2x_2+\cdots + w_{m+1}x_{m+1} \end{aligned}\tag{1} \]
where the input vector \(\mathbf{x}\) and parameter \(\mathbf{w}\) are \(m\)-dimension vectors whose first components are \(1\) and bias \(w_0=b\) respectively. This equation is linear for both the input vector and parameter vector. Then an idea come to us, if we set \(x_i=\phi_i(\mathbf{x})\) then equation (1) convert to:</description>
    </item>
    
    <item>
      <title>Maximum Likelihood Estimation</title>
      <link>https://anthony-tan.com/Maximum-Likelihood-Estimation/</link>
      <pubDate>Sat, 15 Feb 2020 00:41:25 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Maximum-Likelihood-Estimation/</guid>
      <description>Priliminaries A Simple Linear Regression Least Squares Estimation linear algebra  Square Loss Function for Regression1 For any input \(\mathbf{x}\), our goal in a regression task is to give a prediction \(\hat{y}=f(\mathbf{x})\) to approximate target \(t\) where the function \(f(\cdot)\) is the chosen hypothesis or model as mentioned in the post https://anthony-tan.com/A-Simple-Linear-Regression/.
The difference between \(t\) and \(\hat{y}\) can be called ‘error’ or more precisely ‘loss’. Because in an approximation task, ‘error’ occurs by chance and always exists, and ‘loss’ is a good word to represent the difference.</description>
    </item>
    
    <item>
      <title>Least Squares Estimation</title>
      <link>https://anthony-tan.com/Least-Squares-Estimation/</link>
      <pubDate>Fri, 14 Feb 2020 11:33:36 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Least-Squares-Estimation/</guid>
      <description>Priliminaries A Simple Linear Regression the column space  Another Example of Linear Regression 1 In the blog A Simple Linear Regression, squares of the difference between the output of a predictor and the target were used as a loss function in a regression problem. And it could be also written as:
\[ \ell(\hat{\mathbf{y}}_i,\mathbf{y}_i)=(\hat{\mathbf{y}}_i-\mathbf{y}_i)^T(\hat{\mathbf{y}}_i-\mathbf{y}_i) \tag{1} \]
The linear regression model in a matrix form is:
\[ y=\mathbf{w}^T\mathbf{x}+\mathbf{b}\tag{2} \]
What we do in this post is analyze the least-squares methods from two different viewpoints</description>
    </item>
    
    <item>
      <title>A Simple Linear Regression</title>
      <link>https://anthony-tan.com/A-Simple-Linear-Regression/</link>
      <pubDate>Fri, 11 Oct 2019 20:35:27 +0000</pubDate>
      
      <guid>https://anthony-tan.com/A-Simple-Linear-Regression/</guid>
      <description>Preliminaries Linear Algebra(the concepts of space, vector) Calculus An Introduction to Linear Regression  Notations of Linear Regression1 We have already created a simple linear model in the post “An Introduction to Linear Regression”. According to the definition of linearity, we can develop the simplest linear regression model:
\[ Y\sim w_1X+w_0\tag{1} \]
where the symbol \(\sim\) is read as “is approximately modeled as”. Equation (1) can also be described as “regressing \(Y\) on \(X\)(or \(Y\) onto \(X\))”.</description>
    </item>
    
    <item>
      <title>An Introduction to Linear Regression</title>
      <link>https://anthony-tan.com/An-Introduction-to-Linear-Regression/</link>
      <pubDate>Wed, 09 Oct 2019 18:36:40 +0000</pubDate>
      
      <guid>https://anthony-tan.com/An-Introduction-to-Linear-Regression/</guid>
      <description>Preliminariess Linear Algebra(the concepts of space, vector) Calculus  What is Linear Regression Linear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors).
Machine Learning and Statistical Learning Machine learning and statistical learning are similar but have some distinctions. In machine learning, models, regression models, or classification models, are used to predict the outputs of the new incoming inputs.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://anthony-tan.com/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://anthony-tan.com/about/</guid>
      <description>About Me Hi, I&amp;rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I&amp;rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie &amp;ldquo;Iron man&amp;rdquo; when I was a middle school student. And to get deeper into these subjects, I&amp;rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years.</description>
    </item>
    
    
    
  </channel>
</rss>
