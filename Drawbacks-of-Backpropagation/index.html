<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Drawbacks of Backpropagation | Anthony's Blogs</title><meta name=keywords content="Artificial Neural Networks,Artificial Intelligence,Backpropagation,BP,Speed Backpropagation up"><meta name=description content="Preliminaries ‘An Introduction to Backpropagation and Multilayer Perceptrons’ ‘The Backpropagation Algorithm’  Speed Backpropagation up 1 BP algorithm has been described in ‘An Introduction to Backpropagation and Multilayer Perceptrons’. And the implementation of the BP algorithm has been recorded at ‘The Backpropagation Algorithm’. BP has worked in many applications for many years, but there are too many drawbacks in the process. The basic BP algorithm is too slow for most practical applications that it might take days or even weeks in training."><meta name=author content="Anthony Tan"><link rel=canonical href=https://anthony-tan.com/Drawbacks-of-Backpropagation/><link crossorigin=anonymous href=../assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://anthony-tan.com/logo.png><link rel=apple-touch-icon href=https://anthony-tan.com/logo.png><link rel=mask-icon href=https://anthony-tan.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-12","auto"),ga("send","pageview"))</script><meta property="og:title" content="Drawbacks of Backpropagation"><meta property="og:description" content="Preliminaries ‘An Introduction to Backpropagation and Multilayer Perceptrons’ ‘The Backpropagation Algorithm’  Speed Backpropagation up 1 BP algorithm has been described in ‘An Introduction to Backpropagation and Multilayer Perceptrons’. And the implementation of the BP algorithm has been recorded at ‘The Backpropagation Algorithm’. BP has worked in many applications for many years, but there are too many drawbacks in the process. The basic BP algorithm is too slow for most practical applications that it might take days or even weeks in training."><meta property="og:type" content="article"><meta property="og:url" content="https://anthony-tan.com/Drawbacks-of-Backpropagation/"><meta property="article:section" content="deep_learning"><meta property="article:published_time" content="2020-01-07T10:14:53+00:00"><meta property="article:modified_time" content="2022-05-03T10:39:43+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Drawbacks of Backpropagation"><meta name=twitter:description content="Preliminaries ‘An Introduction to Backpropagation and Multilayer Perceptrons’ ‘The Backpropagation Algorithm’  Speed Backpropagation up 1 BP algorithm has been described in ‘An Introduction to Backpropagation and Multilayer Perceptrons’. And the implementation of the BP algorithm has been recorded at ‘The Backpropagation Algorithm’. BP has worked in many applications for many years, but there are too many drawbacks in the process. The basic BP algorithm is too slow for most practical applications that it might take days or even weeks in training."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Deep Learning","item":"https://anthony-tan.com/deep_learning/"},{"@type":"ListItem","position":3,"name":"Drawbacks of Backpropagation","item":"https://anthony-tan.com/Drawbacks-of-Backpropagation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Drawbacks of Backpropagation","name":"Drawbacks of Backpropagation","description":"Preliminaries ‘An Introduction to Backpropagation and Multilayer Perceptrons’ ‘The Backpropagation Algorithm’  Speed Backpropagation up 1 BP algorithm has been described in ‘An Introduction to Backpropagation and Multilayer Perceptrons’. And the implementation of the BP algorithm has been recorded at ‘The Backpropagation Algorithm’. BP has worked in many applications for many years, but there are too many drawbacks in the process. The basic BP algorithm is too slow for most practical applications that it might take days or even weeks in training.","keywords":["Artificial Neural Networks","Artificial Intelligence","Backpropagation","BP","Speed Backpropagation up"],"articleBody":"Preliminaries ‘An Introduction to Backpropagation and Multilayer Perceptrons’ ‘The Backpropagation Algorithm’  Speed Backpropagation up 1 BP algorithm has been described in ‘An Introduction to Backpropagation and Multilayer Perceptrons’. And the implementation of the BP algorithm has been recorded at ‘The Backpropagation Algorithm’. BP has worked in many applications for many years, but there are too many drawbacks in the process. The basic BP algorithm is too slow for most practical applications that it might take days or even weeks in training. And the following posts are some investigations to make the BP algorithm more practical and speed it up.\nIn the post ‘Backpropagation, Batch Training, and Incremental Training’, the BP approximation example had shown that the algorithm converged very slowly. BP is a variation of LMS and LMS is a variation of ‘steepest descent’. So BP is a kind of steepest descent, and the difference between them is the calculation of derivatives. Steepest descent is the simplest and the slowest, while Newton and conjugate algorithms are faster. Then inspiration comes to us whether these algorithms can be used in speeding up the convergence of BP.\nResearch on faster algorithms falls on rough two categories and some aspects would be discussed:\nDevelopment of heuristic techniques  varying learning rate momentum rescaling variables  The standard numerical optimization technique  find a numerical optimization technique already exists. ‘Reinvent the wheel’ is a good and bad idea. Conjugate gradient algorithm Levenberg-Marquardt algorithm   Backpropagation is called ‘back’ because in the calculation of the sensitivities in the hidden layer are calculated by its next layer neurons that have connections with it. And the weights and biases updating process of BP is the same as the steepest descent. So we name the standard backpropagation algorithm steepest descent backpropagation and SDBP for short.\nDrawbacks of Backpropagation LMS guarantee to converge that minimize the MSE under the condition that the learning rate \\(\\alpha\\) is small enough. MSE is a quadratic function that has always a single stationary point and constant Hessian. Because Hessian matrices of quadratic functions do not change, so the curvature of functions would not change and their contours are elliptical.\nWhen BP is used in a layered network, it degenerates to the LMS algorithm. The MSE of the single-layer network is quadratic. So it has a single stationary point and constant curvature.\nBut when the network has more than one layer, its MSE of it is no more quadratic. MSE of multiple layers network has many local minimum points and curvature varies widely in different regions of parameter space. Now let go to look at the surface of MSE of multiple layers network. The simplest network 1-2-1 would be our example and its abbreviated notation is:\nthe transfer functions of both the hidden layer and the output layer are log-sigmoid functions. And in the following experiment the function to approximate has the same architecture as the 1-2-1 network and its parameters are:\n\\[ \\begin{aligned} \\{\u0026\\\\ \u0026w^1_{1,1}=10,b^1_{1}=-5,\\\\ \u0026w^1_{1,2}=10,b^1_{2}=5,\\\\ \u0026w^2_{1,1}=1,w^2_{1,2}=1,b^2=-1\\\\ \\}\u0026 \\end{aligned} \\tag{1} \\]\nThen in this task, the global minimum point is at equation(1). And in the interval \\([-2,2]\\) the target function looks like this:\nBecause the target function to be approximated has the same architecture as our model, the 1-2-1 network can have the correct approximation where MSE is 0. Although this experiment is humble compared to practical applications, it can illustrate some important concepts.\nTo approximate the target function, we generate the inputs:\n\\[ \\{-2.0,-1.9,\\cdot,1.9,2.0\\} \\]\nwith equivalent probability and the corresponding outputs. The performance index is the sum of square errors which equals MSE.\nThere are 7 parameters in this simple network model totally. However, we can not observe them all at one picture. We set \\(b^1_{1},w^1_{1,2},b^1_{2},w^2_{1,2},b^2\\) to their optimum values given by equation(1) and leave \\(w^1_{1,1}\\) and \\(w^2_{1,1}\\) as variables. Then we get the contour map:\nThe performance index is not quadratic. Then it has more than one minimum, for instance, \\(w^1_{1,1}=0.88\\) and \\(w^2_{1,1}= 38.6\\), and solution (1) is also a local minimum of the performance index. The curvature varies drastically over the parameter space and a constant learning rate can not suit the entire process. Because some regions are flat where a large learning rate is good, and some regions of curvature are steep where a small learning rate is necessary. The flat region is a common condition when the transfer function of the network is sigmoid. For example, when the inputs are large, the surface of the performance index of the sigmoid network is very flat.\nBecause the 1-2-1 network has a symmetrical architecture, the surface of \\(b^1_1\\) and \\(b^1_2\\) is symmetric as well. and between these two local minimums there must be a saddle point which is at \\(b^1_1=0,b^1_2=0\\):\nSo \\(b^1_1=0\\) and \\(b^1_2=0\\) are not good initial values. And if initial values of the parameters were large, the learning algorithm would start at a very flat region, this is also the nightmare for most algorithms. Trying several initial guesses is also a good idea but when the whole training process needs days or weeks this method is impractical. A common method for initial parameters of networks is using small random numbers as a random number between \\(-1\\) and \\(1\\) with uniform distribution.\nConvergence Example The batching method has been introduced in ‘Backpropagation, Batch Training, and Incremental Training’. It is a generalized method that uses the whole training set of the ‘The Backpropagation Algorithm’, which uses one point of the training set at a time. The following process is based on the batching method.\nNow let’s consider the parameter \\(w^1_{1,1}\\) and the \\(w^2_{1,1}\\) while other parameters are set to optimum solution as in equation(1).\nThe first example is with initial guesses \\(w^1_{1,1}=-4\\) and \\(w^2_{1,1}=-4\\) whose trajectory is labeled as ‘a’:\nIt takes a long time during the flat region and the entire process takes more than 300,000 epochs. And the sum of the square error of the performance index is:\nThe flat region takes a great part of the whole process.\nThe second example is with initial guesses \\(w^1_{1,1}=-4\\) and \\(w^2_{1,1}=10\\) whose trajectory is labeled as ‘b’:\nIt converges to another local minimum point but not the global minimum. And the sum of the squire error of the performance index is:\nwhose final point can not reduce error to 0.\nAs we have mentioned above, in the flat region a bigger learning rate is required. Now let’s start at \\(w^1_{1,1}=-4\\) and \\(w^2_{1,1}=-4\\) as well. But when we increase the learning rate from \\(1\\) to \\(100\\), the algorithm becomes unstable:\nand its error is not decrease after some iterations:\nAfter all the experiments above, we found that flat regions are everywhere then a larger learning rate is required. However, a large learning rate makes the algorithm unstable. What we do next is to make the algorithm stable and fast.\nReferences  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","wordCount":"1142","inLanguage":"en","datePublished":"2020-01-07T10:14:53Z","dateModified":"2022-05-03T10:39:43+08:00","author":{"@type":"Person","name":"Anthony Tan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://anthony-tan.com/Drawbacks-of-Backpropagation/"},"publisher":{"@type":"Organization","name":"Anthony's Blogs","logo":{"@type":"ImageObject","url":"https://anthony-tan.com/logo.png"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://anthony-tan.com accesskey=h title="Anthony's Blogs (Alt + H)">Anthony's Blogs</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://anthony-tan.com/machine_learning/ title="Machine Learning"><span>Machine Learning</span></a></li><li><a href=https://anthony-tan.com/deep_learning/ title="Deep Learning"><span>Deep Learning</span></a></li><li><a href=https://anthony-tan.com/reinforcement_learning/ title="Reinforcement Learning"><span>Reinforcement Learning</span></a></li><li><a href=https://anthony-tan.com/math/ title=Math><span>Math</span></a></li><li><a href=https://anthony-tan.com/others/ title=Others><span>Others</span></a></li><li><a href=https://anthony-tan.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://anthony-tan.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://anthony-tan.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://anthony-tan.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://anthony-tan.com>Home</a>&nbsp;»&nbsp;<a href=https://anthony-tan.com/deep_learning/>Deep Learning</a></div><h1 class=post-title>Drawbacks of Backpropagation</h1><div class=post-meta><span title="2020-01-07 10:14:53 +0000 UTC">January 7, 2020</span>&nbsp;·&nbsp;<span title="2022-05-03 10:39:43 +0800 +0800">(Last Modification: May 3, 2022)</span>&nbsp;·&nbsp;Anthony Tan</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#preliminaries aria-label=Preliminaries>Preliminaries</a></li><li><a href=#speed-backpropagation-up-1 aria-label="Speed Backpropagation up 1">Speed Backpropagation up <a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a></a></li><li><a href=#drawbacks-of-backpropagation aria-label="Drawbacks of Backpropagation">Drawbacks of Backpropagation</a></li><li><a href=#convergence-example aria-label="Convergence Example">Convergence Example</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=preliminaries>Preliminaries<a hidden class=anchor aria-hidden=true href=#preliminaries>#</a></h2><ol type=1><li><a href=https://anthony-tan.com/An-Introduction-to-Backpropagation-and-Multilayer-Perceptrons/>‘An Introduction to Backpropagation and Multilayer Perceptrons’</a></li><li><a href=https://anthony-tan.com/The-Backpropagation-Algorithm/>‘The Backpropagation Algorithm’</a></li></ol><h2 id=speed-backpropagation-up-1>Speed Backpropagation up <a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a><a hidden class=anchor aria-hidden=true href=#speed-backpropagation-up-1>#</a></h2><p>BP algorithm has been described in <a href=https://anthony-tan.com/An-Introduction-to-Backpropagation-and-Multilayer-Perceptrons/>‘An Introduction to Backpropagation and Multilayer Perceptrons’</a>. And the implementation of the BP algorithm has been recorded at <a href=https://anthony-tan.com/The-Backpropagation-Algorithm/>‘The Backpropagation Algorithm’</a>. BP has worked in many applications for many years, but there are too many drawbacks in the process. The basic BP algorithm is too slow for most practical applications that it might take days or even weeks in training. And the following posts are some investigations to make the BP algorithm more practical and speed it up.</p><p>In the post <a href=https://anthony-tan.com/Backpropagation-Batch-Training-and-Incremental-Training/>‘Backpropagation, Batch Training, and Incremental Training’</a>, the BP approximation example had shown that the algorithm converged very slowly. BP is a variation of LMS and LMS is a variation of <a href=https://anthony-tan.com/Steepest-Descent-Method/>‘steepest descent’</a>. So BP is a kind of steepest descent, and the difference between them is the calculation of derivatives. Steepest descent is the simplest and the slowest, while Newton and conjugate algorithms are faster. Then inspiration comes to us whether these algorithms can be used in speeding up the convergence of BP.</p><p>Research on faster algorithms falls on rough two categories and some aspects would be discussed:</p><ol type=1><li>Development of heuristic techniques<ul><li>varying learning rate</li><li>momentum</li><li>rescaling variables</li></ul></li><li>The standard numerical optimization technique<ul><li>find a numerical optimization technique already exists. ‘Reinvent the wheel’ is a good and bad idea.</li><li>Conjugate gradient algorithm</li><li>Levenberg-Marquardt algorithm</li></ul></li></ol><p>Backpropagation is called ‘back’ because in the calculation of the sensitivities in the hidden layer are calculated by its next layer neurons that have connections with it. And the weights and biases updating process of BP is the same as the steepest descent. So we name the standard backpropagation algorithm steepest descent backpropagation and SDBP for short.</p><h2 id=drawbacks-of-backpropagation>Drawbacks of Backpropagation<a hidden class=anchor aria-hidden=true href=#drawbacks-of-backpropagation>#</a></h2><p>LMS guarantee to converge that minimize the MSE under the condition that the learning rate <span class="math inline">\(\alpha\)</span> is small enough. MSE is a quadratic function that has always a single stationary point and constant Hessian. Because Hessian matrices of quadratic functions do not change, so the curvature of functions would not change and their contours are elliptical.</p><p>When BP is used in a layered network, it degenerates to the LMS algorithm. The MSE of the single-layer network is quadratic. So it has a single stationary point and constant curvature.</p><p>But when the network has more than one layer, its MSE of it is no more quadratic. MSE of multiple layers network has many local minimum points and curvature varies widely in different regions of parameter space. Now let go to look at the surface of MSE of multiple layers network. The simplest network 1-2-1 would be our example and its abbreviated notation is:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_02_17_06_abbreviated_notation.jpeg></p><p>the transfer functions of both the hidden layer and the output layer are log-sigmoid functions. And in the following experiment the function to approximate has the same architecture as the 1-2-1 network and its parameters are:</p><p><span class="math display">\[
\begin{aligned}
\{&\\
&w^1_{1,1}=10,b^1_{1}=-5,\\
&w^1_{1,2}=10,b^1_{2}=5,\\
&w^2_{1,1}=1,w^2_{1,2}=1,b^2=-1\\
\}&
\end{aligned}
\tag{1}
\]</span></p><p>Then in this task, the global minimum point is at equation(1). And in the interval <span class="math inline">\([-2,2]\)</span> the target function looks like this:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_02_17_06_target_function.png></p><p>Because the target function to be approximated has the same architecture as our model, the 1-2-1 network can have the correct approximation where MSE is 0. Although this experiment is humble compared to practical applications, it can illustrate some important concepts.</p><p>To approximate the target function, we generate the inputs:</p><p><span class="math display">\[
\{-2.0,-1.9,\cdot,1.9,2.0\}
\]</span></p><p>with equivalent probability and the corresponding outputs. The performance index is the sum of square errors which equals MSE.</p><p>There are 7 parameters in this simple network model totally. However, we can not observe them all at one picture. We set <span class="math inline">\(b^1_{1},w^1_{1,2},b^1_{2},w^2_{1,2},b^2\)</span> to their optimum values given by equation(1) and leave <span class="math inline">\(w^1_{1,1}\)</span> and <span class="math inline">\(w^2_{1,1}\)</span> as variables. Then we get the contour map:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_02_17_06_contour_map.png></p><p>The performance index is not quadratic. Then it has more than one minimum, for instance, <span class="math inline">\(w^1_{1,1}=0.88\)</span> and <span class="math inline">\(w^2_{1,1}= 38.6\)</span>, and solution (1) is also a local minimum of the performance index. The curvature varies drastically over the parameter space and a constant learning rate can not suit the entire process. Because some regions are flat where a large learning rate is good, and some regions of curvature are steep where a small learning rate is necessary. The flat region is a common condition when the transfer function of the network is sigmoid. For example, when the inputs are large, the surface of the performance index of the sigmoid network is very flat.</p><p>Because the 1-2-1 network has a symmetrical architecture, the surface of <span class="math inline">\(b^1_1\)</span> and <span class="math inline">\(b^1_2\)</span> is symmetric as well. and between these two local minimums there must be a saddle point which is at <span class="math inline">\(b^1_1=0,b^1_2=0\)</span>:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_02_17_06_contour_b.png></p><p>So <span class="math inline">\(b^1_1=0\)</span> and <span class="math inline">\(b^1_2=0\)</span> are not good initial values. And if initial values of the parameters were large, the learning algorithm would start at a very flat region, this is also the nightmare for most algorithms. Trying several initial guesses is also a good idea but when the whole training process needs days or weeks this method is impractical. A common method for initial parameters of networks is using small random numbers as a random number between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span> with uniform distribution.</p><h2 id=convergence-example>Convergence Example<a hidden class=anchor aria-hidden=true href=#convergence-example>#</a></h2><p>The batching method has been introduced in <a href=https://anthony-tan.com/Backpropagation-Batch-Training-and-Incremental-Training/>‘Backpropagation, Batch Training, and Incremental Training’</a>. It is a generalized method that uses the whole training set of the <a href=https://anthony-tan.com/The-Backpropagation-Algorithm/>‘The Backpropagation Algorithm’</a>, which uses one point of the training set at a time. The following process is based on the batching method.</p><p>Now let’s consider the parameter <span class="math inline">\(w^1_{1,1}\)</span> and the <span class="math inline">\(w^2_{1,1}\)</span> while other parameters are set to optimum solution as in equation(1).</p><p>The first example is with initial guesses <span class="math inline">\(w^1_{1,1}=-4\)</span> and <span class="math inline">\(w^2_{1,1}=-4\)</span> whose trajectory is labeled as ‘a’:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_02_17_06_a.gif></p><p>It takes a long time during the flat region and the entire process takes more than 300,000 epochs. And the sum of the square error of the performance index is:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_02_17_06_a_loss.png></p><p>The flat region takes a great part of the whole process.</p><p>The second example is with initial guesses <span class="math inline">\(w^1_{1,1}=-4\)</span> and <span class="math inline">\(w^2_{1,1}=10\)</span> whose trajectory is labeled as ‘b’:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_02_17_06_b.gif></p><p>It converges to another local minimum point but not the global minimum. And the sum of the squire error of the performance index is:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_02_17_07_b_loss.png></p><p>whose final point can not reduce error to 0.</p><p>As we have mentioned above, in the flat region a bigger learning rate is required. Now let’s start at <span class="math inline">\(w^1_{1,1}=-4\)</span> and <span class="math inline">\(w^2_{1,1}=-4\)</span> as well. But when we increase the learning rate from <span class="math inline">\(1\)</span> to <span class="math inline">\(100\)</span>, the algorithm becomes unstable:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_02_17_07_c.gif></p><p>and its error is not decrease after some iterations:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_02_17_07_c_loss.png></p><p>After all the experiments above, we found that flat regions are everywhere then a larger learning rate is required. However, a large learning rate makes the algorithm unstable. What we do next is to make the algorithm stable and fast.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><section class=footnotes role=doc-endnotes><hr><ol><li id=fn1 role=doc-endnote><p>Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.<a href=#fnref1 class=footnote-back role=doc-backlink>↩︎</a></p></li></ol></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://anthony-tan.com/tags/artificial-neural-networks/>Artificial Neural Networks</a></li><li><a href=https://anthony-tan.com/tags/artificial-intelligence/>Artificial Intelligence</a></li><li><a href=https://anthony-tan.com/tags/backpropagation/>backpropagation</a></li><li><a href=https://anthony-tan.com/tags/bp/>BP</a></li><li><a href=https://anthony-tan.com/tags/speed-backpropagation-up/>Speed Backpropagation up</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Drawbacks of Backpropagation on twitter" href="https://twitter.com/intent/tweet/?text=Drawbacks%20of%20Backpropagation&url=https%3a%2f%2fanthony-tan.com%2fDrawbacks-of-Backpropagation%2f&hashtags=ArtificialNeuralNetworks%2cArtificialIntelligence%2cBackpropagation%2cBP%2cSpeedBackpropagationup"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Drawbacks of Backpropagation on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fanthony-tan.com%2fDrawbacks-of-Backpropagation%2f&title=Drawbacks%20of%20Backpropagation&summary=Drawbacks%20of%20Backpropagation&source=https%3a%2f%2fanthony-tan.com%2fDrawbacks-of-Backpropagation%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Drawbacks of Backpropagation on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fanthony-tan.com%2fDrawbacks-of-Backpropagation%2f&title=Drawbacks%20of%20Backpropagation"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Drawbacks of Backpropagation on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fanthony-tan.com%2fDrawbacks-of-Backpropagation%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Drawbacks of Backpropagation on whatsapp" href="https://api.whatsapp.com/send?text=Drawbacks%20of%20Backpropagation%20-%20https%3a%2f%2fanthony-tan.com%2fDrawbacks-of-Backpropagation%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Drawbacks of Backpropagation on telegram" href="https://telegram.me/share/url?text=Drawbacks%20of%20Backpropagation&url=https%3a%2f%2fanthony-tan.com%2fDrawbacks-of-Backpropagation%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><figure class=article-discussion><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//anthony-tan-com.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></figure></article></main><footer class=footer><span>&copy; 2022 <a href=https://anthony-tan.com>Anthony's Blogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.1.4/mermaid.min.js crossorigin=anonymous></script>
<script>mermaid.init(void 0,".language-mermaid")</script></body></html>