<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Boosting and AdaBoost | Anthony's Blogs</title><meta name=keywords content="Machine Learning,Combining Models,AdaBoost,Boosting,classifier"><meta name=description content="Preliminaries Committee  Boosting1 The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:
performances better than any of the base classifiers and each base classifier has a different weight factor  Adaboost Adaboost is short for adaptive boosting."><meta name=author content="Anthony Tan"><link rel=canonical href=https://anthony-tan.com/Boosting-and-AdaBoost/><link crossorigin=anonymous href=../assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://anthony-tan.com/logo.png><link rel=apple-touch-icon href=https://anthony-tan.com/logo.png><link rel=mask-icon href=https://anthony-tan.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-12","auto"),ga("send","pageview"))</script><meta property="og:title" content="Boosting and AdaBoost"><meta property="og:description" content="Preliminaries Committee  Boosting1 The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:
performances better than any of the base classifiers and each base classifier has a different weight factor  Adaboost Adaboost is short for adaptive boosting."><meta property="og:type" content="article"><meta property="og:url" content="https://anthony-tan.com/Boosting-and-AdaBoost/"><meta property="article:section" content="machine_learning"><meta property="article:published_time" content="2020-03-07T15:40:46+00:00"><meta property="article:modified_time" content="2022-04-29T16:10:59+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Boosting and AdaBoost"><meta name=twitter:description content="Preliminaries Committee  Boosting1 The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:
performances better than any of the base classifiers and each base classifier has a different weight factor  Adaboost Adaboost is short for adaptive boosting."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Machine Learning","item":"https://anthony-tan.com/machine_learning/"},{"@type":"ListItem","position":3,"name":"Boosting and AdaBoost","item":"https://anthony-tan.com/Boosting-and-AdaBoost/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Boosting and AdaBoost","name":"Boosting and AdaBoost","description":"Preliminaries Committee  Boosting1 The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:\nperformances better than any of the base classifiers and each base classifier has a different weight factor  Adaboost Adaboost is short for adaptive boosting.","keywords":["Machine Learning","Combining Models","AdaBoost","Boosting","classifier"],"articleBody":"Preliminaries Committee  Boosting1 The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:\nperformances better than any of the base classifiers and each base classifier has a different weight factor  Adaboost Adaboost is short for adaptive boosting. It is a method combining several weak classifiers which are just better than random guesses and it gives a better performance than the committee. The base classifiers in AdaBoost are trained sequentially, and their training set is the same but with different weights for each sample. So when we consider the distribution of training data, every weak classifier was trained on different sample distribution. This might be an important reason for the improvement of AdaBoost from the committee. And the weights for weak classifiers are generated depending on the performance of the previous classifier.\nDuring the prediction process, the input data flows from classifier to classifier and the final result is some kind of combination of all output of weak classifiers.\nImportant ideas in the AdaBoost algorithm are:\nthe data points are predicted incorrectly in the current classifier giving a greater weight once the algorithm was trained, the prediction of each classifier is combined through a weighted majority voting scheme as:  where \\(w_n^{(1)}\\) is the initial weights of input data of the \\(1\\) st weak classifier, \\(y_1(x)\\) is the prediction of the \\(1\\) st weak classifier, \\(\\alpha_m\\) is the weight of each prediction(notably this weight works to \\(y_m(x)\\) and \\(w_n^{(1)}\\) is the weight of input data of the first classifier.). And the final output is the sign function of the weighted sum of all predictions.\nThe procedure of the algorithm is:\n Initial data weighting coefficients \\(\\{\\boldsymbol{w}_n\\}\\) by \\(w_n^{(1)}=\\frac{1}{N}\\) for \\(n=1,2,\\cdots,N\\) For \\(m=1,\\dots,M\\):   Fit a classifier \\(y_m(\\boldsymbol{x})\\) to training set by minimizing the weighted error function: \\[J_m=\\sum_{}^{}w_n^{(m)}I(y_m(\\boldsymbol{x}_n)\\neq t_n)\\] where \\(I(y_m(\\boldsymbol{x})\\neq t_n)\\) is the indicator function and equals 1 when \\(y_m(\\boldsymbol{x})\\neq t_n\\) and 0 otherwise Evaluate the quatities: \\[\\epsilon_m=\\frac{\\sum_{n=1}^Nw_n^{(m)}I(y_m(\\boldsymbol{x})\\neq t_n)}{\\sum_{n=1}^{N}w_n^{(m)}}\\] and then use this to evaluate \\(\\alpha_m=\\ln \\{\\frac{1-\\epsilon_m}{\\epsilon_m}\\}\\) Updata the data weighting coefficients: \\[w_n^{(m+1)}=w_n^{(m)}\\exp\\{\\alpha_mI(y_m(\\boldsymbol{x})\\neq t_n)\\}\\]  Make predictions using the final model, which is given by: \\[Y_M = \\mathrm{sign} (\\sum_{m=1}^{M}\\alpha_my_m(x))\\]   This procedure comes from ‘Pattern recognition and machine learning’2\nPython Code of Adaboost # weak classifier # test each dimension and each value and each direction to find a # best threshold and direction('') class Stump():  def __init__(self):  self.feature = 0  self.threshold = 0  self.direction = '   def loss(self,y_hat, y, weights):  \"\"\" :param y_hat: prediction :param y: target :param weights: weight of each data :return: loss \"\"\"  sum = 0  example_size = y.shape[0]  for i in range(example_size):  if y_hat[i] != y[i]:  sum += weights[i]  return sum   def test_in_traing(self, x, feature, threshold, direction='):  \"\"\" test during training :param x: input data :param feature: classification on which dimension :param threshold: threshold :param direction: '' to threshold :return: classification result \"\"\"  example_size = x.shape[0]  classification_result = -np.ones(example_size)  for i in range(example_size):  if direction == ':  if x[i][feature]  threshold:  classification_result[i] = 1  else:  if x[i][feature]  threshold:  classification_result[i] = 1  return classification_result   def test(self,x):  \"\"\" test during prediction :param x: input :return: classification result \"\"\"  return self.test_in_traing(x, self.feature, self.threshold, self.direction)   def training(self, x, y, weights):  \"\"\" main training process :param x: input :param y: target :param weights: weights :return: none \"\"\"  example_size = x.shape[0]  example_dimension = x.shape[1]  loss_matrix_less = np.zeros(np.shape(x))  loss_matrix_more = np.zeros(np.shape(x))  for i in range(example_dimension):  for j in range(example_size):  results_ji_less = self.test_in_traing(x, i, x[j][i], ')  results_ji_more = self.test_in_traing(x, i, x[j][i], '')  loss_matrix_less[j][i] = self.loss(results_ji_less, y, weights)  loss_matrix_more[j][i] = self.loss(results_ji_more, y, weights)  loss_matrix_less_min = np.min(loss_matrix_less)  loss_matrix_more_min = np.min(loss_matrix_more)  if loss_matrix_less_min  loss_matrix_more_min:  minimum_position = np.where(loss_matrix_more == loss_matrix_more_min)  self.threshold = x[minimum_position[0][0]][minimum_position[1][0]]  self.feature = minimum_position[1][0]  self.direction = ''  else:  minimum_position = np.where(loss_matrix_less == loss_matrix_less_min)  self.threshold = x[minimum_position[0][0]][minimum_position[1][0]]  self.feature = minimum_position[1][0]  self.direction = '   class Adaboost():  def __init__(self, maximum_classifier_size):  self.max_classifier_size = maximum_classifier_size  self.classifiers = []  self.alpha = np.ones(self.max_classifier_size)   def training(self, x, y, classifier_class):  \"\"\" training adaboost main steps :param x: input :param y: target :param classifier_class: what can classifier would be used, here we use stump above :return: none \"\"\"  example_size = x.shape[0]  weights = np.ones(example_size)/example_size   for i in range(self.max_classifier_size):  classifier = classifier_class()  classifier.training(x, y, weights)  test_res = classifier.test(x)  indicator = np.zeros(len(weights))  for j in range(len(indicator)):  if test_res[j] != y[j]:  indicator[j] = 1   cost_function = np.sum(weights*indicator)  epsilon = cost_function/np.sum(weights)  self.alpha[i] = np.log((1-epsilon)/epsilon)  self.classifiers.append(classifier)  weights = weights * np.exp(self.alpha[i]*indicator)   def predictor(self, x):  \"\"\" prediction :param x: input data :return: prediction result \"\"\"  example_size = x.shape[0]  results = np.zeros(example_size)  for i in range(example_size):  y = np.zeros(self.max_classifier_size)  for j in range(self.max_classifier_size):  y[j] = self.classifiers[j].test(x[i].reshape(1,-1))  results[i] = np.sign(np.sum(self.alpha*y))  return results the entire project can be found https://github.com/Tony-Tan/ML. And please star me! Thanks!\nWhen we use different numbers of classifiers, the results of the algorithm are like this:\nwhere the blue circles are the correct classification of class 1 and red circles are the correct classification of class 2. And the blue crosses belong to class 2 but were classified into class 1, and so do the red crosses.\nA 40-classifiers AdaBoost gives a relatively good prediction:\nwhere there is only one misclassified point.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","wordCount":"888","inLanguage":"en","datePublished":"2020-03-07T15:40:46Z","dateModified":"2022-04-29T16:10:59+08:00","author":{"@type":"Person","name":"Anthony Tan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://anthony-tan.com/Boosting-and-AdaBoost/"},"publisher":{"@type":"Organization","name":"Anthony's Blogs","logo":{"@type":"ImageObject","url":"https://anthony-tan.com/logo.png"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://anthony-tan.com accesskey=h title="Anthony's Blogs (Alt + H)">Anthony's Blogs</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://anthony-tan.com/machine_learning/ title="Machine Learning"><span>Machine Learning</span></a></li><li><a href=https://anthony-tan.com/deep_learning/ title="Deep Learning"><span>Deep Learning</span></a></li><li><a href=https://anthony-tan.com/reinforcement_learning/ title="Reinforcement Learning"><span>Reinforcement Learning</span></a></li><li><a href=https://anthony-tan.com/math/ title=Math><span>Math</span></a></li><li><a href=https://anthony-tan.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://anthony-tan.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://anthony-tan.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://anthony-tan.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://anthony-tan.com>Home</a>&nbsp;»&nbsp;<a href=https://anthony-tan.com/machine_learning/>Machine Learning</a></div><h1 class=post-title>Boosting and AdaBoost</h1><div class=post-meta><span title="2020-03-07 15:40:46 +0000 UTC">March 7, 2020</span>&nbsp;·&nbsp;<span title="2022-04-29 16:10:59 +0800 +0800">(Last Modification: April 29, 2022)</span>&nbsp;·&nbsp;Anthony Tan</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#preliminaries aria-label=Preliminaries>Preliminaries</a></li><li><a href=#boosting1 aria-label=Boosting1>Boosting<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a></a></li><li><a href=#adaboost aria-label=Adaboost>Adaboost</a></li><li><a href=#python-code-of-adaboost aria-label="Python Code of Adaboost">Python Code of Adaboost</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=preliminaries>Preliminaries<a hidden class=anchor aria-hidden=true href=#preliminaries>#</a></h2><ol type=1><li><a href=https://anthony-tan.com/Committees/>Committee</a></li></ol><h2 id=boosting1>Boosting<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a><a hidden class=anchor aria-hidden=true href=#boosting1>#</a></h2><p>The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:</p><ol type=1><li>performances better than any of the base classifiers and</li><li>each base classifier has a different weight factor</li></ol><h2 id=adaboost>Adaboost<a hidden class=anchor aria-hidden=true href=#adaboost>#</a></h2><p>Adaboost is short for adaptive boosting. It is a method combining several weak classifiers which are just better than random guesses and it gives a better performance than the committee. The base classifiers in AdaBoost are trained sequentially, and their training set is the same but with different weights for each sample. So when we consider the distribution of training data, every weak classifier was trained on different sample distribution. <strong>This might be an important reason for the improvement of AdaBoost from the committee</strong>. And the weights for weak classifiers are generated depending on the performance of the previous classifier.</p><p>During the prediction process, the input data flows from classifier to classifier and the final result is some kind of combination of all output of weak classifiers.</p><p>Important ideas in the AdaBoost algorithm are:</p><ol type=1><li>the data points are predicted incorrectly in the current classifier giving a greater weight</li><li>once the algorithm was trained, the prediction of each classifier is combined through a weighted majority voting scheme as:</li></ol><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_28_12_52_voting_scheme.jpeg></p><p>where <span class="math inline">\(w_n^{(1)}\)</span> is the initial weights of input data of the <span class="math inline">\(1\)</span> st weak classifier, <span class="math inline">\(y_1(x)\)</span> is the prediction of the <span class="math inline">\(1\)</span> st weak classifier, <span class="math inline">\(\alpha_m\)</span> is the weight of each prediction(notably this weight works to <span class="math inline">\(y_m(x)\)</span> and <span class="math inline">\(w_n^{(1)}\)</span> is the weight of input data of the first classifier.). And the final output is the sign function of the weighted sum of all predictions.</p><p>The procedure of the algorithm is:</p><blockquote><ol type=1><li>Initial data weighting coefficients <span class="math inline">\(\{\boldsymbol{w}_n\}\)</span> by <span class="math inline">\(w_n^{(1)}=\frac{1}{N}\)</span> for <span class="math inline">\(n=1,2,\cdots,N\)</span></li><li>For <span class="math inline">\(m=1,\dots,M\)</span>:</li></ol><ul><li>Fit a classifier <span class="math inline">\(y_m(\boldsymbol{x})\)</span> to training set by minimizing the weighted error function: <span class="math display">\[J_m=\sum_{}^{}w_n^{(m)}I(y_m(\boldsymbol{x}_n)\neq t_n)\]</span> where <span class="math inline">\(I(y_m(\boldsymbol{x})\neq t_n)\)</span> is the indicator function and equals 1 when <span class="math inline">\(y_m(\boldsymbol{x})\neq t_n\)</span> and 0 otherwise</li><li>Evaluate the quatities: <span class="math display">\[\epsilon_m=\frac{\sum_{n=1}^Nw_n^{(m)}I(y_m(\boldsymbol{x})\neq t_n)}{\sum_{n=1}^{N}w_n^{(m)}}\]</span> and then use this to evaluate <span class="math inline">\(\alpha_m=\ln \{\frac{1-\epsilon_m}{\epsilon_m}\}\)</span></li><li>Updata the data weighting coefficients: <span class="math display">\[w_n^{(m+1)}=w_n^{(m)}\exp\{\alpha_mI(y_m(\boldsymbol{x})\neq t_n)\}\]</span></li></ul><ol start=3 type=1><li>Make predictions using the final model, which is given by: <span class="math display">\[Y_M = \mathrm{sign} (\sum_{m=1}^{M}\alpha_my_m(x))\]</span></li></ol></blockquote><p>This procedure comes from ‘Pattern recognition and machine learning’<a href=#fn2 class=footnote-ref id=fnref2 role=doc-noteref><sup>2</sup></a></p><h2 id=python-code-of-adaboost>Python Code of Adaboost<a hidden class=anchor aria-hidden=true href=#python-code-of-adaboost>#</a></h2><div class=sourceCode id=cb1><pre class="sourceCode python"><code class="sourceCode python"><span id=cb1-1><a href=#cb1-1></a><span class=co># weak classifier</span></span>
<span id=cb1-2><a href=#cb1-2></a><span class=co># test each dimension and each value and each direction to find a</span></span>
<span id=cb1-3><a href=#cb1-3></a><span class=co># best threshold and direction(&#39;&lt;&#39; or &#39;&gt;&#39;)</span></span>
<span id=cb1-4><a href=#cb1-4></a><span class=kw>class</span> Stump():</span>
<span id=cb1-5><a href=#cb1-5></a>    <span class=kw>def</span> <span class=fu>__init__</span>(<span class=va>self</span>):</span>
<span id=cb1-6><a href=#cb1-6></a>        <span class=va>self</span>.feature <span class=op>=</span> <span class=dv>0</span></span>
<span id=cb1-7><a href=#cb1-7></a>        <span class=va>self</span>.threshold <span class=op>=</span> <span class=dv>0</span></span>
<span id=cb1-8><a href=#cb1-8></a>        <span class=va>self</span>.direction <span class=op>=</span> <span class=st>&#39;&lt;&#39;</span></span>
<span id=cb1-9><a href=#cb1-9></a></span>
<span id=cb1-10><a href=#cb1-10></a>    <span class=kw>def</span> loss(<span class=va>self</span>,y_hat, y, weights):</span>
<span id=cb1-11><a href=#cb1-11></a>        <span class=co>&quot;&quot;&quot;</span></span>
<span id=cb1-12><a href=#cb1-12></a><span class=co>        :param y_hat: prediction</span></span>
<span id=cb1-13><a href=#cb1-13></a><span class=co>        :param y: target</span></span>
<span id=cb1-14><a href=#cb1-14></a><span class=co>        :param weights:  weight of each data</span></span>
<span id=cb1-15><a href=#cb1-15></a><span class=co>        :return: loss</span></span>
<span id=cb1-16><a href=#cb1-16></a><span class=co>        &quot;&quot;&quot;</span></span>
<span id=cb1-17><a href=#cb1-17></a>        <span class=bu>sum</span> <span class=op>=</span> <span class=dv>0</span></span>
<span id=cb1-18><a href=#cb1-18></a>        example_size <span class=op>=</span> y.shape[<span class=dv>0</span>]</span>
<span id=cb1-19><a href=#cb1-19></a>        <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(example_size):</span>
<span id=cb1-20><a href=#cb1-20></a>            <span class=cf>if</span> y_hat[i] <span class=op>!=</span> y[i]:</span>
<span id=cb1-21><a href=#cb1-21></a>                <span class=bu>sum</span> <span class=op>+=</span> weights[i]</span>
<span id=cb1-22><a href=#cb1-22></a>        <span class=cf>return</span> <span class=bu>sum</span></span>
<span id=cb1-23><a href=#cb1-23></a></span>
<span id=cb1-24><a href=#cb1-24></a>    <span class=kw>def</span> test_in_traing(<span class=va>self</span>, x, feature, threshold, direction<span class=op>=</span><span class=st>&#39;&lt;&#39;</span>):</span>
<span id=cb1-25><a href=#cb1-25></a>        <span class=co>&quot;&quot;&quot;</span></span>
<span id=cb1-26><a href=#cb1-26></a><span class=co>        test during training</span></span>
<span id=cb1-27><a href=#cb1-27></a><span class=co>        :param x: input data</span></span>
<span id=cb1-28><a href=#cb1-28></a><span class=co>        :param feature: classification on which dimension</span></span>
<span id=cb1-29><a href=#cb1-29></a><span class=co>        :param threshold:  threshold</span></span>
<span id=cb1-30><a href=#cb1-30></a><span class=co>        :param direction:  &#39;&lt;&#39; or &#39;&gt;&#39; to threshold</span></span>
<span id=cb1-31><a href=#cb1-31></a><span class=co>        :return: classification result</span></span>
<span id=cb1-32><a href=#cb1-32></a><span class=co>        &quot;&quot;&quot;</span></span>
<span id=cb1-33><a href=#cb1-33></a>        example_size <span class=op>=</span> x.shape[<span class=dv>0</span>]</span>
<span id=cb1-34><a href=#cb1-34></a>        classification_result <span class=op>=</span> <span class=op>-</span>np.ones(example_size)</span>
<span id=cb1-35><a href=#cb1-35></a>        <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(example_size):</span>
<span id=cb1-36><a href=#cb1-36></a>            <span class=cf>if</span> direction <span class=op>==</span> <span class=st>&#39;&lt;&#39;</span>:</span>
<span id=cb1-37><a href=#cb1-37></a>                <span class=cf>if</span> x[i][feature] <span class=op>&lt;</span> threshold:</span>
<span id=cb1-38><a href=#cb1-38></a>                    classification_result[i] <span class=op>=</span> <span class=dv>1</span></span>
<span id=cb1-39><a href=#cb1-39></a>            <span class=cf>else</span>:</span>
<span id=cb1-40><a href=#cb1-40></a>                <span class=cf>if</span> x[i][feature] <span class=op>&gt;</span> threshold:</span>
<span id=cb1-41><a href=#cb1-41></a>                    classification_result[i] <span class=op>=</span> <span class=dv>1</span></span>
<span id=cb1-42><a href=#cb1-42></a>        <span class=cf>return</span> classification_result</span>
<span id=cb1-43><a href=#cb1-43></a></span>
<span id=cb1-44><a href=#cb1-44></a>    <span class=kw>def</span> test(<span class=va>self</span>,x):</span>
<span id=cb1-45><a href=#cb1-45></a>        <span class=co>&quot;&quot;&quot;</span></span>
<span id=cb1-46><a href=#cb1-46></a><span class=co>        test during prediction</span></span>
<span id=cb1-47><a href=#cb1-47></a><span class=co>        :param x:  input</span></span>
<span id=cb1-48><a href=#cb1-48></a><span class=co>        :return: classification result</span></span>
<span id=cb1-49><a href=#cb1-49></a><span class=co>        &quot;&quot;&quot;</span></span>
<span id=cb1-50><a href=#cb1-50></a>        <span class=cf>return</span> <span class=va>self</span>.test_in_traing(x, <span class=va>self</span>.feature, <span class=va>self</span>.threshold, <span class=va>self</span>.direction)</span>
<span id=cb1-51><a href=#cb1-51></a></span>
<span id=cb1-52><a href=#cb1-52></a>    <span class=kw>def</span> training(<span class=va>self</span>, x, y, weights):</span>
<span id=cb1-53><a href=#cb1-53></a>        <span class=co>&quot;&quot;&quot;</span></span>
<span id=cb1-54><a href=#cb1-54></a><span class=co>        main training process</span></span>
<span id=cb1-55><a href=#cb1-55></a><span class=co>        :param x: input</span></span>
<span id=cb1-56><a href=#cb1-56></a><span class=co>        :param y: target</span></span>
<span id=cb1-57><a href=#cb1-57></a><span class=co>        :param weights: weights</span></span>
<span id=cb1-58><a href=#cb1-58></a><span class=co>        :return: none</span></span>
<span id=cb1-59><a href=#cb1-59></a><span class=co>        &quot;&quot;&quot;</span></span>
<span id=cb1-60><a href=#cb1-60></a>        example_size <span class=op>=</span> x.shape[<span class=dv>0</span>]</span>
<span id=cb1-61><a href=#cb1-61></a>        example_dimension <span class=op>=</span> x.shape[<span class=dv>1</span>]</span>
<span id=cb1-62><a href=#cb1-62></a>        loss_matrix_less <span class=op>=</span> np.zeros(np.shape(x))</span>
<span id=cb1-63><a href=#cb1-63></a>        loss_matrix_more <span class=op>=</span> np.zeros(np.shape(x))</span>
<span id=cb1-64><a href=#cb1-64></a>        <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(example_dimension):</span>
<span id=cb1-65><a href=#cb1-65></a>            <span class=cf>for</span> j <span class=kw>in</span> <span class=bu>range</span>(example_size):</span>
<span id=cb1-66><a href=#cb1-66></a>                results_ji_less <span class=op>=</span> <span class=va>self</span>.test_in_traing(x, i, x[j][i], <span class=st>&#39;&lt;&#39;</span>)</span>
<span id=cb1-67><a href=#cb1-67></a>                results_ji_more <span class=op>=</span> <span class=va>self</span>.test_in_traing(x, i, x[j][i], <span class=st>&#39;&gt;&#39;</span>)</span>
<span id=cb1-68><a href=#cb1-68></a>                loss_matrix_less[j][i] <span class=op>=</span> <span class=va>self</span>.loss(results_ji_less, y, weights)</span>
<span id=cb1-69><a href=#cb1-69></a>                loss_matrix_more[j][i] <span class=op>=</span> <span class=va>self</span>.loss(results_ji_more, y, weights)</span>
<span id=cb1-70><a href=#cb1-70></a>        loss_matrix_less_min <span class=op>=</span> np.<span class=bu>min</span>(loss_matrix_less)</span>
<span id=cb1-71><a href=#cb1-71></a>        loss_matrix_more_min <span class=op>=</span> np.<span class=bu>min</span>(loss_matrix_more)</span>
<span id=cb1-72><a href=#cb1-72></a>        <span class=cf>if</span> loss_matrix_less_min <span class=op>&gt;</span> loss_matrix_more_min:</span>
<span id=cb1-73><a href=#cb1-73></a>            minimum_position <span class=op>=</span> np.where(loss_matrix_more <span class=op>==</span> loss_matrix_more_min)</span>
<span id=cb1-74><a href=#cb1-74></a>            <span class=va>self</span>.threshold <span class=op>=</span> x[minimum_position[<span class=dv>0</span>][<span class=dv>0</span>]][minimum_position[<span class=dv>1</span>][<span class=dv>0</span>]]</span>
<span id=cb1-75><a href=#cb1-75></a>            <span class=va>self</span>.feature <span class=op>=</span> minimum_position[<span class=dv>1</span>][<span class=dv>0</span>]</span>
<span id=cb1-76><a href=#cb1-76></a>            <span class=va>self</span>.direction <span class=op>=</span> <span class=st>&#39;&gt;&#39;</span></span>
<span id=cb1-77><a href=#cb1-77></a>        <span class=cf>else</span>:</span>
<span id=cb1-78><a href=#cb1-78></a>            minimum_position <span class=op>=</span> np.where(loss_matrix_less <span class=op>==</span> loss_matrix_less_min)</span>
<span id=cb1-79><a href=#cb1-79></a>            <span class=va>self</span>.threshold <span class=op>=</span> x[minimum_position[<span class=dv>0</span>][<span class=dv>0</span>]][minimum_position[<span class=dv>1</span>][<span class=dv>0</span>]]</span>
<span id=cb1-80><a href=#cb1-80></a>            <span class=va>self</span>.feature <span class=op>=</span> minimum_position[<span class=dv>1</span>][<span class=dv>0</span>]</span>
<span id=cb1-81><a href=#cb1-81></a>            <span class=va>self</span>.direction <span class=op>=</span> <span class=st>&#39;&lt;&#39;</span></span>
<span id=cb1-82><a href=#cb1-82></a></span>
<span id=cb1-83><a href=#cb1-83></a></span>
<span id=cb1-84><a href=#cb1-84></a><span class=kw>class</span> Adaboost():</span>
<span id=cb1-85><a href=#cb1-85></a>    <span class=kw>def</span> <span class=fu>__init__</span>(<span class=va>self</span>, maximum_classifier_size):</span>
<span id=cb1-86><a href=#cb1-86></a>        <span class=va>self</span>.max_classifier_size <span class=op>=</span> maximum_classifier_size</span>
<span id=cb1-87><a href=#cb1-87></a>        <span class=va>self</span>.classifiers <span class=op>=</span> []</span>
<span id=cb1-88><a href=#cb1-88></a>        <span class=va>self</span>.alpha <span class=op>=</span> np.ones(<span class=va>self</span>.max_classifier_size)</span>
<span id=cb1-89><a href=#cb1-89></a></span>
<span id=cb1-90><a href=#cb1-90></a>    <span class=kw>def</span> training(<span class=va>self</span>, x, y, classifier_class):</span>
<span id=cb1-91><a href=#cb1-91></a>        <span class=co>&quot;&quot;&quot;</span></span>
<span id=cb1-92><a href=#cb1-92></a><span class=co>        training adaboost main steps</span></span>
<span id=cb1-93><a href=#cb1-93></a><span class=co>        :param x: input</span></span>
<span id=cb1-94><a href=#cb1-94></a><span class=co>        :param y: target</span></span>
<span id=cb1-95><a href=#cb1-95></a><span class=co>        :param classifier_class:  what can classifier would be used, here we use stump above</span></span>
<span id=cb1-96><a href=#cb1-96></a><span class=co>        :return: none</span></span>
<span id=cb1-97><a href=#cb1-97></a><span class=co>        &quot;&quot;&quot;</span></span>
<span id=cb1-98><a href=#cb1-98></a>        example_size <span class=op>=</span> x.shape[<span class=dv>0</span>]</span>
<span id=cb1-99><a href=#cb1-99></a>        weights <span class=op>=</span> np.ones(example_size)<span class=op>/</span>example_size</span>
<span id=cb1-100><a href=#cb1-100></a></span>
<span id=cb1-101><a href=#cb1-101></a>        <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(<span class=va>self</span>.max_classifier_size):</span>
<span id=cb1-102><a href=#cb1-102></a>            classifier <span class=op>=</span> classifier_class()</span>
<span id=cb1-103><a href=#cb1-103></a>            classifier.training(x, y, weights)</span>
<span id=cb1-104><a href=#cb1-104></a>            test_res <span class=op>=</span> classifier.test(x)</span>
<span id=cb1-105><a href=#cb1-105></a>            indicator <span class=op>=</span> np.zeros(<span class=bu>len</span>(weights))</span>
<span id=cb1-106><a href=#cb1-106></a>            <span class=cf>for</span> j <span class=kw>in</span> <span class=bu>range</span>(<span class=bu>len</span>(indicator)):</span>
<span id=cb1-107><a href=#cb1-107></a>                <span class=cf>if</span> test_res[j] <span class=op>!=</span> y[j]:</span>
<span id=cb1-108><a href=#cb1-108></a>                    indicator[j] <span class=op>=</span> <span class=dv>1</span></span>
<span id=cb1-109><a href=#cb1-109></a></span>
<span id=cb1-110><a href=#cb1-110></a>            cost_function <span class=op>=</span> np.<span class=bu>sum</span>(weights<span class=op>*</span>indicator)</span>
<span id=cb1-111><a href=#cb1-111></a>            epsilon <span class=op>=</span> cost_function<span class=op>/</span>np.<span class=bu>sum</span>(weights)</span>
<span id=cb1-112><a href=#cb1-112></a>            <span class=va>self</span>.alpha[i] <span class=op>=</span> np.log((<span class=dv>1</span><span class=op>-</span>epsilon)<span class=op>/</span>epsilon)</span>
<span id=cb1-113><a href=#cb1-113></a>            <span class=va>self</span>.classifiers.append(classifier)</span>
<span id=cb1-114><a href=#cb1-114></a>            weights <span class=op>=</span> weights <span class=op>*</span> np.exp(<span class=va>self</span>.alpha[i]<span class=op>*</span>indicator)</span>
<span id=cb1-115><a href=#cb1-115></a></span>
<span id=cb1-116><a href=#cb1-116></a>    <span class=kw>def</span> predictor(<span class=va>self</span>, x):</span>
<span id=cb1-117><a href=#cb1-117></a>        <span class=co>&quot;&quot;&quot;</span></span>
<span id=cb1-118><a href=#cb1-118></a><span class=co>        prediction</span></span>
<span id=cb1-119><a href=#cb1-119></a><span class=co>        :param x: input data</span></span>
<span id=cb1-120><a href=#cb1-120></a><span class=co>        :return: prediction result</span></span>
<span id=cb1-121><a href=#cb1-121></a><span class=co>        &quot;&quot;&quot;</span></span>
<span id=cb1-122><a href=#cb1-122></a>        example_size <span class=op>=</span> x.shape[<span class=dv>0</span>]</span>
<span id=cb1-123><a href=#cb1-123></a>        results <span class=op>=</span> np.zeros(example_size)</span>
<span id=cb1-124><a href=#cb1-124></a>        <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(example_size):</span>
<span id=cb1-125><a href=#cb1-125></a>            y <span class=op>=</span> np.zeros(<span class=va>self</span>.max_classifier_size)</span>
<span id=cb1-126><a href=#cb1-126></a>            <span class=cf>for</span> j <span class=kw>in</span> <span class=bu>range</span>(<span class=va>self</span>.max_classifier_size):</span>
<span id=cb1-127><a href=#cb1-127></a>                y[j] <span class=op>=</span> <span class=va>self</span>.classifiers[j].test(x[i].reshape(<span class=dv>1</span>,<span class=op>-</span><span class=dv>1</span>))</span>
<span id=cb1-128><a href=#cb1-128></a>            results[i] <span class=op>=</span> np.sign(np.<span class=bu>sum</span>(<span class=va>self</span>.alpha<span class=op>*</span>y))</span>
<span id=cb1-129><a href=#cb1-129></a>        <span class=cf>return</span> results</span></code></pre></div><p>the entire project can be found <a href=https://github.com/Tony-Tan/ML>https://github.com/Tony-Tan/ML</a>. And please star me! Thanks!</p><p>When we use different numbers of classifiers, the results of the algorithm are like this:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_28_12_56_adaboost.gif></p><p>where the blue circles are the correct classification of class 1 and red circles are the correct classification of class 2. And the blue crosses belong to class 2 but were classified into class 1, and so do the red crosses.</p><p>A 40-classifiers AdaBoost gives a relatively good prediction:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_28_12_56_40.png></p><p>where there is only one misclassified point.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><section class=footnotes role=doc-endnotes><hr><ol><li id=fn1 role=doc-endnote><p>Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.<a href=#fnref1 class=footnote-back role=doc-backlink>↩︎</a></p></li><li id=fn2 role=doc-endnote><p>Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.<a href=#fnref2 class=footnote-back role=doc-backlink>↩︎</a></p></li></ol></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://anthony-tan.com/tags/machine-learning/>machine learning</a></li><li><a href=https://anthony-tan.com/tags/combining-models/>Combining Models</a></li><li><a href=https://anthony-tan.com/tags/adaboost/>AdaBoost</a></li><li><a href=https://anthony-tan.com/tags/boosting/>boosting</a></li><li><a href=https://anthony-tan.com/tags/classifier/>classifier</a></li></ul><nav class=paginav><a class=next href=https://anthony-tan.com/Committees/><span class=title>Next Page »</span><br><span>Committees</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Boosting and AdaBoost on twitter" href="https://twitter.com/intent/tweet/?text=Boosting%20and%20AdaBoost&url=https%3a%2f%2fanthony-tan.com%2fBoosting-and-AdaBoost%2f&hashtags=MachineLearning%2cCombiningModels%2cAdaBoost%2cBoosting%2cclassifier"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Boosting and AdaBoost on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fanthony-tan.com%2fBoosting-and-AdaBoost%2f&title=Boosting%20and%20AdaBoost&summary=Boosting%20and%20AdaBoost&source=https%3a%2f%2fanthony-tan.com%2fBoosting-and-AdaBoost%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Boosting and AdaBoost on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fanthony-tan.com%2fBoosting-and-AdaBoost%2f&title=Boosting%20and%20AdaBoost"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Boosting and AdaBoost on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fanthony-tan.com%2fBoosting-and-AdaBoost%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Boosting and AdaBoost on whatsapp" href="https://api.whatsapp.com/send?text=Boosting%20and%20AdaBoost%20-%20https%3a%2f%2fanthony-tan.com%2fBoosting-and-AdaBoost%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Boosting and AdaBoost on telegram" href="https://telegram.me/share/url?text=Boosting%20and%20AdaBoost&url=https%3a%2f%2fanthony-tan.com%2fBoosting-and-AdaBoost%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><figure class=article-discussion><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//anthony-tan-com.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></figure></article></main><footer class=footer><span>&copy; 2022 <a href=https://anthony-tan.com>Anthony's Blogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>