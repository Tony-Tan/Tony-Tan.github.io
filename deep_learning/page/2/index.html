<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deep Learning | Anthony's Blogs</title><meta name=keywords content><meta name=description content="Deep Learning Posts"><meta name=author content="Anthony Tan"><link rel=canonical href=https://anthony-tan.com/deep_learning/><link crossorigin=anonymous href=../../../assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=icon href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://anthony-tan.com/logo.png><link rel=apple-touch-icon href=https://anthony-tan.com/logo.png><link rel=mask-icon href=https://anthony-tan.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://anthony-tan.com/deep_learning/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-12","auto"),ga("send","pageview"))</script><meta property="og:title" content="Deep Learning"><meta property="og:description" content="Deep Learning Posts"><meta property="og:type" content="website"><meta property="og:url" content="https://anthony-tan.com/deep_learning/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deep Learning"><meta name=twitter:description content="Deep Learning Posts"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Deep Learning","item":"https://anthony-tan.com/deep_learning/"}]}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://anthony-tan.com accesskey=h title="Anthony's Blogs (Alt + H)">Anthony's Blogs</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://anthony-tan.com/machine_learning/ title="Machine Learning"><span>Machine Learning</span></a></li><li><a href=https://anthony-tan.com/deep_learning/ title="Deep Learning"><span class=active>Deep Learning</span></a></li><li><a href=https://anthony-tan.com/reinforcement_learning/ title="Reinforcement Learning"><span>Reinforcement Learning</span></a></li><li><a href=https://anthony-tan.com/math/ title=Math><span>Math</span></a></li><li><a href=https://anthony-tan.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://anthony-tan.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://anthony-tan.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://anthony-tan.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://anthony-tan.com>Home</a></div><h1>Deep Learning</h1><div class=post-description>Deep Learning Posts</div></header><article class=post-entry><header class=entry-header><h2>Performance Surfaces and Optimum Points</h2></header><section class=entry-content><p>Preliminaries Perceptron learning algorithm Hebbian learning algorithm Linear algebra Neural Network Training Technique1 Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:...</p></section><footer class=entry-footer><span title="2019-12-19 08:57:53 +0000 UTC">December 19, 2019</span>&nbsp;·&nbsp;<span title="2022-05-01 22:23:05 +0800 +0800">(Last Modification: May 1, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Performance Surfaces and Optimum Points" href=https://anthony-tan.com/Performance-Surfaces-and-Optimum-Points/></a></article><article class=post-entry><header class=entry-header><h2>Supervised Hebbian Learning</h2></header><section class=entry-content><p>Preliminaries Linear algebra Hebb Rule1 Hebb rule is one of the earliest neural network learning laws. It was published in 1949 by Donald O. Hebb, a Canadian psychologist, in his work ’ The Organization of Behavior’. In this great book, he proposed a possible mechanism for synaptic modification in the brain. And this rule then was used in training the artificial neural networks for pattern recognition.
’ The Organization of Behavior’ The main premise of the book is that behavior could be explained by the action of a neuron....</p></section><footer class=entry-footer><span title="2019-12-17 18:24:40 +0000 UTC">December 17, 2019</span>&nbsp;·&nbsp;<span title="2022-05-01 22:23:05 +0800 +0800">(Last Modification: May 1, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Supervised Hebbian Learning" href=https://anthony-tan.com/Supervised-Hebbian-Learning/></a></article><article class=post-entry><header class=entry-header><h2>Implement of Perceptron</h2></header><section class=entry-content><p>Preliminaries An Introduction to Neural Networks Neuron Model and Network Architecture Perceptron Learning Rule Implement of Perceptron1 What we need to do next is to implement the algorithm described in ‘Perceptron Learning Rule’ and observe the effect of 1. different parameters, 2. different training sets, 3. and different transfer functions.
A single neuron perceptron consists of a linear combination and a threshold operation simply. So we note its capacity is close to a linear classification....</p></section><footer class=entry-footer><span title="2019-12-12 13:10:28 +0000 UTC">December 12, 2019</span>&nbsp;·&nbsp;<span title="2022-04-30 21:21:04 +0800 +0800">(Last Modification: April 30, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Implement of Perceptron" href=https://anthony-tan.com/Implement-of-Perceptron/></a></article><article class=post-entry><header class=entry-header><h2>Learning Rules and Perceptron Learning Rule</h2></header><section class=entry-content><p>Preliminaries supervised learning unsupervised learning reinforcement learning ‘An Introduction to Neural Networks’ Learning Rules1 We have built some neural network models in the post ‘An Introduction to Neural Networks’ and as we know architectures and learning rules are two main aspects of designing a useful network. The architectures we have introduced could not be used yet. What we are going to do is to investigate the learning rules for different architectures....</p></section><footer class=entry-footer><span title="2019-12-11 21:30:42 +0000 UTC">December 11, 2019</span>&nbsp;·&nbsp;<span title="2022-05-03 10:39:43 +0800 +0800">(Last Modification: May 3, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Learning Rules and Perceptron Learning Rule" href=https://anthony-tan.com/Learning-Rules-and-Perceptron-Learning-Rule/></a></article><article class=post-entry><header class=entry-header><h2>Neuron Model and Network Architecture</h2></header><section class=entry-content><p>Preliminaries linear classifier An Introduction to Neural Networks Theory and Notation1 We are not able to build any artificial cells up to now. It seems impossible to build a neuron network through biological materials manually, either. To investigate the ability of neurons we have built mathematical models of the neuron. These models have been assigned a number of neuron-like properties. However, there must be a balance between the number of properties contained by the mathematical models and the current computational abilities of the machines....</p></section><footer class=entry-footer><span title="2019-12-10 10:54:57 +0000 UTC">December 10, 2019</span>&nbsp;·&nbsp;<span title="2022-04-29 16:38:28 +0800 +0800">(Last Modification: April 29, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Neuron Model and Network Architecture" href=https://anthony-tan.com/Neuron-Model-and-Network-Architecture/></a></article><article class=post-entry><header class=entry-header><h2>An Introduction to Neural Networks</h2></header><section class=entry-content><p>Preliminaries Nothing Neural Networks1 Neural Networks are a model of our brain that is built with neurons and is considered the source of intelligence. There are almost \(10^{11}\) neurons in the human brain and \(10^4\) connections of each neuron to other neurons. Some of these brilliant structures were given when we were born. Some other structures could be established by experience, and this progress is called learning. Learning is also considered as the establishment or modification of the connections between neurons....</p></section><footer class=entry-footer><span title="2019-12-08 19:01:32 +0000 UTC">December 8, 2019</span>&nbsp;·&nbsp;<span title="2022-04-30 21:21:04 +0800 +0800">(Last Modification: April 30, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to An Introduction to Neural Networks" href=https://anthony-tan.com/An-Introduction-to-Neural-Networks/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://anthony-tan.com/deep_learning/>« Prev Page</a></nav></footer></main><footer class=footer><span>&copy; 2022 <a href=https://anthony-tan.com>Anthony's Blogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>