<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>K-means Clustering | Anthony's Blogs</title><meta name=keywords content="Machine Learning,Mixture Models,Clustering,unsupervised learning,1-of-K coding scheme,Fisher Linear Discriminant,K-means algorithm"><meta name=description content="Preliminaries Numerical Optimization  necessary conditions for maximum  K-means algorithm Fisher Linear Discriminant  Clustering Problem1 The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.
Clustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete."><meta name=author content="Anthony Tan"><link rel=canonical href=https://anthony-tan.com/K-means-Clustering/><link crossorigin=anonymous href=../assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://anthony-tan.com/logo.png><link rel=apple-touch-icon href=https://anthony-tan.com/logo.png><link rel=mask-icon href=https://anthony-tan.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-12","auto"),ga("send","pageview"))</script><meta property="og:title" content="K-means Clustering"><meta property="og:description" content="Preliminaries Numerical Optimization  necessary conditions for maximum  K-means algorithm Fisher Linear Discriminant  Clustering Problem1 The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.
Clustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete."><meta property="og:type" content="article"><meta property="og:url" content="https://anthony-tan.com/K-means-Clustering/"><meta property="article:section" content="machine_learning"><meta property="article:published_time" content="2020-03-04T22:08:03+00:00"><meta property="article:modified_time" content="2022-04-28T16:11:17+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="K-means Clustering"><meta name=twitter:description content="Preliminaries Numerical Optimization  necessary conditions for maximum  K-means algorithm Fisher Linear Discriminant  Clustering Problem1 The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.
Clustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Machine Learning","item":"https://anthony-tan.com/machine_learning/"},{"@type":"ListItem","position":3,"name":"K-means Clustering","item":"https://anthony-tan.com/K-means-Clustering/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"K-means Clustering","name":"K-means Clustering","description":"Preliminaries Numerical Optimization  necessary conditions for maximum  K-means algorithm Fisher Linear Discriminant  Clustering Problem1 The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.\nClustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete.","keywords":["Machine Learning","Mixture Models","Clustering","unsupervised learning","1-of-K coding scheme","Fisher Linear Discriminant","K-means algorithm"],"articleBody":"Preliminaries Numerical Optimization  necessary conditions for maximum  K-means algorithm Fisher Linear Discriminant  Clustering Problem1 The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.\nClustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete. However, during training classifiers, we always have a certain target corresponding to every input. On the contrary, clustering has no target at all, and what we have is only\n\\[ \\{x_1,\\cdots, x_N\\}\\tag{1} \\]\nwhere \\(x_i\\in\\Re^D\\) for \\(i=1,2,\\cdots,N\\). And our mission is to separate the dataset into \\(K\\) groups where \\(K\\) has been given before task\nAn intuitive strategy of clustering is based on two considerations: 1. the distance between data points in the same group should be as small as possible. 2. the distance between data points in the different groups should be as large as possible.\nThis is a little like Fisher Linear Discriminant. Based on these two points, some concepts could be formed.\nThe first one is how to represent a group. We take\n\\[ \\mu_i:i\\in\\{1,2,\\cdots, K\\}\\tag{2} \\]\nas the prototype associated with \\(i\\) th group. A group always contains several points, and a spontaneous idea is using the center of all the points belonging to one group as its prototype. To represent which group \\(\\mathbf{x}_i\\) in equation (1) belongs to, an indicator is necessary, and a 1-of-K coding scheme is used:\n\\[ r_{nk}\\in\\{0,1\\}\\tag{3} \\]\nfor \\(k=1,2,\\cdots,K\\) representing the group number and \\(n = 1,2,\\cdots,N\\) denoting the number of sample point, and where \\(r_{nk}=1\\) then \\(r_{nj}=0\\) for all \\(j\\neq k\\).\nObjective Function A loss function is a good way to measure the quantity of our model during both the training and testing stages. And in the clustering task loss function could not be used because we have no idea about what is correct. However, we can build another function that plays the same role as the loss function and it is also the target of what we want to optimize.\nAccording to the two base points above, we build our objective function:\n\\[ J=\\sum_{n=1}^{N}\\sum_{k=1}^{K}r_{nk}||\\mathbf{x}_n-\\mu_k||^2\\tag{4} \\]\nIn this objective function, the distance is defined as Euclidean distance(However, other measurements of similarity could also be used). Then the mission is to minimize \\(J\\) by finding some certain \\(\\{r_{nk}\\}\\) and \\(\\{\\mu_k\\}\\)\nK-Means Algorithm Now, letâ€™s represent the famous K-Means algorithm. The method includes two steps:\nMinimising \\(J\\) respect to \\(r_{nk}\\) keeping \\(\\mu_k\\) fixed Minimising \\(J\\) respect to \\(\\mu_k\\) keeping \\(r_{nk}\\) fixed  In the first step, according to equation (4), the objective function is linear of \\(r_{nk}\\). So there is a close solution. Then we set:\n\\[ r_{nk}=\\begin{cases} 1\u0026\\text{ if } k=\\arg\\min_{j}||x_n-\\mu_j||^2\\\\ 0\u0026\\text{otherwise} \\end{cases}\\tag{5} \\]\nAnd in the second step, \\(r_{nk}\\) is fixed and we minimize objective function \\(J\\). For it is quadratic, the minimum point is on the stationary point where:\n\\[ \\frac{\\partial J}{\\partial \\mu_k}=-\\sum_{n=1}^{N}r_{nk}(x_n-\\mu_k)=0\\tag{6} \\]\nand we get:\n\\[ \\mu_k = \\frac{\\sum_{n=1}^{N}r_{nk}x_n}{\\sum_{n=1}^{N}r_{nk}}\\tag{7} \\]\n\\(\\sum_{n=1}^{N} r_{nk}\\) is the total number of points from the sample \\(\\{x_1,\\cdots, x_N\\}\\) who belong to prototype \\(\\mu_k\\) or group \\(k\\) at current step. And \\(\\mu_k\\) is just the average of all the points in the group \\(k\\).\nThis two-step, which was calculated by equation (5),(7), would repeat until \\(r_{nk}\\) and \\(\\mu_k\\) not change.\nThe K-means algorithm guarantees to converge because at every step the objective function \\(J\\) is reduced. So when there is only one minimum, the global minimum, the algorithm must converge.\nInput Data Preprocessing before K-means Most algorithms need their input data to obey some rules. To the K-means algorithm, we rescale the input data to mean 0 and variance 1. This is always done by\n\\[ x_n^{(i)} = \\frac{x_n^{(i)}- \\bar{x}^{(i)}}{\\delta^{i}} \\]\nwhere \\(x_n^{(i)}\\) is the \\(i\\) th component of the \\(n\\) th data point, and \\(x_n\\) comes from equation (1), \\(\\bar{x}^{(i)}\\) and \\(\\delta^{i}\\) is the \\(i\\) th mean and standard deviation repectively\nPython code of K-means  class K_Means():  \"\"\" input data should be normalized: mean 0, variance 1 \"\"\"  def clustering(self, x, K):  \"\"\" :param x: inputs :param K: how many groups :return: prototype(center of each group), r_nk, which group k does the n th point belong to \"\"\"  data_point_dimension = x.shape[1]  data_point_size = x.shape[0]  center_matrix = np.zeros((K, data_point_dimension))  for i in range(len(center_matrix)):  center_matrix[i] = x[np.random.randint(0, len(x)-1)]   center_matrix_last_time = np.zeros((K, data_point_dimension))  cluster_for_each_point = np.zeros(data_point_size, dtype=np.int32)  # -----------------------------------visualization-----------------------------------  # the part can be deleted  center_color = np.random.randint(0,1000, (K, 3))/1000.  plt.scatter(x[:, 0], x[:, 1], color='green', s=30, marker='o', alpha=0.3)  for i in range(len(center_matrix)):  plt.scatter(center_matrix[i][0], center_matrix[i][1], marker='x', s=65, color=center_color[i])  plt.show()  # -----------------------------------------------------------------------------------  while (center_matrix_last_time-center_matrix).all() != 0:  # E step  for i in range(len(x)):  distance_to_center = np.zeros(K)  for k in range(K):  distance_to_center[k] = (center_matrix[k]-x[i]).dot((center_matrix[k]-x[i]))  cluster_for_each_point[i] = int(np.argmin(distance_to_center))  # M step  number_of_point_in_k = np.zeros(K)  center_matrix_last_time = center_matrix  center_matrix = np.zeros((K, data_point_dimension))  for i in range(len(x)):  center_matrix[cluster_for_each_point[i]] += x[i]  number_of_point_in_k[cluster_for_each_point[i]] += 1   for i in range(len(center_matrix)):  if number_of_point_in_k[i] != 0:  center_matrix[i] /= number_of_point_in_k[i]  # -----------------------------------visualization-----------------------------------  # the part can be deleted  print(center_matrix)  plt.cla()  for i in range(len(center_matrix)):  plt.scatter(center_matrix[i][0], center_matrix[i][1], marker='x', s=65, color=center_color[i])  for i in range(len(x)):  plt.scatter(x[i][0], x[i][1], marker='o',s=30, color=center_color[cluster_for_each_point[i]],alpha=0.7)  plt.show()  # -----------------------------------------------------------------------------------  return center_matrix, cluster_for_each_point and the entire project can be found : https://github.com/Tony-Tan/ML and please star me(^_^).\nResults during K-means We use a tool https://github.com/Tony-Tan/2DRandomSampleGenerater to generate the input data from:\nThere are two classes the brown circle and the green circle. Then the K-means algorithm initial two prototypes, the centers of groups, randomly:\nthe two crosses represent the initial centers \\(\\mu_i\\). And then we iterate the two steps:\n Iteration 1\n  Iteration 2\n  Iteration 3\n  Iteration 4\n The result of iterations 3 and 4 do not vary for both objective function value \\(J\\) and parameters. Then the algorithm stopped.\nAnd different initial centers may have different convergence speeds, but they always have the same stop positions.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.â†©ï¸Ž\n   ","wordCount":"989","inLanguage":"en","datePublished":"2020-03-04T22:08:03Z","dateModified":"2022-04-28T16:11:17+08:00","author":{"@type":"Person","name":"Anthony Tan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://anthony-tan.com/K-means-Clustering/"},"publisher":{"@type":"Organization","name":"Anthony's Blogs","logo":{"@type":"ImageObject","url":"https://anthony-tan.com/logo.png"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://anthony-tan.com accesskey=h title="Anthony's Blogs (Alt + H)">Anthony's Blogs</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://anthony-tan.com/machine_learning/ title="Machine Learning"><span>Machine Learning</span></a></li><li><a href=https://anthony-tan.com/deep_learning/ title="Deep Learning"><span>Deep Learning</span></a></li><li><a href=https://anthony-tan.com/reinforcement_learning/ title="Reinforcement Learning"><span>Reinforcement Learning</span></a></li><li><a href=https://anthony-tan.com/math/ title=Math><span>Math</span></a></li><li><a href=https://anthony-tan.com/others/ title=Others><span>Others</span></a></li><li><a href=https://anthony-tan.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://anthony-tan.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://anthony-tan.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://anthony-tan.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://anthony-tan.com>Home</a>&nbsp;Â»&nbsp;<a href=https://anthony-tan.com/machine_learning/>Machine Learning</a></div><h1 class=post-title>K-means Clustering</h1><div class=post-meta><span title="2020-03-04 22:08:03 +0000 UTC">March 4, 2020</span>&nbsp;Â·&nbsp;<span title="2022-04-28 16:11:17 +0800 +0800">(Last Modification: April 28, 2022)</span>&nbsp;Â·&nbsp;Anthony Tan</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#preliminaries aria-label=Preliminaries>Preliminaries</a></li><li><a href=#clustering-problem1 aria-label="Clustering Problem1">Clustering Problem<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a></a></li><li><a href=#objective-function aria-label="Objective Function">Objective Function</a></li><li><a href=#k-means-algorithm aria-label="K-Means Algorithm">K-Means Algorithm</a><ul><li><a href=#input-data-preprocessing-before-k-means aria-label="Input Data Preprocessing before K-means">Input Data Preprocessing before K-means</a></li><li><a href=#python-code-of-k-means aria-label="Python code of K-means">Python code of K-means</a></li><li><a href=#results-during-k-means aria-label="Results during K-means">Results during K-means</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=preliminaries>Preliminaries<a hidden class=anchor aria-hidden=true href=#preliminaries>#</a></h2><ol type=1><li>Numerical Optimization<ul><li>necessary conditions for maximum</li></ul></li><li><a href=https://anthony-tan.com/K-means-Clustering/>K-means algorithm</a></li><li><a href=https://anthony-tan.com/Fisher-Linear-Discriminant/>Fisher Linear Discriminant</a></li></ol><h2 id=clustering-problem1>Clustering Problem<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a><a hidden class=anchor aria-hidden=true href=#clustering-problem1>#</a></h2><p>The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.</p><p>Clustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete. However, during training classifiers, we always have a certain target corresponding to every input. On the contrary, clustering has no target at all, and what we have is only</p><p><span class="math display">\[
\{x_1,\cdots, x_N\}\tag{1}
\]</span></p><p>where <span class="math inline">\(x_i\in\Re^D\)</span> for <span class="math inline">\(i=1,2,\cdots,N\)</span>. And our mission is to separate the dataset into <span class="math inline">\(K\)</span> groups where <span class="math inline">\(K\)</span> has been given before task</p><p>An intuitive strategy of clustering is based on two considerations: 1. the distance between data points in the same group should be as small as possible. 2. the distance between data points in the different groups should be as large as possible.</p><p>This is a little like <a href=https://anthony-tan.com/Fisher-Linear-Discriminant/>Fisher Linear Discriminant</a>. Based on these two points, some concepts could be formed.</p><p>The first one is how to represent a group. We take</p><p><span class="math display">\[
\mu_i:i\in\{1,2,\cdots, K\}\tag{2}
\]</span></p><p>as the prototype associated with <span class="math inline">\(i\)</span> th group. A group always contains several points, and a spontaneous idea is using the center of all the points belonging to one group as its prototype. To represent which group <span class="math inline">\(\mathbf{x}_i\)</span> in equation (1) belongs to, an indicator is necessary, and a 1-of-K coding scheme is used:</p><p><span class="math display">\[
r_{nk}\in\{0,1\}\tag{3}
\]</span></p><p>for <span class="math inline">\(k=1,2,\cdots,K\)</span> representing the group number and <span class="math inline">\(n = 1,2,\cdots,N\)</span> denoting the number of sample point, and where <span class="math inline">\(r_{nk}=1\)</span> then <span class="math inline">\(r_{nj}=0\)</span> for all <span class="math inline">\(j\neq k\)</span>.</p><h2 id=objective-function>Objective Function<a hidden class=anchor aria-hidden=true href=#objective-function>#</a></h2><p>A loss function is a good way to measure the quantity of our model during both the training and testing stages. And in the clustering task loss function could not be used because we have no idea about what is correct. However, we can build another function that plays the same role as the loss function and it is also the target of what we want to optimize.</p><p>According to the two base points above, we build our objective function:</p><p><span class="math display">\[
J=\sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}||\mathbf{x}_n-\mu_k||^2\tag{4}
\]</span></p><p>In this objective function, the distance is defined as Euclidean distance(However, other measurements of similarity could also be used). Then the mission is to minimize <span class="math inline">\(J\)</span> by finding some certain <span class="math inline">\(\{r_{nk}\}\)</span> and <span class="math inline">\(\{\mu_k\}\)</span></p><h2 id=k-means-algorithm>K-Means Algorithm<a hidden class=anchor aria-hidden=true href=#k-means-algorithm>#</a></h2><p>Now, letâ€™s represent the famous K-Means algorithm. The method includes two steps:</p><ol type=1><li>Minimising <span class="math inline">\(J\)</span> respect to <span class="math inline">\(r_{nk}\)</span> keeping <span class="math inline">\(\mu_k\)</span> fixed</li><li>Minimising <span class="math inline">\(J\)</span> respect to <span class="math inline">\(\mu_k\)</span> keeping <span class="math inline">\(r_{nk}\)</span> fixed</li></ol><p>In the first step, according to equation (4), the objective function is linear of <span class="math inline">\(r_{nk}\)</span>. So there is a close solution. Then we set:</p><p><span class="math display">\[
r_{nk}=\begin{cases}
1&\text{ if } k=\arg\min_{j}||x_n-\mu_j||^2\\
0&\text{otherwise}
\end{cases}\tag{5}
\]</span></p><p>And in the second step, <span class="math inline">\(r_{nk}\)</span> is fixed and we minimize objective function <span class="math inline">\(J\)</span>. For it is quadratic, the minimum point is on the stationary point where:</p><p><span class="math display">\[
\frac{\partial J}{\partial \mu_k}=-\sum_{n=1}^{N}r_{nk}(x_n-\mu_k)=0\tag{6}
\]</span></p><p>and we get:</p><p><span class="math display">\[
\mu_k = \frac{\sum_{n=1}^{N}r_{nk}x_n}{\sum_{n=1}^{N}r_{nk}}\tag{7}
\]</span></p><p><span class="math inline">\(\sum_{n=1}^{N} r_{nk}\)</span> is the total number of points from the sample <span class="math inline">\(\{x_1,\cdots, x_N\}\)</span> who belong to prototype <span class="math inline">\(\mu_k\)</span> or group <span class="math inline">\(k\)</span> at current step. And <span class="math inline">\(\mu_k\)</span> is just the average of all the points in the group <span class="math inline">\(k\)</span>.</p><p>This two-step, which was calculated by equation (5),(7), would repeat until <span class="math inline">\(r_{nk}\)</span> and <span class="math inline">\(\mu_k\)</span> not change.</p><p>The K-means algorithm guarantees to converge because at every step the objective function <span class="math inline">\(J\)</span> is reduced. So when there is only one minimum, the global minimum, the algorithm must converge.</p><h3 id=input-data-preprocessing-before-k-means>Input Data Preprocessing before K-means<a hidden class=anchor aria-hidden=true href=#input-data-preprocessing-before-k-means>#</a></h3><p>Most algorithms need their input data to obey some rules. To the K-means algorithm, we rescale the input data to mean 0 and variance 1. This is always done by</p><p><span class="math display">\[
x_n^{(i)} = \frac{x_n^{(i)}- \bar{x}^{(i)}}{\delta^{i}}
\]</span></p><p>where <span class="math inline">\(x_n^{(i)}\)</span> is the <span class="math inline">\(i\)</span> th component of the <span class="math inline">\(n\)</span> th data point, and <span class="math inline">\(x_n\)</span> comes from equation (1), <span class="math inline">\(\bar{x}^{(i)}\)</span> and <span class="math inline">\(\delta^{i}\)</span> is the <span class="math inline">\(i\)</span> th mean and standard deviation repectively</p><h3 id=python-code-of-k-means>Python code of K-means<a hidden class=anchor aria-hidden=true href=#python-code-of-k-means>#</a></h3><div class=sourceCode id=cb1><pre class="sourceCode python"><code class="sourceCode python"><span id=cb1-1><a href=#cb1-1></a></span>
<span id=cb1-2><a href=#cb1-2></a><span class=kw>class</span> K_Means():</span>
<span id=cb1-3><a href=#cb1-3></a>    <span class=co>&quot;&quot;&quot;</span></span>
<span id=cb1-4><a href=#cb1-4></a><span class=co>    input data should be normalized: mean 0, variance 1</span></span>
<span id=cb1-5><a href=#cb1-5></a><span class=co>    &quot;&quot;&quot;</span></span>
<span id=cb1-6><a href=#cb1-6></a>    <span class=kw>def</span> clustering(<span class=va>self</span>, x, K):</span>
<span id=cb1-7><a href=#cb1-7></a>        <span class=co>&quot;&quot;&quot;</span></span>
<span id=cb1-8><a href=#cb1-8></a><span class=co>        :param x: inputs</span></span>
<span id=cb1-9><a href=#cb1-9></a><span class=co>        :param K: how many groups</span></span>
<span id=cb1-10><a href=#cb1-10></a><span class=co>        :return: prototype(center of each group), r_nk, which group k does the n th point belong to</span></span>
<span id=cb1-11><a href=#cb1-11></a><span class=co>        &quot;&quot;&quot;</span></span>
<span id=cb1-12><a href=#cb1-12></a>        data_point_dimension <span class=op>=</span> x.shape[<span class=dv>1</span>]</span>
<span id=cb1-13><a href=#cb1-13></a>        data_point_size <span class=op>=</span> x.shape[<span class=dv>0</span>]</span>
<span id=cb1-14><a href=#cb1-14></a>        center_matrix <span class=op>=</span> np.zeros((K, data_point_dimension))</span>
<span id=cb1-15><a href=#cb1-15></a>        <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(<span class=bu>len</span>(center_matrix)):</span>
<span id=cb1-16><a href=#cb1-16></a>            center_matrix[i] <span class=op>=</span> x[np.random.randint(<span class=dv>0</span>, <span class=bu>len</span>(x)<span class=op>-</span><span class=dv>1</span>)]</span>
<span id=cb1-17><a href=#cb1-17></a></span>
<span id=cb1-18><a href=#cb1-18></a>        center_matrix_last_time <span class=op>=</span> np.zeros((K, data_point_dimension))</span>
<span id=cb1-19><a href=#cb1-19></a>        cluster_for_each_point <span class=op>=</span> np.zeros(data_point_size, dtype<span class=op>=</span>np.int32)</span>
<span id=cb1-20><a href=#cb1-20></a>        <span class=co># -----------------------------------visualization-----------------------------------</span></span>
<span id=cb1-21><a href=#cb1-21></a>        <span class=co># the part can be deleted</span></span>
<span id=cb1-22><a href=#cb1-22></a>        center_color <span class=op>=</span> np.random.randint(<span class=dv>0</span>,<span class=dv>1000</span>, (K, <span class=dv>3</span>))<span class=op>/</span><span class=fl>1000.</span></span>
<span id=cb1-23><a href=#cb1-23></a>        plt.scatter(x[:, <span class=dv>0</span>], x[:, <span class=dv>1</span>], color<span class=op>=</span><span class=st>&#39;green&#39;</span>, s<span class=op>=</span><span class=dv>30</span>, marker<span class=op>=</span><span class=st>&#39;o&#39;</span>, alpha<span class=op>=</span><span class=fl>0.3</span>)</span>
<span id=cb1-24><a href=#cb1-24></a>        <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(<span class=bu>len</span>(center_matrix)):</span>
<span id=cb1-25><a href=#cb1-25></a>            plt.scatter(center_matrix[i][<span class=dv>0</span>], center_matrix[i][<span class=dv>1</span>],  marker<span class=op>=</span><span class=st>&#39;x&#39;</span>, s<span class=op>=</span><span class=dv>65</span>, color<span class=op>=</span>center_color[i])</span>
<span id=cb1-26><a href=#cb1-26></a>        plt.show()</span>
<span id=cb1-27><a href=#cb1-27></a>        <span class=co># -----------------------------------------------------------------------------------</span></span>
<span id=cb1-28><a href=#cb1-28></a>        <span class=cf>while</span> (center_matrix_last_time<span class=op>-</span>center_matrix).<span class=bu>all</span>() <span class=op>!=</span> <span class=dv>0</span>:</span>
<span id=cb1-29><a href=#cb1-29></a>            <span class=co># E step</span></span>
<span id=cb1-30><a href=#cb1-30></a>            <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(<span class=bu>len</span>(x)):</span>
<span id=cb1-31><a href=#cb1-31></a>                distance_to_center <span class=op>=</span> np.zeros(K)</span>
<span id=cb1-32><a href=#cb1-32></a>                <span class=cf>for</span> k <span class=kw>in</span> <span class=bu>range</span>(K):</span>
<span id=cb1-33><a href=#cb1-33></a>                    distance_to_center[k] <span class=op>=</span> (center_matrix[k]<span class=op>-</span>x[i]).dot((center_matrix[k]<span class=op>-</span>x[i]))</span>
<span id=cb1-34><a href=#cb1-34></a>                cluster_for_each_point[i] <span class=op>=</span> <span class=bu>int</span>(np.argmin(distance_to_center))</span>
<span id=cb1-35><a href=#cb1-35></a>            <span class=co># M step</span></span>
<span id=cb1-36><a href=#cb1-36></a>            number_of_point_in_k <span class=op>=</span> np.zeros(K)</span>
<span id=cb1-37><a href=#cb1-37></a>            center_matrix_last_time <span class=op>=</span> center_matrix</span>
<span id=cb1-38><a href=#cb1-38></a>            center_matrix <span class=op>=</span> np.zeros((K, data_point_dimension))</span>
<span id=cb1-39><a href=#cb1-39></a>            <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(<span class=bu>len</span>(x)):</span>
<span id=cb1-40><a href=#cb1-40></a>                center_matrix[cluster_for_each_point[i]] <span class=op>+=</span> x[i]</span>
<span id=cb1-41><a href=#cb1-41></a>                number_of_point_in_k[cluster_for_each_point[i]] <span class=op>+=</span> <span class=dv>1</span></span>
<span id=cb1-42><a href=#cb1-42></a></span>
<span id=cb1-43><a href=#cb1-43></a>            <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(<span class=bu>len</span>(center_matrix)):</span>
<span id=cb1-44><a href=#cb1-44></a>                <span class=cf>if</span> number_of_point_in_k[i] <span class=op>!=</span> <span class=dv>0</span>:</span>
<span id=cb1-45><a href=#cb1-45></a>                    center_matrix[i] <span class=op>/=</span> number_of_point_in_k[i]</span>
<span id=cb1-46><a href=#cb1-46></a>            <span class=co># -----------------------------------visualization-----------------------------------</span></span>
<span id=cb1-47><a href=#cb1-47></a>            <span class=co># the part can be deleted</span></span>
<span id=cb1-48><a href=#cb1-48></a>            <span class=bu>print</span>(center_matrix)</span>
<span id=cb1-49><a href=#cb1-49></a>            plt.cla()</span>
<span id=cb1-50><a href=#cb1-50></a>            <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(<span class=bu>len</span>(center_matrix)):</span>
<span id=cb1-51><a href=#cb1-51></a>                plt.scatter(center_matrix[i][<span class=dv>0</span>], center_matrix[i][<span class=dv>1</span>], marker<span class=op>=</span><span class=st>&#39;x&#39;</span>, s<span class=op>=</span><span class=dv>65</span>,  color<span class=op>=</span>center_color[i])</span>
<span id=cb1-52><a href=#cb1-52></a>            <span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(<span class=bu>len</span>(x)):</span>
<span id=cb1-53><a href=#cb1-53></a>                plt.scatter(x[i][<span class=dv>0</span>], x[i][<span class=dv>1</span>], marker<span class=op>=</span><span class=st>&#39;o&#39;</span>,s<span class=op>=</span><span class=dv>30</span>, color<span class=op>=</span>center_color[cluster_for_each_point[i]],alpha<span class=op>=</span><span class=fl>0.7</span>)</span>
<span id=cb1-54><a href=#cb1-54></a>            plt.show()</span>
<span id=cb1-55><a href=#cb1-55></a>            <span class=co># -----------------------------------------------------------------------------------</span></span>
<span id=cb1-56><a href=#cb1-56></a>        <span class=cf>return</span> center_matrix, cluster_for_each_point</span></code></pre></div><p>and the entire project can be found : <a href=https://github.com/Tony-Tan/ML>https://github.com/Tony-Tan/ML</a> and please star me(^_^).</p><h3 id=results-during-k-means>Results during K-means<a hidden class=anchor aria-hidden=true href=#results-during-k-means>#</a></h3><p>We use a tool <a href=https://github.com/Tony-Tan/2DRandomSampleGenerater>https://github.com/Tony-Tan/2DRandomSampleGenerater</a> to generate the input data from:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_27_21_46_generator.png></p><p>There are two classes the brown circle and the green circle. Then the K-means algorithm initial two prototypes, the centers of groups, randomly:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_27_21_46_1.png></p><p>the two crosses represent the initial centers <span class="math inline">\(\mu_i\)</span>. And then we iterate the two steps:</p><blockquote><p>Iteration 1</p></blockquote><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_27_21_46_2.png></p><blockquote><p>Iteration 2</p></blockquote><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_27_21_47_3.png></p><blockquote><p>Iteration 3</p></blockquote><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_27_21_47_4.png></p><blockquote><p>Iteration 4</p></blockquote><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_27_21_47_5.png></p><p>The result of iterations 3 and 4 do not vary for both objective function value <span class="math inline">\(J\)</span> and parameters. Then the algorithm stopped.</p><p>And different initial centers may have different convergence speeds, but they always have the same stop positions.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><section class=footnotes role=doc-endnotes><hr><ol><li id=fn1 role=doc-endnote><p>Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.<a href=#fnref1 class=footnote-back role=doc-backlink>â†©ï¸Ž</a></p></li></ol></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://anthony-tan.com/tags/machine-learning/>machine learning</a></li><li><a href=https://anthony-tan.com/tags/mixture-models/>mixture models</a></li><li><a href=https://anthony-tan.com/tags/clustering/>clustering</a></li><li><a href=https://anthony-tan.com/tags/unsupervised-learning/>unsupervised learning</a></li><li><a href=https://anthony-tan.com/tags/1-of-k-coding-scheme/>1-of-K coding scheme</a></li><li><a href=https://anthony-tan.com/tags/fisher-linear-discriminant/>Fisher linear discriminant</a></li><li><a href=https://anthony-tan.com/tags/k-means-algorithm/>K-means algorithm</a></li></ul><nav class=paginav><a class=prev href=https://anthony-tan.com/Mixtures-of-Gaussians/><span class=title>Â« Prev Page</span><br><span>Mixtures of Gaussians</span></a>
<a class=next href=https://anthony-tan.com/An-Introduction-to-Mixture-Models/><span class=title>Next Page Â»</span><br><span>An Introduction to Mixture Models</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share K-means Clustering on twitter" href="https://twitter.com/intent/tweet/?text=K-means%20Clustering&url=https%3a%2f%2fanthony-tan.com%2fK-means-Clustering%2f&hashtags=MachineLearning%2cMixtureModels%2cClustering%2cunsupervisedlearning%2c1-of-Kcodingscheme%2cFisherLinearDiscriminant%2cK-meansalgorithm"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share K-means Clustering on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fanthony-tan.com%2fK-means-Clustering%2f&title=K-means%20Clustering&summary=K-means%20Clustering&source=https%3a%2f%2fanthony-tan.com%2fK-means-Clustering%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share K-means Clustering on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fanthony-tan.com%2fK-means-Clustering%2f&title=K-means%20Clustering"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share K-means Clustering on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fanthony-tan.com%2fK-means-Clustering%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share K-means Clustering on whatsapp" href="https://api.whatsapp.com/send?text=K-means%20Clustering%20-%20https%3a%2f%2fanthony-tan.com%2fK-means-Clustering%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share K-means Clustering on telegram" href="https://telegram.me/share/url?text=K-means%20Clustering&url=https%3a%2f%2fanthony-tan.com%2fK-means-Clustering%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><figure class=article-discussion><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//anthony-tan-com.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></figure></article></main><footer class=footer><span>&copy; 2022 <a href=https://anthony-tan.com>Anthony's Blogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>