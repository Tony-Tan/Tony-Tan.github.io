<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Neuron Model and Network Architecture | Anthony's Blogs</title><meta name=keywords content="Artificial Neural Networks,Artificial Intelligence,Hard Limit Transfer Function,artificial neuron,Transfer Functions,Single Neuron Model,threshold function,active function,Log-sigmoid Transfer Function,Linear Transfer Function,Recurrent Networks,Multiple-inputs Neuron"><meta name=description content="Preliminaries linear classifier An Introduction to Neural Networks  Theory and Notation1 We are not able to build any artificial cells up to now. It seems impossible to build a neuron network through biological materials manually, either. To investigate the ability of neurons we have built mathematical models of the neuron. These models have been assigned a number of neuron-like properties. However, there must be a balance between the number of properties contained by the mathematical models and the current computational abilities of the machines."><meta name=author content="Anthony Tan"><link rel=canonical href=https://anthony-tan.com/Neuron-Model-and-Network-Architecture/><link crossorigin=anonymous href=../assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://anthony-tan.com/logo.png><link rel=apple-touch-icon href=https://anthony-tan.com/logo.png><link rel=mask-icon href=https://anthony-tan.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-12","auto"),ga("send","pageview"))</script><meta property="og:title" content="Neuron Model and Network Architecture"><meta property="og:description" content="Preliminaries linear classifier An Introduction to Neural Networks  Theory and Notation1 We are not able to build any artificial cells up to now. It seems impossible to build a neuron network through biological materials manually, either. To investigate the ability of neurons we have built mathematical models of the neuron. These models have been assigned a number of neuron-like properties. However, there must be a balance between the number of properties contained by the mathematical models and the current computational abilities of the machines."><meta property="og:type" content="article"><meta property="og:url" content="https://anthony-tan.com/Neuron-Model-and-Network-Architecture/"><meta property="article:section" content="deep_learning"><meta property="article:published_time" content="2019-12-10T10:54:57+00:00"><meta property="article:modified_time" content="2022-04-29T16:38:28+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Neuron Model and Network Architecture"><meta name=twitter:description content="Preliminaries linear classifier An Introduction to Neural Networks  Theory and Notation1 We are not able to build any artificial cells up to now. It seems impossible to build a neuron network through biological materials manually, either. To investigate the ability of neurons we have built mathematical models of the neuron. These models have been assigned a number of neuron-like properties. However, there must be a balance between the number of properties contained by the mathematical models and the current computational abilities of the machines."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Deep Learning","item":"https://anthony-tan.com/deep_learning/"},{"@type":"ListItem","position":3,"name":"Neuron Model and Network Architecture","item":"https://anthony-tan.com/Neuron-Model-and-Network-Architecture/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Neuron Model and Network Architecture","name":"Neuron Model and Network Architecture","description":"Preliminaries linear classifier An Introduction to Neural Networks  Theory and Notation1 We are not able to build any artificial cells up to now. It seems impossible to build a neuron network through biological materials manually, either. To investigate the ability of neurons we have built mathematical models of the neuron. These models have been assigned a number of neuron-like properties. However, there must be a balance between the number of properties contained by the mathematical models and the current computational abilities of the machines.","keywords":["Artificial Neural Networks","Artificial Intelligence","Hard Limit Transfer Function","artificial neuron","Transfer Functions","Single Neuron Model","threshold function","active function","Log-sigmoid Transfer Function","Linear Transfer Function","Recurrent Networks","Multiple-inputs Neuron"],"articleBody":"Preliminaries linear classifier An Introduction to Neural Networks  Theory and Notation1 We are not able to build any artificial cells up to now. It seems impossible to build a neuron network through biological materials manually, either. To investigate the ability of neurons we have built mathematical models of the neuron. These models have been assigned a number of neuron-like properties. However, there must be a balance between the number of properties contained by the mathematical models and the current computational abilities of the machines.\nFrom now on, we begin our study of the neuron network, and it looks like a good idea, to begin with, the simplest but basic model – artificial neuron. Because tons of network architectures are built on these simple neurons.\nSingle Neuron Model Let’s begin with the simplest neuron, which has only one input, one synapse which is represented by weight, a bias, a threshold operation which is expressed by a transfer function, and an output.\nWe know that the cell body of neurons plays summation and threshold operation. This is very like a linear classifier\nThen our simplest model is constructed as follows:\nSynapse is represented by a scalar which is called a weight, it will be multiplied by the input as a received signal, and then the signal is transferred to the cell body Cell body here is represented by two functional properties property:  the first one is summation which is used to collect all signals in a short time interval, while in this naive example only one input is concerned so it looks redundant but in the following models, it is an important operation of a neuron; the second function is a threshold operation that acts as a gatekeeper, only the signal stronger than some value could excite this neuron. Only excited neurons could pass signals to the other neurons connected to them.  and a scalar  The scalar property represents an original faint signal of the neuron. From a biological point, it makes sense because every nerve cell has its resting membrane potential(RMP).  Axon is expressed by an output that is produced by the threshold function. It can be any form in a biological neuron, like amplitude or frequency, but here it can just be a number that is decided by the selected threshold function.  The threshold function is called an active function or transfer functions officially. And it will be listed in the next section.\nLet’s review the single input neuron model and its components:\n \\(P\\): input signal, a scalar or vector, coming from a previous nerve cell or external signal \\(w\\): weight, a scalar or vector, coming from the synapse, act as the strength of a synapse \\(b\\): bias, a scalar, a property of this neuron \\(f\\): transfer function, act as a gatekeeper, perform a threshold operation \\(a\\): the output of the neuron, a scalar, can be a signal to the next neuron or as a final output to the external system.  The final mathematical expression is:\n\\[ a=f(w\\cdot P + b)\\tag{1} \\]\nFor instance, we have input \\(P=2.0\\), synapse weight \\(w=3.0\\), nerve cell bias \\(b=-1.5\\) and then we get the output:\n\\[ a= f(2.0\\times 3.0 + (-1.5))=f(4.5)\\tag{2} \\]\nhowever, bias can be omitted, or be rewritten as a special input and weight combination:\n\\[ a=f(w_1\\cdot P + w_0\\cdot 1)\\text{ where } w_1=w \\text{ and }w_0=b\\tag{3} \\]\n\\(w\\) and \\(b\\) came from equation(1). In the model, \\(w\\) and \\(b\\) are adjustable. And the ideal procedure is: 1. computing summation of all weighted inputs 2. select a transfer function 3. put the result of step 1 into the selected function from step 2 and get a final output of the neuron 4. using the learning rule to adjust \\(w\\) and \\(b\\) to adapt the task which is our purpose.\nTransfer Functions Every part of the neuron no matter the biological or mathematical one directly affects the function of the neuron. And this also makes the design of neurons more interesting, because we can build different kinds of neurons to simulate different kinds of operations. And this also provides sufficient basic blocks for us to develop a more complicated network.\nLet’s recall the single input model above, the components are \\(w\\), \\(b\\), \\(\\sum\\), and \\(f\\), however, \\(w\\) and \\(b\\) are objectives of learning, and \\(\\sum\\) is relatively stable which is hard to be replaced by any other operations. So, we move our attention to the threshold operation-\\(f\\). Threshold operation is like a switch that when some conditions are achieved produces a special output. But when the conditions are not reached, it gives another output. A simple mathematical equation to express this function is:\n\\[ f(x) = \\begin{cases} 0, \u0026 \\text{if $x0$} \\\\ 1, \u0026 \\text{else} \\end{cases}\\tag{4} \\]\nTransfer functions can be linear or nonlinear; the Following three functions are mostly used.\nHard Limit Transfer Function The first commonly used threshold function is the most intuitive one, a piecewise function, when \\(x0\\) the output is ‘on’ or ‘off’ for others. And by convention, ‘on’ is replaced by \\(1\\) and ‘off’ is replaced by \\(-1\\). So it becomes:\n\\[ f(x) = \\begin{cases} 1, \u0026 \\text{if $x0$} \\\\ -1, \u0026 \\text{else} \\end{cases}\\tag{5} \\]\nand it looks like this:\nThen we take equation(1) into equation(5), we get: \\[ f(w\\cdot P +b) = \\begin{cases} 1, \u0026 \\text{if $w\\cdot P +b0$} \\\\ -1, \u0026 \\text{else} \\end{cases}\\tag{6} \\]\nWe always regard the input as an independent variable so we replace \\(P\\) with \\(x\\) without loss of generality. Then we get:\n\\[ g(x) = \\begin{cases} 1, \u0026 \\text{if $x -\\frac{b}{w}$} \\\\ -1, \u0026 \\text{else} \\end{cases}\\tag{7} \\]\nwhere \\(w\\neq 0\\). \\(g(x)\\) is a special case for equation(5) as the transfer function of this single-input neuron.\nThis is the famous threshold operation function, the Hard Limit Transfer Function\nLinear Transfer Function Another mostly used function is the linear function, which has the simplest form:\n\\[ f(x)=x\\tag{8} \\]\nand it is a line going through the origin\nWhen tale equation(1) into equation(8) we get:\n\\[ f(w\\cdot P+b)=w\\cdot P+b\\tag{9} \\]\nand we can get the special case of the linear transfer function for the single-input neuron:\n\\[ g(x)=w\\cdot x+b\\tag{10} \\]\nThe linear transfer function just seems as if there is no transfer function in the model but in some networks, it plays an important part.\nLog-sigmoid Transfer Function Another useful transfer function is the log-sigmoid function:\n\\[ f(x)=\\frac{1}{1+e^{-x}}\\tag{11} \\]\nThis sigmoid function has a similar appearance to the ‘Hard Limit Transfer Function’ however, the sigmoid has a more mathematical advantage than the hard limit transfer function, like it has derivative everywhere while equation(5) does not.\nThe single-input neuron model’s special case of the log-sigmoid function is:\n\\[ g(x)=\\frac{1}{1+e^{-w\\cdot x+b}}\\tag{12} \\]\nand it looks like this:\nThese three transfer functions are the most common ones and also the easiest ones. More transfer functions can be found:‘Transfer Function’\nMultiple-inputs Neuron After the insight of the single-input neuron, we can easily build a more complex and powerful neuron model- a multiple-inputs neuron, whose structure is more like the biological nerve cell than the single-input neuron:\nthen, the mathematical expression is:\n\\[ a=w_{1,1}\\cdot p_1+w_{1,2}\\cdot p_2+\\dots+ w_{1,R}\\cdot p_R+b\\tag{13} \\]\nThere are two numbers of subscript of \\(w\\) which seem unnecessary in the equation because the first number does not vary anymore. But as a long concern, it is better to remain this number for it is used to label the neuron. So \\(w_{1,2}\\) represents the second synapse’s weight belonging to the first neuron. When we have \\(k\\) neurons the \\(m\\)th synapse weight of \\(n\\)th neuron is \\(w_{n,m}\\).\nLet’s go back to the equation(13). It can be rewritten as:\n\\[ n=W\\mathbf{p}+b\\tag{14} \\]\nwhere: - \\(W\\) is a matrix that has only one row containing the weights - \\(\\mathbf{p}\\) is a vector representing inputs - \\(b\\) is a scalar representing bias - \\(n\\) is the result of the cell body operation,\nthen the output is:\n\\[ a=f(W\\mathbf{p}+b)\\tag{15} \\]\nThe diagram is a very powerful tool to express a neuron or a network because it’s good at showing the topological structure of the network. And for further research, an abbreviated notation had been designed. To the multiple-inputs neuron, we have:\na feature of this kind of notation is that the dimensions of each variable are labeled and the input dimension \\(R\\) is decided by the designer.\nNetwork Architecture A single neuron is not sufficient, even though it has multiple inputs.\nA layer of neurons To perform a more complicated function, we need more than one neuron and construct a network that contains a layer of neurons:\nin this model, we have \\(R\\)-dimensions input and \\(S\\) neurons then we get:\n\\[ a_i=f(\\sum_{j=1}^{R}w_{i,j}\\cdot p_{j}+b_j)\\tag{16} \\]\nthis is the output of \\(j\\) the neuron in the whole network, and we can rewrite the whole network in a metrical form:\n\\[ \\mathbf{a}=\\mathbf{f}(W\\mathbf{p}+\\mathbf{b})\\tag{17} \\]\nwhere\n \\(W\\) is a matrix \\(\\begin{bmatrix}w_{1,1}\u0026\\cdots\u0026w_{1,R}\\\\ \\vdots\u0026\u0026\\vdots\\\\w_{S,1}\u0026\\cdots\u0026w_{S_R}\\end{bmatrix}\\), where \\(w_{i,j}\\) is the \\(j\\)th weight of the \\(i\\)th neuron \\(\\mathbf{p}\\) is the vector of input \\(\\begin{bmatrix}p_1\\\\ \\vdots\\\\p_R\\end{bmatrix}\\) \\(\\mathbf{a}\\) is the vector of output \\(\\begin{bmatrix}a_1\\\\ \\vdots\\\\a_S\\end{bmatrix}\\) \\(\\mathbf{f}\\) is the vector of transfer functions \\(\\begin{bmatrix}f_1\\\\ \\vdots\\\\f_S\\end{bmatrix}\\) where each \\(f_i\\) can be different.  This network is much more powerful than the single neuron but they have a very similar abbreviated notation:\nthe only distinction is the dimension of each variable.\nMultiple Layers of Neurons The next stage of extending a single-layer network is multiple layers:\nand, its final output is:\n\\[ \\mathbf{a}=\\mathbf{f}^3(W^3\\mathbf{f}^2(W^2\\mathbf{f}^1(W^1\\mathbf{p}+\\mathbf{b}^1)+\\mathbf{b}^2)+\\mathbf{b}^3)\\tag{18} \\]\nthe numbers on the right-top of the variable are the layer number, for example, \\(w^1_{2,3}\\) is the weight of \\(2\\) nd synapse of the \\(3\\) rd neuron at the 1st layer.\nEach layer has also its name, for instance, the first layer whose input is external input is called the input layer. The layer whose output is external output is called the output layer. Other layers are called hidden layers. Its abbreviated notation is:\nThe new model with multiple layers is powerful but it is hard to design because the layer number is arbitrary and the neurons number in each layer is also untractable. So it becomes an experimental work. However, the input layer and output layer usually have a certain number and they are decided by the specialized task. Transfer functions are decided by the designer, and each neuron can have its transfer function different from any other neurons in the network.\nBias can be omitted but this can cause a problem that it will always output \\(\\mathbf{0}\\) when the input is \\(\\mathbf{0}\\). This phenomenon could not make sense in some tasks, so bias plays an important part in the \\(\\mathbf{0}\\) input situation. But to some other input, bias seems not so important.\nRecurrent Networks It seems possible that a neuron’s output also connects to its input. It acts somehow like\n\\[ \\mathbf{a}=\\mathbf{f}(W\\mathbf{f}(W\\mathbf{p}+\\mathbf{b})+\\mathbf{b})\\tag{19} \\]\nto illustrate the procedure, we present the delay block\nwhere the output is the input delayed 1-time unit:\n\\[ a(t)=u(t-1)\\tag{20} \\]\nand the block is initialized by \\(a(0)\\)\nAnother useful operation for the recurrent network is integrator:\nwhose output is:\n\\[ a(t)=\\int^t_0u(t)dt +a(0)\\tag{21} \\]\nA recurrent network is a network in which there is a feedback connection. Here we just list some basic concepts and more details would be researched in the following posts. The recurrent network works more powerful than a feedforward network because it exhibits temporal behavior which is a fundamental property of the biological brain. A typical recurrent network is:\nwhere:\n\\[ a(0)=\\mathbf{p} \\\\ a(t+1)=f(W\\mathbf{p}+\\mathbf{b})\\tag{22} \\]\nReferences  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","wordCount":"1905","inLanguage":"en","datePublished":"2019-12-10T10:54:57Z","dateModified":"2022-04-29T16:38:28+08:00","author":{"@type":"Person","name":"Anthony Tan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://anthony-tan.com/Neuron-Model-and-Network-Architecture/"},"publisher":{"@type":"Organization","name":"Anthony's Blogs","logo":{"@type":"ImageObject","url":"https://anthony-tan.com/logo.png"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-0PYGB86V72"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-0PYGB86V72")</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://anthony-tan.com accesskey=h title="Anthony's Blogs (Alt + H)">Anthony's Blogs</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://anthony-tan.com/machine_learning/ title="Machine Learning"><span>Machine Learning</span></a></li><li><a href=https://anthony-tan.com/deep_learning/ title="Deep Learning"><span>Deep Learning</span></a></li><li><a href=https://anthony-tan.com/reinforcement_learning/ title="Reinforcement Learning"><span>Reinforcement Learning</span></a></li><li><a href=https://anthony-tan.com/math/ title=Math><span>Math</span></a></li><li><a href=https://anthony-tan.com/others/ title=Others><span>Others</span></a></li><li><a href=https://anthony-tan.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://anthony-tan.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://anthony-tan.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://anthony-tan.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://anthony-tan.com>Home</a>&nbsp;»&nbsp;<a href=https://anthony-tan.com/deep_learning/>Deep Learning</a></div><h1 class=post-title>Neuron Model and Network Architecture</h1><div class=post-meta><span title="2019-12-10 10:54:57 +0000 UTC">December 10, 2019</span>&nbsp;·&nbsp;<span title="2022-04-29 16:38:28 +0800 +0800">(Last Modification: April 29, 2022)</span>&nbsp;·&nbsp;Anthony Tan</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#preliminaries aria-label=Preliminaries>Preliminaries</a></li><li><a href=#theory-and-notation1 aria-label="Theory and Notation1">Theory and Notation<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a></a></li><li><a href=#single-neuron-model aria-label="Single Neuron Model">Single Neuron Model</a></li><li><a href=#transfer-functions aria-label="Transfer Functions">Transfer Functions</a><ul><li><a href=#hard-limit-transfer-function aria-label="Hard Limit Transfer Function">Hard Limit Transfer Function</a></li><li><a href=#linear-transfer-function aria-label="Linear Transfer Function">Linear Transfer Function</a></li><li><a href=#log-sigmoid-transfer-function aria-label="Log-sigmoid Transfer Function">Log-sigmoid Transfer Function</a></li></ul></li><li><a href=#multiple-inputs-neuron aria-label="Multiple-inputs Neuron">Multiple-inputs Neuron</a></li><li><a href=#network-architecture aria-label="Network Architecture">Network Architecture</a><ul><li><a href=#a-layer-of-neurons aria-label="A layer of neurons">A layer of neurons</a></li><li><a href=#multiple-layers-of-neurons aria-label="Multiple Layers of Neurons">Multiple Layers of Neurons</a></li></ul></li><li><a href=#recurrent-networks aria-label="Recurrent Networks">Recurrent Networks</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=preliminaries>Preliminaries<a hidden class=anchor aria-hidden=true href=#preliminaries>#</a></h2><ol type=1><li><a href=https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/>linear classifier</a></li><li><a href=https://anthony-tan.com/An-Introduction-to-Neural-Networks>An Introduction to Neural Networks</a></li></ol><h2 id=theory-and-notation1>Theory and Notation<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a><a hidden class=anchor aria-hidden=true href=#theory-and-notation1>#</a></h2><p>We are not able to build any artificial cells up to now. It seems impossible to build a neuron network through biological materials manually, either. To investigate the ability of neurons we have built mathematical models of the neuron. These models have been assigned a number of neuron-like properties. However, there must be a balance between the number of properties contained by the mathematical models and the current computational abilities of the machines.</p><p>From now on, we begin our study of the neuron network, and it looks like a good idea, to begin with, the simplest but basic model – artificial neuron. Because tons of network architectures are built on these simple neurons.</p><h2 id=single-neuron-model>Single Neuron Model<a hidden class=anchor aria-hidden=true href=#single-neuron-model>#</a></h2><p>Let’s begin with the simplest neuron, which has only one input, one synapse which is represented by weight, a bias, a threshold operation which is expressed by a transfer function, and an output.</p><p>We know that the cell body of neurons plays summation and threshold operation. This is very like a <a href=https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/>linear classifier</a></p><p>Then our simplest model is constructed as follows:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_46_single_input.gif></p><ol type=1><li>Synapse is represented by a scalar which is called a weight, it will be multiplied by the input as a received signal, and then the signal is transferred to the cell body</li><li>Cell body here is represented by two functional properties property:<ul><li>the first one is summation which is used to collect all signals in a short time interval, while in this naive example only one input is concerned so it looks redundant but in the following models, it is an important operation of a neuron;</li><li>the second function is a threshold operation that acts as a gatekeeper, only the signal stronger than some value could excite this neuron. Only excited neurons could pass signals to the other neurons connected to them.</li></ul></li><li>and a scalar<ul><li>The scalar property represents an original faint signal of the neuron. From a biological point, it makes sense because every nerve cell has its resting membrane potential(RMP).</li></ul></li><li>Axon is expressed by an output that is produced by the threshold function. It can be any form in a biological neuron, like amplitude or frequency, but here it can just be a number that is decided by the selected threshold function.</li></ol><p>The threshold function is called an <a href>active function</a> or <a href>transfer functions</a> officially. And it will be listed in the next section.</p><p>Let’s review the single input neuron model and its components:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_52_single_input_neuron.jpeg></p><ul><li><span class="math inline">\(P\)</span>: input signal, a scalar or vector, coming from a previous nerve cell or external signal</li><li><span class="math inline">\(w\)</span>: weight, a scalar or vector, coming from the synapse, act as the strength of a synapse</li><li><span class="math inline">\(b\)</span>: bias, a scalar, a property of this neuron</li><li><span class="math inline">\(f\)</span>: transfer function, act as a gatekeeper, perform a threshold operation</li><li><span class="math inline">\(a\)</span>: the output of the neuron, a scalar, can be a signal to the next neuron or as a final output to the external system.</li></ul><p>The final mathematical expression is:</p><p><span class="math display">\[
a=f(w\cdot P + b)\tag{1}
\]</span></p><p>For instance, we have input <span class="math inline">\(P=2.0\)</span>, synapse weight <span class="math inline">\(w=3.0\)</span>, nerve cell bias <span class="math inline">\(b=-1.5\)</span> and then we get the output:</p><p><span class="math display">\[
a= f(2.0\times 3.0 + (-1.5))=f(4.5)\tag{2}
\]</span></p><p>however, bias can be omitted, or be rewritten as a special input and weight combination:</p><p><span class="math display">\[
a=f(w_1\cdot P + w_0\cdot 1)\text{ where } w_1=w \text{ and }w_0=b\tag{3}
\]</span></p><p><span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> came from equation(1). In the model, <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> are adjustable. And the ideal procedure is: 1. computing summation of all weighted inputs 2. select a transfer function 3. put the result of step 1 into the selected function from step 2 and get a final output of the neuron 4. using the learning rule to adjust <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> to adapt the task which is our purpose.</p><h2 id=transfer-functions>Transfer Functions<a hidden class=anchor aria-hidden=true href=#transfer-functions>#</a></h2><p>Every part of the neuron no matter the biological or mathematical one directly affects the function of the neuron. And this also makes the design of neurons more interesting, because we can build different kinds of neurons to simulate different kinds of operations. And this also provides sufficient basic blocks for us to develop a more complicated network.</p><p>Let’s recall the single input model above, the components are <span class="math inline">\(w\)</span>, <span class="math inline">\(b\)</span>, <span class="math inline">\(\sum\)</span>, and <span class="math inline">\(f\)</span>, however, <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> are objectives of learning, and <span class="math inline">\(\sum\)</span> is relatively stable which is hard to be replaced by any other operations. So, we move our attention to the threshold operation-<span class="math inline">\(f\)</span>. Threshold operation is like a switch that when some conditions are achieved produces a special output. But when the conditions are not reached, it gives another output. A simple mathematical equation to express this function is:</p><p><span class="math display">\[
f(x) =
\begin{cases}
0, & \text{if $x>0$} \\
1, & \text{else}
\end{cases}\tag{4}
\]</span></p><p>Transfer functions can be linear or nonlinear; the Following three functions are mostly used.</p><h3 id=hard-limit-transfer-function>Hard Limit Transfer Function<a hidden class=anchor aria-hidden=true href=#hard-limit-transfer-function>#</a></h3><p>The first commonly used threshold function is the most intuitive one, a piecewise function, when <span class="math inline">\(x>0\)</span> the output is ‘on’ or ‘off’ for others. And by convention, ‘on’ is replaced by <span class="math inline">\(1\)</span> and ‘off’ is replaced by <span class="math inline">\(-1\)</span>. So it becomes:</p><p><span class="math display">\[
f(x) =
\begin{cases}
1, & \text{if $x>0$} \\
-1, & \text{else}
\end{cases}\tag{5}
\]</span></p><p>and it looks like this:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_52_threshold_1.jpeg></p><p>Then we take equation(1) into equation(5), we get: <span class="math display">\[
f(w\cdot P +b) =
\begin{cases}
1, & \text{if $w\cdot P +b>0$} \\
-1, & \text{else}
\end{cases}\tag{6}
\]</span></p><p>We always regard the input as an independent variable so we replace <span class="math inline">\(P\)</span> with <span class="math inline">\(x\)</span> without loss of generality. Then we get:</p><p><span class="math display">\[
g(x) =
\begin{cases}
1, & \text{if $x> -\frac{b}{w}$} \\
-1, & \text{else}
\end{cases}\tag{7}
\]</span></p><p>where <span class="math inline">\(w\neq 0\)</span>. <span class="math inline">\(g(x)\)</span> is a special case for equation(5) as the transfer function of this single-input neuron.</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_52_threshold_1_re.jpeg></p><p>This is the famous threshold operation function, the Hard Limit Transfer Function</p><h3 id=linear-transfer-function>Linear Transfer Function<a hidden class=anchor aria-hidden=true href=#linear-transfer-function>#</a></h3><p>Another mostly used function is the linear function, which has the simplest form:</p><p><span class="math display">\[
f(x)=x\tag{8}
\]</span></p><p>and it is a line going through the origin</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_52_threshold_2.jpeg></p><p>When tale equation(1) into equation(8) we get:</p><p><span class="math display">\[
f(w\cdot P+b)=w\cdot P+b\tag{9}
\]</span></p><p>and we can get the special case of the linear transfer function for the single-input neuron:</p><p><span class="math display">\[
g(x)=w\cdot x+b\tag{10}
\]</span></p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_52_threshold_2_re.jpeg></p><p>The linear transfer function just seems as if there is no transfer function in the model but in some networks, it plays an important part.</p><h3 id=log-sigmoid-transfer-function>Log-sigmoid Transfer Function<a hidden class=anchor aria-hidden=true href=#log-sigmoid-transfer-function>#</a></h3><p>Another useful transfer function is the log-sigmoid function:</p><p><span class="math display">\[
f(x)=\frac{1}{1+e^{-x}}\tag{11}
\]</span></p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_53_threshold_3.jpeg></p><p>This sigmoid function has a similar appearance to the ‘Hard Limit Transfer Function’ however, the sigmoid has a more mathematical advantage than the hard limit transfer function, like it has derivative everywhere while equation(5) does not.</p><p>The single-input neuron model’s special case of the log-sigmoid function is:</p><p><span class="math display">\[
g(x)=\frac{1}{1+e^{-w\cdot x+b}}\tag{12}
\]</span></p><p>and it looks like this:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_53_threshold_3_re.jpeg></p><p>These three transfer functions are the most common ones and also the easiest ones. More transfer functions can be found:<a href>‘Transfer Function’</a></p><h2 id=multiple-inputs-neuron>Multiple-inputs Neuron<a hidden class=anchor aria-hidden=true href=#multiple-inputs-neuron>#</a></h2><p>After the insight of the single-input neuron, we can easily build a more complex and powerful neuron model- a multiple-inputs neuron, whose structure is more like the biological nerve cell than the single-input neuron:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_53_multiple_inputs.jpeg></p><p>then, the mathematical expression is:</p><p><span class="math display">\[
a=w_{1,1}\cdot p_1+w_{1,2}\cdot p_2+\dots+ w_{1,R}\cdot p_R+b\tag{13}
\]</span></p><p>There are two numbers of subscript of <span class="math inline">\(w\)</span> which seem unnecessary in the equation because the first number does not vary anymore. But as a long concern, it is better to remain this number for it is used to label the neuron. So <span class="math inline">\(w_{1,2}\)</span> represents the second synapse’s weight belonging to the first neuron. When we have <span class="math inline">\(k\)</span> neurons the <span class="math inline">\(m\)</span>th synapse weight of <span class="math inline">\(n\)</span>th neuron is <span class="math inline">\(w_{n,m}\)</span>.</p><p>Let’s go back to the equation(13). It can be rewritten as:</p><p><span class="math display">\[
n=W\mathbf{p}+b\tag{14}
\]</span></p><p>where: - <span class="math inline">\(W\)</span> is a matrix that has only one row containing the weights - <span class="math inline">\(\mathbf{p}\)</span> is a vector representing inputs - <span class="math inline">\(b\)</span> is a scalar representing bias - <span class="math inline">\(n\)</span> is the result of the cell body operation,</p><p>then the output is:</p><p><span class="math display">\[
a=f(W\mathbf{p}+b)\tag{15}
\]</span></p><p>The diagram is a very powerful tool to express a neuron or a network because it’s good at showing the topological structure of the network. And for further research, an abbreviated notation had been designed. To the multiple-inputs neuron, we have:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_53_abbreviated_notation.jpeg></p><p>a feature of this kind of notation is that the dimensions of each variable are labeled and the input dimension <span class="math inline">\(R\)</span> is decided by the designer.</p><h2 id=network-architecture>Network Architecture<a hidden class=anchor aria-hidden=true href=#network-architecture>#</a></h2><p>A single neuron is not sufficient, even though it has multiple inputs.</p><h3 id=a-layer-of-neurons>A layer of neurons<a hidden class=anchor aria-hidden=true href=#a-layer-of-neurons>#</a></h3><p>To perform a more complicated function, we need more than one neuron and construct a network that contains a layer of neurons:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_53_a_layer_of_neurons.jpeg></p><p>in this model, we have <span class="math inline">\(R\)</span>-dimensions input and <span class="math inline">\(S\)</span> neurons then we get:</p><p><span class="math display">\[
a_i=f(\sum_{j=1}^{R}w_{i,j}\cdot p_{j}+b_j)\tag{16}
\]</span></p><p>this is the output of <span class="math inline">\(j\)</span> the neuron in the whole network, and we can rewrite the whole network in a metrical form:</p><p><span class="math display">\[
\mathbf{a}=\mathbf{f}(W\mathbf{p}+\mathbf{b})\tag{17}
\]</span></p><p>where</p><ul><li><span class="math inline">\(W\)</span> is a matrix <span class="math inline">\(\begin{bmatrix}w_{1,1}&\cdots&w_{1,R}\\ \vdots&&\vdots\\w_{S,1}&\cdots&w_{S_R}\end{bmatrix}\)</span>, where <span class="math inline">\(w_{i,j}\)</span> is the <span class="math inline">\(j\)</span>th weight of the <span class="math inline">\(i\)</span>th neuron</li><li><span class="math inline">\(\mathbf{p}\)</span> is the vector of input <span class="math inline">\(\begin{bmatrix}p_1\\ \vdots\\p_R\end{bmatrix}\)</span></li><li><span class="math inline">\(\mathbf{a}\)</span> is the vector of output <span class="math inline">\(\begin{bmatrix}a_1\\ \vdots\\a_S\end{bmatrix}\)</span></li><li><span class="math inline">\(\mathbf{f}\)</span> is the vector of transfer functions <span class="math inline">\(\begin{bmatrix}f_1\\ \vdots\\f_S\end{bmatrix}\)</span> where each <span class="math inline">\(f_i\)</span> can be different.</li></ul><p>This network is much more powerful than the single neuron but they have a very similar abbreviated notation:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_53_a_layer_of_neurons_abbreviated_notation.jpeg></p><p>the only distinction is the dimension of each variable.</p><h3 id=multiple-layers-of-neurons>Multiple Layers of Neurons<a hidden class=anchor aria-hidden=true href=#multiple-layers-of-neurons>#</a></h3><p>The next stage of extending a single-layer network is multiple layers:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_53_mutiple_layers_neurons.jpeg></p><p>and, its final output is:</p><p><span class="math display">\[
\mathbf{a}=\mathbf{f}^3(W^3\mathbf{f}^2(W^2\mathbf{f}^1(W^1\mathbf{p}+\mathbf{b}^1)+\mathbf{b}^2)+\mathbf{b}^3)\tag{18}
\]</span></p><p>the numbers on the right-top of the variable are the layer number, for example, <span class="math inline">\(w^1_{2,3}\)</span> is the weight of <span class="math inline">\(2\)</span> nd synapse of the <span class="math inline">\(3\)</span> rd neuron at the 1st layer.</p><p>Each layer has also its name, for instance, the first layer whose input is external input is called the input layer. The layer whose output is external output is called the output layer. Other layers are called hidden layers. Its abbreviated notation is:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_53_mutiple_layers_neurons_abbreviated_notation.jpeg></p><p>The new model with multiple layers is powerful but it is hard to design because the layer number is arbitrary and the neurons number in each layer is also untractable. So it becomes an experimental work. However, the input layer and output layer usually have a certain number and they are decided by the specialized task. Transfer functions are decided by the designer, and each neuron can have its transfer function different from any other neurons in the network.</p><p>Bias can be omitted but this can cause a problem that it will always output <span class="math inline">\(\mathbf{0}\)</span> when the input is <span class="math inline">\(\mathbf{0}\)</span>. This phenomenon could not make sense in some tasks, so bias plays an important part in the <span class="math inline">\(\mathbf{0}\)</span> input situation. But to some other input, bias seems not so important.</p><h2 id=recurrent-networks>Recurrent Networks<a hidden class=anchor aria-hidden=true href=#recurrent-networks>#</a></h2><p>It seems possible that a neuron’s output also connects to its input. It acts somehow like</p><p><span class="math display">\[
\mathbf{a}=\mathbf{f}(W\mathbf{f}(W\mathbf{p}+\mathbf{b})+\mathbf{b})\tag{19}
\]</span></p><p>to illustrate the procedure, we present the delay block</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_54_delay_block.jpeg></p><p>where the output is the input delayed 1-time unit:</p><p><span class="math display">\[
a(t)=u(t-1)\tag{20}
\]</span></p><p>and the block is initialized by <span class="math inline">\(a(0)\)</span></p><p>Another useful operation for the recurrent network is integrator:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_54_integrator.jpeg></p><p>whose output is:</p><p><span class="math display">\[
a(t)=\int^t_0u(t)dt +a(0)\tag{21}
\]</span></p><p>A recurrent network is a network in which there is a feedback connection. Here we just list some basic concepts and more details would be researched in the following posts. The recurrent network works more powerful than a feedforward network because it exhibits temporal behavior which is a fundamental property of the biological brain. A typical recurrent network is:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_29_15_54_recurrent_network.jpeg></p><p>where:</p><p><span class="math display">\[
a(0)=\mathbf{p} \\
a(t+1)=f(W\mathbf{p}+\mathbf{b})\tag{22}
\]</span></p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><section class=footnotes role=doc-endnotes><hr><ol><li id=fn1 role=doc-endnote><p>Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.<a href=#fnref1 class=footnote-back role=doc-backlink>↩︎</a></p></li></ol></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://anthony-tan.com/tags/artificial-neural-networks/>Artificial Neural Networks</a></li><li><a href=https://anthony-tan.com/tags/artificial-intelligence/>Artificial Intelligence</a></li><li><a href=https://anthony-tan.com/tags/hard-limit-transfer-function/>Hard Limit Transfer Function</a></li><li><a href=https://anthony-tan.com/tags/artificial-neuron/>artificial neuron</a></li><li><a href=https://anthony-tan.com/tags/transfer-functions/>Transfer Functions</a></li><li><a href=https://anthony-tan.com/tags/single-neuron-model/>Single Neuron Model</a></li><li><a href=https://anthony-tan.com/tags/threshold-function/>threshold function</a></li><li><a href=https://anthony-tan.com/tags/active-function/>active function</a></li><li><a href=https://anthony-tan.com/tags/log-sigmoid-transfer-function/>Log-sigmoid Transfer Function</a></li><li><a href=https://anthony-tan.com/tags/linear-transfer-function/>Linear Transfer Function</a></li><li><a href=https://anthony-tan.com/tags/recurrent-networks/>Recurrent Networks</a></li><li><a href=https://anthony-tan.com/tags/multiple-inputs-neuron/>Multiple-inputs Neuron</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Neuron Model and Network Architecture on twitter" href="https://twitter.com/intent/tweet/?text=Neuron%20Model%20and%20Network%20Architecture&url=https%3a%2f%2fanthony-tan.com%2fNeuron-Model-and-Network-Architecture%2f&hashtags=ArtificialNeuralNetworks%2cArtificialIntelligence%2cHardLimitTransferFunction%2cartificialneuron%2cTransferFunctions%2cSingleNeuronModel%2cthresholdfunction%2cactivefunction%2cLog-sigmoidTransferFunction%2cLinearTransferFunction%2cRecurrentNetworks%2cMultiple-inputsNeuron"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neuron Model and Network Architecture on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fanthony-tan.com%2fNeuron-Model-and-Network-Architecture%2f&title=Neuron%20Model%20and%20Network%20Architecture&summary=Neuron%20Model%20and%20Network%20Architecture&source=https%3a%2f%2fanthony-tan.com%2fNeuron-Model-and-Network-Architecture%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neuron Model and Network Architecture on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fanthony-tan.com%2fNeuron-Model-and-Network-Architecture%2f&title=Neuron%20Model%20and%20Network%20Architecture"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neuron Model and Network Architecture on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fanthony-tan.com%2fNeuron-Model-and-Network-Architecture%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neuron Model and Network Architecture on whatsapp" href="https://api.whatsapp.com/send?text=Neuron%20Model%20and%20Network%20Architecture%20-%20https%3a%2f%2fanthony-tan.com%2fNeuron-Model-and-Network-Architecture%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Neuron Model and Network Architecture on telegram" href="https://telegram.me/share/url?text=Neuron%20Model%20and%20Network%20Architecture&url=https%3a%2f%2fanthony-tan.com%2fNeuron-Model-and-Network-Architecture%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><figure class=article-discussion><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//anthony-tan-com.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></figure></article></main><footer class=footer><span>&copy; 2023 <a href=https://anthony-tan.com>Anthony's Blogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>