<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Performance Surfaces and Optimum Points | Anthony's Blogs</title><meta name=keywords content="Artificial Neural Networks,Artificial Intelligence,performance surfaces,optimum points,Performance Learning,Associative Learning,Competitive Learning,Taylor Series,Necessary Conditions for Optimality,First-order Conditions,Second-order Condition,Saddle Points,global optimization"><meta name=description content="Preliminaries Perceptron learning algorithm Hebbian learning algorithm Linear algebra  Neural Network Training Technique1 Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:"><meta name=author content="Anthony Tan"><link rel=canonical href=https://anthony-tan.com/Performance-Surfaces-and-Optimum-Points/><link crossorigin=anonymous href=../assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://anthony-tan.com/logo.png><link rel=apple-touch-icon href=https://anthony-tan.com/logo.png><link rel=mask-icon href=https://anthony-tan.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-12","auto"),ga("send","pageview"))</script><meta property="og:title" content="Performance Surfaces and Optimum Points"><meta property="og:description" content="Preliminaries Perceptron learning algorithm Hebbian learning algorithm Linear algebra  Neural Network Training Technique1 Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:"><meta property="og:type" content="article"><meta property="og:url" content="https://anthony-tan.com/Performance-Surfaces-and-Optimum-Points/"><meta property="article:section" content="deep_learning"><meta property="article:published_time" content="2019-12-19T08:57:53+00:00"><meta property="article:modified_time" content="2022-05-01T22:23:05+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Performance Surfaces and Optimum Points"><meta name=twitter:description content="Preliminaries Perceptron learning algorithm Hebbian learning algorithm Linear algebra  Neural Network Training Technique1 Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Deep Learning","item":"https://anthony-tan.com/deep_learning/"},{"@type":"ListItem","position":3,"name":"Performance Surfaces and Optimum Points","item":"https://anthony-tan.com/Performance-Surfaces-and-Optimum-Points/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Performance Surfaces and Optimum Points","name":"Performance Surfaces and Optimum Points","description":"Preliminaries Perceptron learning algorithm Hebbian learning algorithm Linear algebra  Neural Network Training Technique1 Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:","keywords":["Artificial Neural Networks","Artificial Intelligence","performance surfaces","optimum points","Performance Learning","Associative Learning","Competitive Learning","Taylor Series","Necessary Conditions for Optimality","First-order Conditions","Second-order Condition","Saddle Points","global optimization"],"articleBody":"Preliminaries Perceptron learning algorithm Hebbian learning algorithm Linear algebra  Neural Network Training Technique1 Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:\nPerformance Learning Associative Learning Competitive Learning  The linear associator we discussed in ‘Hebbian Learning’ is a kind of associative learning, which is used to build a connection between two events. Competitive learning is a kind of unsupervised learning in which nodes(neurons) of the neural network compete for the right to respond to a subset of the input data.2\nThe main topic we are going to discuss today is Performance Learning which is a widely used training method in neural network projects. By the way, the categories of learning can be classified in different ways. This is not the only kind of classification.\nPerformance Learning and Performance Index Training a network is to find suitable parameters for the model to meet our requirements for different tasks. If we can measure how suitable the neural network is for the task, we can then decide what to do next to modify the parameters. Performance learning is the procedure that modifies the parameters of neural networks by their performance based on certain measurements of the neural network performance.\nThe measurement of the performance we investigate here is called performance index and its appearance is called performance surface. What we should also be concerned about is the conditions for the existence of the minima or maxima. Because the minima or maxima decide the final result of the performance of the neural network for the task. In a word, what we need to do is to optimize the performance index by adjusting the parameters of the neural network with the information given by a training set(for different tasks).\nPerformance learning contains several different laws. And there are two common steps involved in the optimization process: 1. Define the ‘performance’ - a quantitative measure of network performance is called the performance index which has the properties when the neural network works well it has a lower value but when the neural network works poorly it has a larger value 2. Search parameters space to reduce the performance index\nThis is the heart of performance learning. We cloud also analyze the characteristics of the performance index before searching for the minima in the parameter space. Because, if the performance index we had selected did not meet the conditions of the existence of minima, searching for a minimum is just a waste of time. Or we can also set the additional condition to guarantee the existence of a minimum point.\nTaylor Series When we have a suitable performance index, what we need is a certain algorithm or a framework to deal with the optimization task. To design such a framework, we should study the performance index function first. Taylor series is a powerful tool to analyze the variation around a certain point of a function.\nScalar form function \\(F(x)\\) is an analytic function which means derivatives of \\(F(x)\\) exist everywhere. Then the Taylor series expansion of point \\(x^{\\star}\\) is:\n\\[ \\begin{aligned} F(x)=F(x^{\\star})\u0026+\\frac{d}{dx}F(x)|_{x=x^{\\star}}(x-x^{\\star})\\\\ \u0026+\\frac{1}{2}\\frac{d^2}{d^2x}F(x)|_{x=x^{\\star}}(x-x^{\\star})^2\\\\ \u0026\\vdots \\\\ \u0026+\\frac{1}{n!}\\frac{d^n}{d^nx}F(x)|_{x=x^{\\star}}(x-x^{\\star})^n \\end{aligned}\\tag{1} \\]\nThis series with infinity items can exactly equal the origin analytic function.\nIf we only want to approximate the function in a small region near \\(x^{\\star}\\), finite items in equation(1) are usually enough.\nFor instance, we have a function \\(F(x)=\\cos(x)\\) when \\(x^{\\star}=0\\), then:\n\\[ \\begin{aligned} F(x)=\\cos(x)\u0026=\\cos(0)-\\sin(0)(x-0)-\\frac{1}{2}\\cos(0)(x-0)^2+\\cdots\\\\ \u0026=1-\\frac{1}{2}x^2+\\frac{1}{24}x^4+\\cdots \\end{aligned}\\tag{2} \\]\n \\(0^{\\text{th}}\\) order approximation of \\(F(x)\\) near \\(0\\) is \\(F(x)\\approx F_0(x)=1\\) \\(1^{\\text{st}}\\) order approximation of \\(F(x)\\) near \\(0\\) is \\(F(x)\\approx F_1(x)=1+0\\) \\(2^{\\text{nd}}\\) order approximation of \\(F(x)\\) near \\(0\\) is \\(F(x)\\approx F_2(x)=1+0-\\frac{1}{2}x^2\\) \\(3^{\\text{rd}}\\) order approximation of \\(F(x)\\) near \\(0\\) is \\(F(x)\\approx F_3(x)=1+0-\\frac{1}{2}x^2+0\\) \\(4^{\\text{th}}\\) order approximation of \\(F(x)\\) near \\(0\\) is \\(F(x)\\approx F_4(x)=1+0-\\frac{1}{2}x^2+0+\\frac{1}{24}x^4\\)  Odd number\\(^{\\text{th}}\\) iterm is \\(0\\) because of the value of \\(\\sin(x)\\) at \\(0\\) is \\(0\\)\nAnd the \\(0^{\\text{th}},1^{\\text{st}},2^{\\text{nd}},3^{\\text{rd}}\\) and \\(4^{\\text{th}}\\) approximation of \\(F(x)\\) looks like:\nand, from the figure above we can observe that \\(F_0(x)\\) can only approximate \\(F(x)\\) at \\(x^{\\star}=0\\) point. While in the interval between \\(-1\\) and \\(+1\\), \\(F(x)\\) can be precisely approximated by \\(F_2(x)\\). In a more wider interval like \\([-1.6,+1.6]\\) to approximate \\(F(x)\\), \\(4^{\\text{th}}\\) order Taylor series are needed.\nThen we can get the conclusion that if we want a certain precise extent in a relatively larger interval we need more items in the Taylor series. And we should pay attention to the interval out of the precise region, such as \\(F_2(x)\\) in the interval \\((-\\infty,-1]\\cup [1,+\\infty)\\) are going away from \\(F(x)\\) as \\(x\\) moving away from \\(x^{\\star}=0\\)\nVector form function When the performance index is a function of a vector \\(\\mathbf{x}\\), a vector from the Taylor series should be presented. And in the neural network, parameters are variables of the performance index so the vector from the Taylor series is the basic tool in performance learning. Function \\(F(\\mathbf{x})\\) can be decomposed in any precise:\n\\[ \\begin{aligned} F(\\mathbf{x})=F(\\mathbf{x}^{\\star})\u0026 +\\frac{\\partial}{\\partial x_1}F(\\mathbf{x})|_{\\mathbf{x}=\\mathbf{x}^{\\star}}(x_1-x_1^{\\star})\\\\ \u0026+\\frac{\\partial}{\\partial x_2}F(\\mathbf{x})|_{\\mathbf{x}=\\mathbf{x}^{\\star}}(x_2-x_2^{\\star})+\\cdots\\\\ \u0026+\\frac{1}{2}\\frac{\\partial^2}{\\partial^2 x_1}F(\\mathbf{x})|_{\\mathbf{x}=\\mathbf{x}^{\\star}}(x_1-x_1^{\\star})^2\\\\ \u0026+\\frac{1}{2}\\frac{\\partial^2}{\\partial x_1 \\partial x_2}F(\\mathbf{x})|_{\\mathbf{x}=\\mathbf{x}^{\\star}}(x_1-x_1^{\\star})(x_2-x_2^{\\star}) +\\cdots\\\\ \u0026\\vdots \\end{aligned}\\tag{3} \\]\nif we notate the gradient as : \\[ \\nabla F(x)=\\begin{bmatrix} \\frac{\\partial F}{\\partial x_1}\\\\ \\frac{\\partial F}{\\partial x_2}\\\\ \\vdots\\\\ \\frac{\\partial F}{\\partial x_n} \\end{bmatrix}\\tag{4} \\]\nthen the Taylor series can be written as: \\[ \\begin{aligned} F(\\mathbf{x})=F(\\mathbf{x}^{\\star})\u0026+(\\mathbf{x}-\\mathbf{x}^{\\star})^T\\nabla F(x)|_{\\mathbf{x}=\\mathbf{x}^{\\star}}\\\\ \u0026+\\frac{1}{2}(\\mathbf{x}-\\mathbf{x}^{\\star})^T\\nabla^2 F(x)|_{\\mathbf{x}=\\mathbf{x}^{\\star}}(x_1-x_1^{\\star})\\\\ \u0026 \\vdots \\end{aligned}\\tag{5} \\]\nThe coefficients of the second-order item can be written in a matrix form, and it is also called the Hessian matrix:\n\\[ \\nabla^2 F(x) = \\begin{bmatrix} \\frac{\\partial^2}{\\partial^2 x_1}\u0026\\cdots\u0026\\frac{\\partial^2}{\\partial x_1 \\partial x_n}\\\\ \\vdots\u0026\\ddots\u0026\\vdots \\\\ \\frac{\\partial^2}{\\partial x_n \\partial x_1}\u0026\\cdots\u0026\\frac{\\partial^2}{\\partial^2 x_n} \\end{bmatrix}\\tag{6} \\]\nHessian matrix has many beautiful properties, such as it is always - Square matrix - Symmetric matrix\nIn the matrix, the elements on the diagonal are \\(\\frac{\\partial^2 F}{\\partial^2 x_i}\\) is the \\(2^{\\text{nd}}\\) derivative along the \\(x_i\\)-axis and in the gradient vector, \\(\\frac{d F}{d x_i}\\) is the first derivative along the \\(x_i\\)-axis.\nTo calculate the \\(1^{\\text{st}}\\) or \\(2^{\\text{nd}}\\) derivative along arbitrary deriction \\(\\mathbf{p}\\), we have: 1. \\(1^{\\text{st}}\\)-order derivative along \\(\\mathbf{p}\\) : \\(\\frac{\\mathbf{p}^T\\nabla F(\\mathbf{x})}{||\\mathbf{p}||}\\) 2. \\(2^{\\text{nd}}\\)-order derivative along \\(\\mathbf{p}\\) : \\(\\frac{\\mathbf{p}^T\\nabla^2 F(\\mathbf{x})\\mathbf{p}}{||\\mathbf{p}||^2}\\)\nFor instance, we have a function \\[ F(\\mathbf{x})=x_1^2+2x_2^2\\tag{7} \\]\nto find the derivative at \\(\\mathbf{x}^{\\star}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}\\) in the direction \\(\\mathbf{p}=\\begin{bmatrix}2\\\\-1\\end{bmatrix}\\), we get the derivative at \\(\\mathbf{x}^{\\star}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}\\):\n\\[ \\nabla F|_{\\mathbf{x}=\\mathbf{x}^{\\star}} = \\begin{bmatrix} \\frac{\\partial F}{\\partial x_1}\\\\ \\frac{\\partial F}{\\partial x_2} \\end{bmatrix}\\bigg|_{\\mathbf{x}=\\mathbf{x}^{\\star}}=\\begin{bmatrix} 2x_1\\\\ 4x_2 \\end{bmatrix}\\bigg|_{\\mathbf{x}=\\mathbf{x}^{\\star}}=\\begin{bmatrix} 1\\\\ 2 \\end{bmatrix}\\tag{8} \\]\nand the direction \\(\\mathbf{p}=\\begin{bmatrix}2\\\\-1\\end{bmatrix}\\), the derivative is\n\\[ \\frac{\\mathbf{p}^T\\nabla F(\\mathbf{x}^{\\star})}{||\\mathbf{p}||}= \\frac{\\begin{bmatrix}2\u0026-1\\end{bmatrix} \\begin{bmatrix}1\\\\2\\end{bmatrix}}{\\sqrt{2^2+(-1)^2}}=\\frac{0}{\\sqrt{5}}=0\\tag{9} \\]\n\\(0\\) is a special number in the whole real numbers set. And the derivative is zero also meaningful in the optimization procedure. To find a zero slop direction we should solve the equation:\n\\[ \\frac{\\mathbf{p}^T\\nabla F(\\mathbf{x}^{\\star})}{||\\mathbf{p}||}=0\\tag{10} \\]\nthis means \\(\\mathbf{p}\\) can not be \\(\\mathbf{0}\\) because \\(0\\) length is illegle. And \\(\\mathbf{p}_{\\text{unit}}=\\frac{\\mathbf{p}}{||\\mathbf{p}||}\\) is a unit vector along deriction \\(\\mathbf{p}\\) so this can be written as:\n\\[ \\mathbf{p}_{\\text{unit}}^T\\nabla F(\\mathbf{x}^{\\star})=0\\tag{11} \\]\nwhich means \\(\\mathbf{p}_{\\text{unit}}\\) is orthogonal to gradient \\(\\nabla F(\\mathbf{x}^{\\star})\\)\nAnother special deriction is which one has the greatest slop. Assuming the deriction \\(\\mathbf{p}_{\\text{unit}}=\\frac{\\mathbf{p}}{||\\mathbf{p}||}\\) is the greatest slope, so:\n\\[ \\mathbf{p}_{\\text{unit}}^T\\nabla F(\\mathbf{x}^{\\star})=||\\mathbf{p}_{\\text{unit}}^T|| \\cdot ||\\nabla F(\\mathbf{x}^{\\star})||\\cos(\\theta)\\tag{12} \\]\nhas the greatest value. We know this can only happen when \\(\\cos(\\theta)=1\\) which means the direction of the gradient has the greatest slop.\nNecessary Conditions for Optimality The main objective of performance learning is to minimize the performance index. So the condition of existence of a minimum performance index should be investigated:\nA strong minimum \\(\\mathbf{x}\\) which mean in any deriction \\(\\mathbf{p}_{\\text{unit}}\\) around this point with a short distance \\(\\delta\\) always has \\(F(\\mathbf{x}), where \\(\\delta \\to 0\\) A weak minimum \\(\\mathbf{x}\\) which mean in any deriction \\(\\mathbf{p}_{\\text{unit}}\\) around this point with a short distance \\(\\delta\\) always has \\(F(\\mathbf{x})\\leq F(\\mathbf{x}+\\delta \\mathbf{p}_{\\text{unit}})\\), where \\(\\delta \\to 0\\) Global minimum is the minimum one of all weak or strong minimum sets.  For instance,\n\\[ F(x)=3x^4-7x^2-\\frac{1}{2}x+6\\tag{13} \\]\n 2 local minimums at near \\(x_1=-1.1\\) and near \\(x_2=1.1\\) near \\(x_2=1.1\\) gives the global minimum  If the variable of function is a vector:\n\\[ F(\\mathbf{x})=(x_2-x_1)^4+8x_1x_2-x_1+x_2+3\\tag{14} \\]\nand it looks like this:\nin 3-D space. And the contour plot of this function is:\nThere are three points in this contour figure:\nand the pink points are the two local minimums, and the black points are called a saddle points.\nThese two examples illustrate the minimum points and saddle points. But what conditions are needed to confirm the existence?\nFirst-order Conditions Go back to the Taylor series with \\(\\Delta x=x-x^{\\star}\\neq 0\\), the first item of the series is taken into account:\n\\[ \\begin{aligned} F(x)\u0026=F(x^{\\star})+\\frac{d}{dx}F(x)|_{x=x^{\\star}}(x-x^{\\star})\\\\ F(x^{\\star}+\\Delta x)\u0026=F(x^{\\star})+\\nabla F(x)\\Delta x \\end{aligned}\\tag{15} \\]\nwhen \\(x^{\\star}\\) is a candidate of minimum point, we want:\n\\[ F(x^{\\star}+\\Delta x)\\geq F(x^{\\star})\\tag{16} \\]\nso, in equation(15), \\[ \\nabla F(x)\\Delta x\\geq 0 \\tag{17} \\]\nare needed. Considering another direction, if \\(x^{\\star}\\) is the minimum point, it also has:\n\\[ F(x^{\\star}-\\Delta x)=F(x^{\\star})-\\nabla F(x)\\Delta x\\geq F(x^{\\star})\\tag{18} \\]\nthen\n\\[ \\nabla F(x)\\Delta x\\leq 0 \\tag{19} \\]\nTo satisfy both equation(17) and equation(19) if and only if\n\\[ \\nabla F(x)\\Delta x=0\\tag{20} \\]\nBecause \\(\\Delta x\\neq 0\\), so we must have \\(\\nabla F(x)\\) when \\(x\\) is the minimum point. \\(\\nabla F(x)=0\\) is a necessary but not sufficient condition. The first-order condition has been concluded above.\nSecond-order Condition The first-order condition does not give a sufficient condition. Now let’s consider the second-order condition. With \\(\\Delta\\mathbf{x}=\\mathbf{x}-\\mathbf{x}^{\\star}\\neq \\mathbf{0}\\) and gradient equal to \\(\\mathbf{0}\\), and take them into equation(5) . Then the second-order Taylor series is:\n\\[ F(\\mathbf{\\Delta\\mathbf{x}-\\mathbf{x}^{\\star}})=F(\\mathbf{x}^{\\star})+\\frac{1}{2}\\Delta\\mathbf{x}^T\\nabla^2 F(x)|_{\\mathbf{x}=\\mathbf{x}^{\\star}}\\Delta\\mathbf{x}\\tag{21} \\]\nWhen \\(||\\Delta\\mathbf{x}||\\) is small, zero-order, first-order, and second-order terms of the Taylor series are precise enough to approximate the original function. When \\(F(\\mathbf{x}^{\\star})\\) is a strong minimum, the second-order item should be:\n\\[ \\frac{1}{2}\\Delta\\mathbf{x}^T\\nabla^2 F(x)|_{\\mathbf{x}=\\mathbf{x}^{\\star}}\\Delta\\mathbf{x}0 \\]\nRecalling linear algebra knowledge, positive definite matrix \\(A\\) is:\n\\[ \\mathbf{z}^T A \\mathbf{z}0 \\text{ for any } \\mathbf{z} \\]\nand positive semidefinite matrix \\(A\\) is:\n\\[ \\mathbf{z}^T A \\mathbf{z}\\geq0 \\text{ for any } \\mathbf{z} \\]\nSo when the gradient is \\(\\mathbf{0}\\) and the second-order derivative is positive definite(semidefinite), this point is a strong(weak) minimum. And positive definite Hessian matrix is a sufficient condition to a strong minimum point, but it’s not a necessary condition. Because when the Hessian matrix is \\(0\\) the third-order item has to be calculated.\nReferences  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n https://en.wikipedia.org/wiki/Competitive_learning↩︎\n   ","wordCount":"1711","inLanguage":"en","datePublished":"2019-12-19T08:57:53Z","dateModified":"2022-05-01T22:23:05+08:00","author":{"@type":"Person","name":"Anthony Tan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://anthony-tan.com/Performance-Surfaces-and-Optimum-Points/"},"publisher":{"@type":"Organization","name":"Anthony's Blogs","logo":{"@type":"ImageObject","url":"https://anthony-tan.com/logo.png"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://anthony-tan.com accesskey=h title="Anthony's Blogs (Alt + H)">Anthony's Blogs</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://anthony-tan.com/machine_learning/ title="Machine Learning"><span>Machine Learning</span></a></li><li><a href=https://anthony-tan.com/deep_learning/ title="Deep Learning"><span>Deep Learning</span></a></li><li><a href=https://anthony-tan.com/reinforcement_learning/ title="Reinforcement Learning"><span>Reinforcement Learning</span></a></li><li><a href=https://anthony-tan.com/math/ title=Math><span>Math</span></a></li><li><a href=https://anthony-tan.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://anthony-tan.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://anthony-tan.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://anthony-tan.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://anthony-tan.com>Home</a>&nbsp;»&nbsp;<a href=https://anthony-tan.com/deep_learning/>Deep Learning</a></div><h1 class=post-title>Performance Surfaces and Optimum Points</h1><div class=post-meta><span title="2019-12-19 08:57:53 +0000 UTC">December 19, 2019</span>&nbsp;·&nbsp;<span title="2022-05-01 22:23:05 +0800 +0800">(Last Modification: May 1, 2022)</span>&nbsp;·&nbsp;Anthony Tan</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#preliminaries aria-label=Preliminaries>Preliminaries</a></li><li><a href=#neural-network-training-technique1 aria-label="Neural Network Training Technique1">Neural Network Training Technique<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a></a></li><li><a href=#performance-learning-and-performance-index aria-label="Performance Learning and Performance Index">Performance Learning and Performance Index</a></li><li><a href=#taylor-series aria-label="Taylor Series">Taylor Series</a><ul><li><a href=#scalar-form-function aria-label="Scalar form function">Scalar form function</a></li><li><a href=#vector-form-function aria-label="Vector form function">Vector form function</a></li></ul></li><li><a href=#necessary-conditions-for-optimality aria-label="Necessary Conditions for Optimality">Necessary Conditions for Optimality</a><ul><li><a href=#first-order-conditions aria-label="First-order Conditions">First-order Conditions</a></li><li><a href=#second-order-condition aria-label="Second-order Condition">Second-order Condition</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=preliminaries>Preliminaries<a hidden class=anchor aria-hidden=true href=#preliminaries>#</a></h2><ol type=1><li><a href=https://anthony-tan.com/Learning-Rules-and-Perceptron-Learning-Rule/>Perceptron learning algorithm</a></li><li><a href=https://anthony-tan.com/Supervised-Hebbian-Learning/>Hebbian learning algorithm</a></li><li>Linear algebra</li></ol><h2 id=neural-network-training-technique1>Neural Network Training Technique<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a><a hidden class=anchor aria-hidden=true href=#neural-network-training-technique1>#</a></h2><p>Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:</p><ol type=1><li>Performance Learning</li><li>Associative Learning</li><li>Competitive Learning</li></ol><p>The linear associator we discussed in <a href=https://anthony-tan.com/Supervised-Hebbian-Learning/>‘Hebbian Learning’</a> is a kind of associative learning, which is used to build a connection between two events. Competitive learning is a kind of unsupervised learning in which nodes(neurons) of the neural network compete for the right to respond to a subset of the input data.<a href=#fn2 class=footnote-ref id=fnref2 role=doc-noteref><sup>2</sup></a></p><p>The main topic we are going to discuss today is Performance Learning which is a widely used training method in neural network projects. By the way, the categories of learning can be classified in different ways. This is not the only kind of classification.</p><h2 id=performance-learning-and-performance-index>Performance Learning and Performance Index<a hidden class=anchor aria-hidden=true href=#performance-learning-and-performance-index>#</a></h2><p>Training a network is to find suitable parameters for the model to meet our requirements for different tasks. If we can measure how suitable the neural network is for the task, we can then decide what to do next to modify the parameters. Performance learning is the procedure that modifies the parameters of neural networks by their performance based on certain measurements of the neural network performance.</p><p>The measurement of the performance we investigate here is called <strong>performance index</strong> and its appearance is called <strong>performance surface</strong>. What we should also be concerned about is the conditions for the existence of the minima or maxima. Because the minima or maxima decide the final result of the performance of the neural network for the task. In a word, what we need to do is to optimize the performance index by adjusting the parameters of the neural network with the information given by a training set(for different tasks).</p><p>Performance learning contains several different laws. And there are two common steps involved in the optimization process: 1. Define the ‘performance’ - a quantitative measure of network performance is called the performance index which has the properties when the neural network works well it has a lower value but when the neural network works poorly it has a larger value 2. Search parameters space to reduce the performance index</p><p>This is the heart of performance learning. We cloud also analyze the characteristics of the performance index before searching for the minima in the parameter space. Because, if the performance index we had selected did not meet the conditions of the existence of minima, searching for a minimum is just a waste of time. Or we can also set the additional condition to guarantee the existence of a minimum point.</p><h2 id=taylor-series>Taylor Series<a hidden class=anchor aria-hidden=true href=#taylor-series>#</a></h2><p>When we have a suitable performance index, what we need is a certain algorithm or a framework to deal with the optimization task. To design such a framework, we should study the performance index function first. Taylor series is a powerful tool to analyze the variation around a certain point of a function.</p><h3 id=scalar-form-function>Scalar form function<a hidden class=anchor aria-hidden=true href=#scalar-form-function>#</a></h3><p><span class="math inline">\(F(x)\)</span> is an <strong>analytic function</strong> which means derivatives of <span class="math inline">\(F(x)\)</span> exist everywhere. Then the Taylor series expansion of point <span class="math inline">\(x^{\star}\)</span> is:</p><p><span class="math display">\[
\begin{aligned}
F(x)=F(x^{\star})&+\frac{d}{dx}F(x)|_{x=x^{\star}}(x-x^{\star})\\
&+\frac{1}{2}\frac{d^2}{d^2x}F(x)|_{x=x^{\star}}(x-x^{\star})^2\\
&\vdots \\
&+\frac{1}{n!}\frac{d^n}{d^nx}F(x)|_{x=x^{\star}}(x-x^{\star})^n
\end{aligned}\tag{1}
\]</span></p><p>This series with infinity items can exactly equal the origin analytic function.</p><p>If we only want to approximate the function in a small region near <span class="math inline">\(x^{\star}\)</span>, finite items in equation(1) are usually enough.</p><p>For instance, we have a function <span class="math inline">\(F(x)=\cos(x)\)</span> when <span class="math inline">\(x^{\star}=0\)</span>, then:</p><p><span class="math display">\[
\begin{aligned}
F(x)=\cos(x)&=\cos(0)-\sin(0)(x-0)-\frac{1}{2}\cos(0)(x-0)^2+\cdots\\
&=1-\frac{1}{2}x^2+\frac{1}{24}x^4+\cdots
\end{aligned}\tag{2}
\]</span></p><ul><li><span class="math inline">\(0^{\text{th}}\)</span> order approximation of <span class="math inline">\(F(x)\)</span> near <span class="math inline">\(0\)</span> is <span class="math inline">\(F(x)\approx F_0(x)=1\)</span></li><li><span class="math inline">\(1^{\text{st}}\)</span> order approximation of <span class="math inline">\(F(x)\)</span> near <span class="math inline">\(0\)</span> is <span class="math inline">\(F(x)\approx F_1(x)=1+0\)</span></li><li><span class="math inline">\(2^{\text{nd}}\)</span> order approximation of <span class="math inline">\(F(x)\)</span> near <span class="math inline">\(0\)</span> is <span class="math inline">\(F(x)\approx F_2(x)=1+0-\frac{1}{2}x^2\)</span></li><li><span class="math inline">\(3^{\text{rd}}\)</span> order approximation of <span class="math inline">\(F(x)\)</span> near <span class="math inline">\(0\)</span> is <span class="math inline">\(F(x)\approx F_3(x)=1+0-\frac{1}{2}x^2+0\)</span></li><li><span class="math inline">\(4^{\text{th}}\)</span> order approximation of <span class="math inline">\(F(x)\)</span> near <span class="math inline">\(0\)</span> is <span class="math inline">\(F(x)\approx F_4(x)=1+0-\frac{1}{2}x^2+0+\frac{1}{24}x^4\)</span></li></ul><p>Odd number<span class="math inline">\(^{\text{th}}\)</span> iterm is <span class="math inline">\(0\)</span> because of the value of <span class="math inline">\(\sin(x)\)</span> at <span class="math inline">\(0\)</span> is <span class="math inline">\(0\)</span></p><p>And the <span class="math inline">\(0^{\text{th}},1^{\text{st}},2^{\text{nd}},3^{\text{rd}}\)</span> and <span class="math inline">\(4^{\text{th}}\)</span> approximation of <span class="math inline">\(F(x)\)</span> looks like:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_01_22_04_Taylor_sieres.png></p><p>and, from the figure above we can observe that <span class="math inline">\(F_0(x)\)</span> can only approximate <span class="math inline">\(F(x)\)</span> at <span class="math inline">\(x^{\star}=0\)</span> point. While in the interval between <span class="math inline">\(-1\)</span> and <span class="math inline">\(+1\)</span>, <span class="math inline">\(F(x)\)</span> can be precisely approximated by <span class="math inline">\(F_2(x)\)</span>. In a more wider interval like <span class="math inline">\([-1.6,+1.6]\)</span> to approximate <span class="math inline">\(F(x)\)</span>, <span class="math inline">\(4^{\text{th}}\)</span> order Taylor series are needed.</p><p>Then we can get the conclusion that if we want a certain precise extent in a relatively larger interval we need more items in the Taylor series. And we should pay attention to the interval out of the precise region, such as <span class="math inline">\(F_2(x)\)</span> in the interval <span class="math inline">\((-\infty,-1]\cup [1,+\infty)\)</span> are going away from <span class="math inline">\(F(x)\)</span> as <span class="math inline">\(x\)</span> moving away from <span class="math inline">\(x^{\star}=0\)</span></p><h3 id=vector-form-function>Vector form function<a hidden class=anchor aria-hidden=true href=#vector-form-function>#</a></h3><p>When the performance index is a function of a vector <span class="math inline">\(\mathbf{x}\)</span>, a vector from the Taylor series should be presented. And in the neural network, parameters are variables of the performance index so the vector from the Taylor series is the basic tool in performance learning. Function <span class="math inline">\(F(\mathbf{x})\)</span> can be decomposed in any precise:</p><p><span class="math display">\[
\begin{aligned}
F(\mathbf{x})=F(\mathbf{x}^{\star})&
+\frac{\partial}{\partial x_1}F(\mathbf{x})|_{\mathbf{x}=\mathbf{x}^{\star}}(x_1-x_1^{\star})\\
&+\frac{\partial}{\partial x_2}F(\mathbf{x})|_{\mathbf{x}=\mathbf{x}^{\star}}(x_2-x_2^{\star})+\cdots\\
&+\frac{1}{2}\frac{\partial^2}{\partial^2 x_1}F(\mathbf{x})|_{\mathbf{x}=\mathbf{x}^{\star}}(x_1-x_1^{\star})^2\\
&+\frac{1}{2}\frac{\partial^2}{\partial x_1 \partial x_2}F(\mathbf{x})|_{\mathbf{x}=\mathbf{x}^{\star}}(x_1-x_1^{\star})(x_2-x_2^{\star})
+\cdots\\
&\vdots
\end{aligned}\tag{3}
\]</span></p><p>if we notate the gradient as : <span class="math display">\[
\nabla F(x)=\begin{bmatrix}
\frac{\partial F}{\partial x_1}\\
\frac{\partial F}{\partial x_2}\\
\vdots\\
\frac{\partial F}{\partial x_n}
\end{bmatrix}\tag{4}
\]</span></p><p>then the Taylor series can be written as: <span class="math display">\[
\begin{aligned}
F(\mathbf{x})=F(\mathbf{x}^{\star})&+(\mathbf{x}-\mathbf{x}^{\star})^T\nabla F(x)|_{\mathbf{x}=\mathbf{x}^{\star}}\\
&+\frac{1}{2}(\mathbf{x}-\mathbf{x}^{\star})^T\nabla^2 F(x)|_{\mathbf{x}=\mathbf{x}^{\star}}(x_1-x_1^{\star})\\
& \vdots
\end{aligned}\tag{5}
\]</span></p><p>The coefficients of the second-order item can be written in a matrix form, and it is also called the <strong>Hessian matrix</strong>:</p><p><span class="math display">\[
\nabla^2 F(x) = \begin{bmatrix}
\frac{\partial^2}{\partial^2 x_1}&\cdots&\frac{\partial^2}{\partial x_1 \partial x_n}\\
\vdots&\ddots&\vdots \\
\frac{\partial^2}{\partial x_n \partial x_1}&\cdots&\frac{\partial^2}{\partial^2 x_n}
\end{bmatrix}\tag{6}
\]</span></p><p><strong>Hessian matrix</strong> has many beautiful properties, such as it is always - Square matrix - Symmetric matrix</p><p>In the matrix, the elements on the diagonal are <span class="math inline">\(\frac{\partial^2 F}{\partial^2 x_i}\)</span> is the <span class="math inline">\(2^{\text{nd}}\)</span> derivative along the <span class="math inline">\(x_i\)</span>-axis and in the gradient vector, <span class="math inline">\(\frac{d F}{d x_i}\)</span> is the first derivative along the <span class="math inline">\(x_i\)</span>-axis.</p><p>To calculate the <span class="math inline">\(1^{\text{st}}\)</span> or <span class="math inline">\(2^{\text{nd}}\)</span> derivative along arbitrary deriction <span class="math inline">\(\mathbf{p}\)</span>, we have: 1. <span class="math inline">\(1^{\text{st}}\)</span>-order derivative along <span class="math inline">\(\mathbf{p}\)</span> : <span class="math inline">\(\frac{\mathbf{p}^T\nabla F(\mathbf{x})}{||\mathbf{p}||}\)</span> 2. <span class="math inline">\(2^{\text{nd}}\)</span>-order derivative along <span class="math inline">\(\mathbf{p}\)</span> : <span class="math inline">\(\frac{\mathbf{p}^T\nabla^2 F(\mathbf{x})\mathbf{p}}{||\mathbf{p}||^2}\)</span></p><p>For instance, we have a function <span class="math display">\[
F(\mathbf{x})=x_1^2+2x_2^2\tag{7}
\]</span></p><p>to find the derivative at <span class="math inline">\(\mathbf{x}^{\star}=\begin{bmatrix}0.5\\0.5\end{bmatrix}\)</span> in the direction <span class="math inline">\(\mathbf{p}=\begin{bmatrix}2\\-1\end{bmatrix}\)</span>, we get the derivative at <span class="math inline">\(\mathbf{x}^{\star}=\begin{bmatrix}0.5\\0.5\end{bmatrix}\)</span>:</p><p><span class="math display">\[
\nabla F|_{\mathbf{x}=\mathbf{x}^{\star}} =
\begin{bmatrix}
\frac{\partial F}{\partial x_1}\\
\frac{\partial F}{\partial x_2}
\end{bmatrix}\bigg|_{\mathbf{x}=\mathbf{x}^{\star}}=\begin{bmatrix}
2x_1\\
4x_2
\end{bmatrix}\bigg|_{\mathbf{x}=\mathbf{x}^{\star}}=\begin{bmatrix}
1\\
2
\end{bmatrix}\tag{8}
\]</span></p><p>and the direction <span class="math inline">\(\mathbf{p}=\begin{bmatrix}2\\-1\end{bmatrix}\)</span>, the derivative is</p><p><span class="math display">\[
\frac{\mathbf{p}^T\nabla F(\mathbf{x}^{\star})}{||\mathbf{p}||}=
\frac{\begin{bmatrix}2&-1\end{bmatrix}
\begin{bmatrix}1\\2\end{bmatrix}}{\sqrt{2^2+(-1)^2}}=\frac{0}{\sqrt{5}}=0\tag{9}
\]</span></p><p><span class="math inline">\(0\)</span> is a special number in the whole real numbers set. And the derivative is zero also meaningful in the optimization procedure. To find a zero slop direction we should solve the equation:</p><p><span class="math display">\[
\frac{\mathbf{p}^T\nabla F(\mathbf{x}^{\star})}{||\mathbf{p}||}=0\tag{10}
\]</span></p><p>this means <span class="math inline">\(\mathbf{p}\)</span> can not be <span class="math inline">\(\mathbf{0}\)</span> because <span class="math inline">\(0\)</span> length is illegle. And <span class="math inline">\(\mathbf{p}_{\text{unit}}=\frac{\mathbf{p}}{||\mathbf{p}||}\)</span> is a unit vector along deriction <span class="math inline">\(\mathbf{p}\)</span> so this can be written as:</p><p><span class="math display">\[
\mathbf{p}_{\text{unit}}^T\nabla F(\mathbf{x}^{\star})=0\tag{11}
\]</span></p><p>which means <span class="math inline">\(\mathbf{p}_{\text{unit}}\)</span> is orthogonal to gradient <span class="math inline">\(\nabla F(\mathbf{x}^{\star})\)</span></p><p>Another special deriction is which one has the greatest slop. Assuming the deriction <span class="math inline">\(\mathbf{p}_{\text{unit}}=\frac{\mathbf{p}}{||\mathbf{p}||}\)</span> is the greatest slope, so:</p><p><span class="math display">\[
\mathbf{p}_{\text{unit}}^T\nabla F(\mathbf{x}^{\star})=||\mathbf{p}_{\text{unit}}^T|| \cdot ||\nabla F(\mathbf{x}^{\star})||\cos(\theta)\tag{12}
\]</span></p><p>has the greatest value. We know this can only happen when <span class="math inline">\(\cos(\theta)=1\)</span> which means the direction of the gradient has the greatest slop.</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_01_22_04_unit_circle.png></p><h2 id=necessary-conditions-for-optimality>Necessary Conditions for Optimality<a hidden class=anchor aria-hidden=true href=#necessary-conditions-for-optimality>#</a></h2><p>The main objective of performance learning is to minimize the performance index. So the condition of existence of a minimum performance index should be investigated:</p><ol type=1><li>A strong minimum <span class="math inline">\(\mathbf{x}\)</span> which mean in any deriction <span class="math inline">\(\mathbf{p}_{\text{unit}}\)</span> around this point with a short distance <span class="math inline">\(\delta\)</span> always has <span class="math inline">\(F(\mathbf{x})&lt;F(\mathbf{x}+\delta \mathbf{p}_{\text{unit}})\)</span>, where <span class="math inline">\(\delta \to 0\)</span></li><li>A weak minimum <span class="math inline">\(\mathbf{x}\)</span> which mean in any deriction <span class="math inline">\(\mathbf{p}_{\text{unit}}\)</span> around this point with a short distance <span class="math inline">\(\delta\)</span> always has <span class="math inline">\(F(\mathbf{x})\leq F(\mathbf{x}+\delta \mathbf{p}_{\text{unit}})\)</span>, where <span class="math inline">\(\delta \to 0\)</span></li><li>Global minimum is the minimum one of all weak or strong minimum sets.</li></ol><p>For instance,</p><p><span class="math display">\[
F(x)=3x^4-7x^2-\frac{1}{2}x+6\tag{13}
\]</span></p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_01_22_04_minimums.png></p><ul><li>2 local minimums at near <span class="math inline">\(x_1=-1.1\)</span> and near <span class="math inline">\(x_2=1.1\)</span></li><li>near <span class="math inline">\(x_2=1.1\)</span> gives the global minimum</li></ul><p>If the variable of function is a vector:</p><p><span class="math display">\[
F(\mathbf{x})=(x_2-x_1)^4+8x_1x_2-x_1+x_2+3\tag{14}
\]</span></p><p>and it looks like this:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_01_22_04_vector_performance_index.gif></p><p>in 3-D space. And the contour plot of this function is:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_01_22_05_contour_plot.png></p><p>There are three points in this contour figure:</p><p><img src=https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_05_01_22_05_contour_plot.gif></p><p>and the pink points are the two local minimums, and the black points are called a <em>saddle points</em>.</p><p>These two examples illustrate the minimum points and saddle points. But what conditions are needed to confirm the existence?</p><h3 id=first-order-conditions>First-order Conditions<a hidden class=anchor aria-hidden=true href=#first-order-conditions>#</a></h3><p>Go back to the Taylor series with <span class="math inline">\(\Delta x=x-x^{\star}\neq 0\)</span>, the first item of the series is taken into account:</p><p><span class="math display">\[
\begin{aligned}
F(x)&=F(x^{\star})+\frac{d}{dx}F(x)|_{x=x^{\star}}(x-x^{\star})\\
F(x^{\star}+\Delta x)&=F(x^{\star})+\nabla F(x)\Delta x
\end{aligned}\tag{15}
\]</span></p><p>when <span class="math inline">\(x^{\star}\)</span> is a candidate of minimum point, we want:</p><p><span class="math display">\[
F(x^{\star}+\Delta x)\geq F(x^{\star})\tag{16}
\]</span></p><p>so, in equation(15), <span class="math display">\[
\nabla F(x)\Delta x\geq 0 \tag{17}
\]</span></p><p>are needed. Considering another direction, if <span class="math inline">\(x^{\star}\)</span> is the minimum point, it also has:</p><p><span class="math display">\[
F(x^{\star}-\Delta x)=F(x^{\star})-\nabla F(x)\Delta x\geq F(x^{\star})\tag{18}
\]</span></p><p>then</p><p><span class="math display">\[
\nabla F(x)\Delta x\leq 0 \tag{19}
\]</span></p><p>To satisfy both equation(17) and equation(19) if and only if</p><p><span class="math display">\[
\nabla F(x)\Delta x=0\tag{20}
\]</span></p><p>Because <span class="math inline">\(\Delta x\neq 0\)</span>, so we must have <span class="math inline">\(\nabla F(x)\)</span> when <span class="math inline">\(x\)</span> is the minimum point. <span class="math inline">\(\nabla F(x)=0\)</span> is a necessary but not sufficient condition. The first-order condition has been concluded above.</p><h3 id=second-order-condition>Second-order Condition<a hidden class=anchor aria-hidden=true href=#second-order-condition>#</a></h3><p>The first-order condition does not give a sufficient condition. Now let’s consider the second-order condition. With <span class="math inline">\(\Delta\mathbf{x}=\mathbf{x}-\mathbf{x}^{\star}\neq \mathbf{0}\)</span> and gradient equal to <span class="math inline">\(\mathbf{0}\)</span>, and take them into equation(5) . Then the second-order Taylor series is:</p><p><span class="math display">\[
F(\mathbf{\Delta\mathbf{x}-\mathbf{x}^{\star}})=F(\mathbf{x}^{\star})+\frac{1}{2}\Delta\mathbf{x}^T\nabla^2 F(x)|_{\mathbf{x}=\mathbf{x}^{\star}}\Delta\mathbf{x}\tag{21}
\]</span></p><p>When <span class="math inline">\(||\Delta\mathbf{x}||\)</span> is small, zero-order, first-order, and second-order terms of the Taylor series are precise enough to approximate the original function. When <span class="math inline">\(F(\mathbf{x}^{\star})\)</span> is a strong minimum, the second-order item should be:</p><p><span class="math display">\[
\frac{1}{2}\Delta\mathbf{x}^T\nabla^2 F(x)|_{\mathbf{x}=\mathbf{x}^{\star}}\Delta\mathbf{x}>0
\]</span></p><p>Recalling linear algebra knowledge, positive definite matrix <span class="math inline">\(A\)</span> is:</p><p><span class="math display">\[
\mathbf{z}^T A \mathbf{z}>0 \text{ for any } \mathbf{z}
\]</span></p><p>and positive semidefinite matrix <span class="math inline">\(A\)</span> is:</p><p><span class="math display">\[
\mathbf{z}^T A \mathbf{z}\geq0 \text{ for any } \mathbf{z}
\]</span></p><p>So when the gradient is <span class="math inline">\(\mathbf{0}\)</span> and the second-order derivative is positive definite(semidefinite), this point is a strong(weak) minimum. And positive definite Hessian matrix is a sufficient condition to a strong minimum point, but it’s not a necessary condition. Because when the Hessian matrix is <span class="math inline">\(0\)</span> the third-order item has to be calculated.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><section class=footnotes role=doc-endnotes><hr><ol><li id=fn1 role=doc-endnote><p>Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.<a href=#fnref1 class=footnote-back role=doc-backlink>↩︎</a></p></li><li id=fn2 role=doc-endnote><p>https://en.wikipedia.org/wiki/Competitive_learning<a href=#fnref2 class=footnote-back role=doc-backlink>↩︎</a></p></li></ol></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://anthony-tan.com/tags/artificial-neural-networks/>Artificial Neural Networks</a></li><li><a href=https://anthony-tan.com/tags/artificial-intelligence/>Artificial Intelligence</a></li><li><a href=https://anthony-tan.com/tags/performance-surfaces/>performance surfaces</a></li><li><a href=https://anthony-tan.com/tags/optimum-points/>optimum points</a></li><li><a href=https://anthony-tan.com/tags/performance-learning/>Performance Learning</a></li><li><a href=https://anthony-tan.com/tags/associative-learning/>Associative Learning</a></li><li><a href=https://anthony-tan.com/tags/competitive-learning/>Competitive Learning</a></li><li><a href=https://anthony-tan.com/tags/taylor-series/>Taylor Series</a></li><li><a href=https://anthony-tan.com/tags/necessary-conditions-for-optimality/>Necessary Conditions for Optimality</a></li><li><a href=https://anthony-tan.com/tags/first-order-conditions/>First-order Conditions</a></li><li><a href=https://anthony-tan.com/tags/second-order-condition/>Second-order Condition</a></li><li><a href=https://anthony-tan.com/tags/saddle-points/>Saddle Points</a></li><li><a href=https://anthony-tan.com/tags/global-optimization/>global optimization</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Performance Surfaces and Optimum Points on twitter" href="https://twitter.com/intent/tweet/?text=Performance%20Surfaces%20and%20Optimum%20Points&url=https%3a%2f%2fanthony-tan.com%2fPerformance-Surfaces-and-Optimum-Points%2f&hashtags=ArtificialNeuralNetworks%2cArtificialIntelligence%2cperformancesurfaces%2coptimumpoints%2cPerformanceLearning%2cAssociativeLearning%2cCompetitiveLearning%2cTaylorSeries%2cNecessaryConditionsforOptimality%2cFirst-orderConditions%2cSecond-orderCondition%2cSaddlePoints%2cglobaloptimization"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Performance Surfaces and Optimum Points on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fanthony-tan.com%2fPerformance-Surfaces-and-Optimum-Points%2f&title=Performance%20Surfaces%20and%20Optimum%20Points&summary=Performance%20Surfaces%20and%20Optimum%20Points&source=https%3a%2f%2fanthony-tan.com%2fPerformance-Surfaces-and-Optimum-Points%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Performance Surfaces and Optimum Points on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fanthony-tan.com%2fPerformance-Surfaces-and-Optimum-Points%2f&title=Performance%20Surfaces%20and%20Optimum%20Points"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Performance Surfaces and Optimum Points on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fanthony-tan.com%2fPerformance-Surfaces-and-Optimum-Points%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Performance Surfaces and Optimum Points on whatsapp" href="https://api.whatsapp.com/send?text=Performance%20Surfaces%20and%20Optimum%20Points%20-%20https%3a%2f%2fanthony-tan.com%2fPerformance-Surfaces-and-Optimum-Points%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Performance Surfaces and Optimum Points on telegram" href="https://telegram.me/share/url?text=Performance%20Surfaces%20and%20Optimum%20Points&url=https%3a%2f%2fanthony-tan.com%2fPerformance-Surfaces-and-Optimum-Points%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><figure class=article-discussion><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//anthony-tan-com.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></figure></article></main><footer class=footer><span>&copy; 2022 <a href=https://anthony-tan.com>Anthony's Blogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>