<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Artificial Neural Networks | Anthony's Blogs</title><meta name=keywords content><meta name=description content="Machine Learning, Reinforcement Learning, Algorithms, Math"><meta name=author content="Anthony Tan"><link rel=canonical href=https://anthony-tan.com/tags/artificial-neural-networks/><link crossorigin=anonymous href=../../assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><link rel=icon href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://anthony-tan.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://anthony-tan.com/logo.png><link rel=apple-touch-icon href=https://anthony-tan.com/logo.png><link rel=mask-icon href=https://anthony-tan.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://anthony-tan.com/tags/artificial-neural-networks/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-12","auto"),ga("send","pageview"))</script><meta property="og:title" content="Artificial Neural Networks"><meta property="og:description" content="Machine Learning, Reinforcement Learning, Algorithms, Math"><meta property="og:type" content="website"><meta property="og:url" content="https://anthony-tan.com/tags/artificial-neural-networks/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Artificial Neural Networks"><meta name=twitter:description content="Machine Learning, Reinforcement Learning, Algorithms, Math"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://anthony-tan.com accesskey=h title="Anthony's Blogs (Alt + H)">Anthony's Blogs</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://anthony-tan.com/machine_learning/ title="Machine Learning"><span>Machine Learning</span></a></li><li><a href=https://anthony-tan.com/deep_learning/ title="Deep Learning"><span>Deep Learning</span></a></li><li><a href=https://anthony-tan.com/reinforcement_learning/ title="Reinforcement Learning"><span>Reinforcement Learning</span></a></li><li><a href=https://anthony-tan.com/math/ title=Math><span>Math</span></a></li><li><a href=https://anthony-tan.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://anthony-tan.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://anthony-tan.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://anthony-tan.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://anthony-tan.com>Home</a>&nbsp;»&nbsp;<a href=https://anthony-tan.com/tags/>Tags</a></div><h1>Artificial Neural Networks</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2>Drawbacks of Backpropagation</h2></header><section class=entry-content><p>Preliminaries ‘An Introduction to Backpropagation and Multilayer Perceptrons’ ‘The Backpropagation Algorithm’ Speed Backpropagation up 1 BP algorithm has been described in ‘An Introduction to Backpropagation and Multilayer Perceptrons’. And the implementation of the BP algorithm has been recorded at ‘The Backpropagation Algorithm’. BP has worked in many applications for many years, but there are too many drawbacks in the process. The basic BP algorithm is too slow for most practical applications that it might take days or even weeks in training....</p></section><footer class=entry-footer><span title="2020-01-07 10:14:53 +0000 UTC">January 7, 2020</span>&nbsp;·&nbsp;<span title="2022-05-03 10:39:43 +0800 +0800">(Last Modification: May 3, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Drawbacks of Backpropagation" href=https://anthony-tan.com/Drawbacks-of-Backpropagation/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Backpropagation, Batch Training, and Incremental Training</h2></header><section class=entry-content><p>Preliminaries Calculus 1,2 Linear Algebra Batch v.s. Incremental Training1 In both LMS and BP algorithms, the error in each update process step is not MSE but SE \(e=t_i-a_i\) which is calculated just by a data point of the training set. This is called a stochastic gradient descent algorithm. And why it is called ‘stochastic’ is because error at every iterative step is approximated by randomly selected train data points but not the whole data set....</p></section><footer class=entry-footer><span title="2020-01-02 17:49:55 +0000 UTC">January 2, 2020</span>&nbsp;·&nbsp;<span title="2022-05-03 10:39:43 +0800 +0800">(Last Modification: May 3, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Backpropagation, Batch Training, and Incremental Training" href=https://anthony-tan.com/Backpropagation-Batch-Training-and-Incremental-Training/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>The Backpropagation Algorithm</h2></header><section class=entry-content><p>Preliminaries An Introduction to Backpropagation and Multilayer Perceptrons Culculus 1,2 Linear algebra Jacobian matrix Architecture and Notations1 We have seen a three-layer network is flexible in approximating functions(An Introduction to Backpropagation and Multilayer Perceptrons). If we had a more-than-three-layer network, it could be used to approximate any functions as accurately as we want. However, another trouble that came to us is the learning rules. This problem almost killed neural networks in the 1970s....</p></section><footer class=entry-footer><span title="2020-01-01 14:26:55 +0000 UTC">January 1, 2020</span>&nbsp;·&nbsp;<span title="2022-05-03 10:39:43 +0800 +0800">(Last Modification: May 3, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to The Backpropagation Algorithm" href=https://anthony-tan.com/The-Backpropagation-Algorithm/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>An Introduction to Backpropagation and Multilayer Perceptrons</h2></header><section class=entry-content><p>Preliminaries Performance learning Perceptron learning rule Supervised Hebbian learning LMS Form LMS to Backpropagation1 The LMS algorithm is a kind of ‘performance learning’. And we have studied several learning rules(algorithms) till now, such as ‘Perceptron learning rule’ and ‘Supervised Hebbian learning’. And they were based on the idea of the physical mechanism of biological neuron networks.
Then performance learning was represented. Because of its outstanding performance, we go further and further away from natural intelligence into performance learning....</p></section><footer class=entry-footer><span title="2019-12-31 10:29:33 +0000 UTC">December 31, 2019</span>&nbsp;·&nbsp;<span title="2022-05-02 18:43:13 +0800 +0800">(Last Modification: May 2, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to An Introduction to Backpropagation and Multilayer Perceptrons" href=https://anthony-tan.com/An-Introduction-to-Backpropagation-and-Multilayer-Perceptrons/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Widrow-Hoff Learning</h2></header><section class=entry-content><p>Preliminaries ‘Performance Surfaces and Optimum Points’ Linear algebra stochastic approximation Probability Theory ADALINE, LMS, and Widrow-Hoff learning1 Performance learning had been discussed. But we have not used it in any neural network. In this post, we talk about an important application of performance learning. And this new neural network was invented by Frank Widrow and his graduate student Marcian Hoff in 1960. It was almost the same time as Perceptron was developed which had been discussed in ‘Perceptron Learning Rule’....</p></section><footer class=entry-footer><span title="2019-12-23 18:51:59 +0000 UTC">December 23, 2019</span>&nbsp;·&nbsp;<span title="2022-05-03 10:39:43 +0800 +0800">(Last Modification: May 3, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Widrow-Hoff Learning" href=https://anthony-tan.com/Widrow-Hoff-Learning/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Conjugate Gradient</h2></header><section class=entry-content><p>Preliminaries ‘steepest descent method’ “Newton’s method” Conjugate Gradient1 We have learned ‘steepest descent method’ and “Newton’s method”. The main advantage of Newton’s method is the speed, it converges quickly. And the main advantage of the steepest descent method guarantees to converge to a local minimum. But the limit of Newton’s method is that it needs too many resources for both computation and storage when the number of parameters is large....</p></section><footer class=entry-footer><span title="2019-12-21 13:40:24 +0000 UTC">December 21, 2019</span>&nbsp;·&nbsp;<span title="2022-05-03 10:39:43 +0800 +0800">(Last Modification: May 3, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Conjugate Gradient" href=https://anthony-tan.com/Conjugate-Gradient/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Newton's Method</h2></header><section class=entry-content><p>Preliminaries ‘steepest descent algorithm’ Linear Algebra Calculus 1,2 Newton’s Method1 Taylor series gives us the conditions for minimum points based on both first-order items and the second-order item. And first-order item approximation of a performance index function produced a powerful algorithm for locating the minimum points which we call ‘steepest descent algorithm’.
Now we want to have an insight into the second-order approximation of a function to find out whether there is an algorithm that can also work as a guide to the minimum points....</p></section><footer class=entry-footer><span title="2019-12-21 11:39:56 +0000 UTC">December 21, 2019</span>&nbsp;·&nbsp;<span title="2022-05-03 10:39:43 +0800 +0800">(Last Modification: May 3, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Newton's Method" href=https://anthony-tan.com/Newton_s-Method/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Steepest Descent Method</h2></header><section class=entry-content><p>Preliminaries ‘An Introduction to Performance Optimization’ Linear algebra Calculus 1,2 Direction Based Algorithm and a Variation1 This post describes a direction searching algorithm(\(\mathbf{x}_{k}\)). And its variation gives a way to estimate step length (\(\alpha_k\)).
Steepest Descent To find the minimum points of a performance index by an iterative algorithm, we want to decrease the value of the performance index step by step which looks like going down from the top of the hill....</p></section><footer class=entry-footer><span title="2019-12-20 11:39:19 +0000 UTC">December 20, 2019</span>&nbsp;·&nbsp;<span title="2022-05-03 10:39:43 +0800 +0800">(Last Modification: May 3, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Steepest Descent Method" href=https://anthony-tan.com/Steepest-Descent-Method/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>An Introduction to Performance Optimization</h2></header><section class=entry-content><p>Preliminaries Nothing Performance Optimization1 Taylor series had been used for analyzing the performance surface and locating the optimum points of a certain performance index. This short post is a brief introduction to performance optimization and the following posts are the samples of three optimization algorithms categories:
‘Steepest Descent’ “Newton’s Method” ‘Conjugate Gradient’ Recall the analysis of the performance index, which is a function of the parameters of the model....</p></section><footer class=entry-footer><span title="2019-12-20 11:38:50 +0000 UTC">December 20, 2019</span>&nbsp;·&nbsp;<span title="2022-05-03 10:39:43 +0800 +0800">(Last Modification: May 3, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to An Introduction to Performance Optimization" href=https://anthony-tan.com/An-Introduction-to-Performance-Optimization/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Quadratic Functions</h2></header><section class=entry-content><p>Preliminaries Linear algebra Calculus 1,2 Taylor series Quadratic Functions1 Quadratic function, a type of performance index, is universal. One of its key properties is that it can be represented in a second-order Taylor series precisely.
\[ F(\mathbf{x})=\frac{1}{2}\mathbf{x}^TA\mathbf{x}+\mathbf{d}\mathbf{x}+c\tag{1} \]
where \(A\) is a symmetric matrix(if it is not symmetric, it can be easily converted into symmetric). And recall the property of gradient:
\[ \nabla (\mathbf{h}^T\mathbf{x})=\nabla (\mathbf{x}^T\mathbf{h})=\mathbf{h}\tag{2} \]
and
\[ \nabla (\mathbf{x}^TQ\mathbf{x})=Q\mathbf{x}+Q^T\mathbf{x}=2Q\mathbf{x}\tag{3} \]...</p></section><footer class=entry-footer><span title="2019-12-19 15:45:37 +0000 UTC">December 19, 2019</span>&nbsp;·&nbsp;<span title="2022-05-01 22:45:32 +0800 +0800">(Last Modification: May 1, 2022)</span>&nbsp;·&nbsp;Anthony Tan</footer><a class=entry-link aria-label="post link to Quadratic Functions" href=https://anthony-tan.com/Quadratic-Functions/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://anthony-tan.com/tags/artificial-neural-networks/page/2/>Next Page »</a></nav></footer></main><footer class=footer><span>&copy; 2022 <a href=https://anthony-tan.com>Anthony's Blogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>