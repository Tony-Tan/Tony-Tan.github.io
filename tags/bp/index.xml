<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>BP on Anthony&#39;s Blogs</title>
    <link>https://anthony-tan.com/tags/bp/</link>
    <description>Recent content in BP on Anthony&#39;s Blogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 07 Jan 2020 10:14:53 +0000</lastBuildDate><atom:link href="https://anthony-tan.com/tags/bp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Drawbacks of Backpropagation</title>
      <link>https://anthony-tan.com/Drawbacks-of-Backpropagation/</link>
      <pubDate>Tue, 07 Jan 2020 10:14:53 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Drawbacks-of-Backpropagation/</guid>
      <description>Preliminaries ‘An Introduction to Backpropagation and Multilayer Perceptrons’ ‘The Backpropagation Algorithm’  Speed Backpropagation up 1 BP algorithm has been described in ‘An Introduction to Backpropagation and Multilayer Perceptrons’. And the implementation of the BP algorithm has been recorded at ‘The Backpropagation Algorithm’. BP has worked in many applications for many years, but there are too many drawbacks in the process. The basic BP algorithm is too slow for most practical applications that it might take days or even weeks in training.</description>
    </item>
    
    <item>
      <title>The Backpropagation Algorithm</title>
      <link>https://anthony-tan.com/The-Backpropagation-Algorithm/</link>
      <pubDate>Wed, 01 Jan 2020 14:26:55 +0000</pubDate>
      
      <guid>https://anthony-tan.com/The-Backpropagation-Algorithm/</guid>
      <description>Preliminaries An Introduction to Backpropagation and Multilayer Perceptrons Culculus 1,2 Linear algebra Jacobian matrix  Architecture and Notations1 We have seen a three-layer network is flexible in approximating functions(An Introduction to Backpropagation and Multilayer Perceptrons). If we had a more-than-three-layer network, it could be used to approximate any functions as accurately as we want. However, another trouble that came to us is the learning rules. This problem almost killed neural networks in the 1970s.</description>
    </item>
    
    <item>
      <title>An Introduction to Backpropagation and Multilayer Perceptrons</title>
      <link>https://anthony-tan.com/An-Introduction-to-Backpropagation-and-Multilayer-Perceptrons/</link>
      <pubDate>Tue, 31 Dec 2019 10:29:33 +0000</pubDate>
      
      <guid>https://anthony-tan.com/An-Introduction-to-Backpropagation-and-Multilayer-Perceptrons/</guid>
      <description>Preliminaries Performance learning Perceptron learning rule Supervised Hebbian learning LMS  Form LMS to Backpropagation1 The LMS algorithm is a kind of ‘performance learning’. And we have studied several learning rules(algorithms) till now, such as ‘Perceptron learning rule’ and ‘Supervised Hebbian learning’. And they were based on the idea of the physical mechanism of biological neuron networks.
Then performance learning was represented. Because of its outstanding performance, we go further and further away from natural intelligence into performance learning.</description>
    </item>
    
  </channel>
</rss>
