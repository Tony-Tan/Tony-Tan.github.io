<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>EM algorithm on Anthony&#39;s Blogs</title>
    <link>https://anthony-tan.com/tags/em-algorithm/</link>
    <description>Recent content in EM algorithm on Anthony&#39;s Blogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 07 Mar 2020 12:04:00 +0000</lastBuildDate><atom:link href="https://anthony-tan.com/tags/em-algorithm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>An Introduction to Combining Models</title>
      <link>https://anthony-tan.com/An-Introduction-to-Combining-Models/</link>
      <pubDate>Sat, 07 Mar 2020 12:04:00 +0000</pubDate>
      
      <guid>https://anthony-tan.com/An-Introduction-to-Combining-Models/</guid>
      <description>Preliminaries ‘Mixtures of Gaussians’ Basic machine learning concepts  Combining Models1 The mixture of Gaussians had been discussed in the post ‘Mixtures of Gaussians’. It was used to introduce the ‘EM algorithm’ but it gave us the inspiration of improving model performance.
All models we have studied, besides neural networks, are all single-distribution models. That is just like that, to solve a problem we invite an expert who is very good at this kind of problem, then we just do whatever the expert said.</description>
    </item>
    
    <item>
      <title>EM Algorithm</title>
      <link>https://anthony-tan.com/EM-Algorithm/</link>
      <pubDate>Thu, 05 Mar 2020 20:04:15 +0000</pubDate>
      
      <guid>https://anthony-tan.com/EM-Algorithm/</guid>
      <description>Preliminaries Gaussian distribution log-likelihood Calculus  partial derivative Lagrange multiplier   EM Algorithm for Gaussian Mixture1 Analysis Maximizing likelihood could not be used in the Gaussian mixture model directly, because of its severe defects which we have come across at ‘Maximum Likelihood of Gaussian Mixtures’. With the inspiration of K-means, a two-step algorithm was developed.
The objective function is the log-likelihood function:
\[ \begin{aligned} \ln \Pr(\mathbf{x}|\mathbf{\pi},\mathbf{\mu},\Sigma)&amp;amp;=\ln (\Pi_{n=1}^N\sum_{j=1}^{K}\pi_k\mathcal{N}(\mathbf{x}|\mathbf{\mu}_k,\Sigma_k))\\ &amp;amp;=\sum_{n=1}^{N}\ln \sum_{j=1}^{K}\pi_j\mathcal{N}(\mathbf{x}_n|\mathbf{\mu}_j,\Sigma_j)\\ \end{aligned}\tag{1} \] ### \(\mu_k\)</description>
    </item>
    
  </channel>
</rss>
