<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Jacobian matrix on Anthony&#39;s Blogs</title>
    <link>https://anthony-tan.com/tags/jacobian-matrix/</link>
    <description>Recent content in Jacobian matrix on Anthony&#39;s Blogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 01 Jan 2020 14:26:55 +0000</lastBuildDate><atom:link href="https://anthony-tan.com/tags/jacobian-matrix/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Backpropagation Algorithm</title>
      <link>https://anthony-tan.com/The-Backpropagation-Algorithm/</link>
      <pubDate>Wed, 01 Jan 2020 14:26:55 +0000</pubDate>
      
      <guid>https://anthony-tan.com/The-Backpropagation-Algorithm/</guid>
      <description>Preliminaries An Introduction to Backpropagation and Multilayer Perceptrons Culculus 1,2 Linear algebra Jacobian matrix  Architecture and Notations1 We have seen a three-layer network is flexible in approximating functions(An Introduction to Backpropagation and Multilayer Perceptrons). If we had a more-than-three-layer network, it could be used to approximate any functions as accurately as we want. However, another trouble that came to us is the learning rules. This problem almost killed neural networks in the 1970s.</description>
    </item>
    
  </channel>
</rss>
