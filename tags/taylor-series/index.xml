<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Taylor series on Anthony&#39;s Blogs</title>
    <link>https://anthony-tan.com/tags/taylor-series/</link>
    <description>Recent content in Taylor series on Anthony&#39;s Blogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 21 Dec 2019 11:39:56 +0000</lastBuildDate><atom:link href="https://anthony-tan.com/tags/taylor-series/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Newton&#39;s Method</title>
      <link>https://anthony-tan.com/Newton_s-Method/</link>
      <pubDate>Sat, 21 Dec 2019 11:39:56 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Newton_s-Method/</guid>
      <description>Preliminaries ‘steepest descent algorithm’ Linear Algebra Calculus 1,2  Newton’s Method1 Taylor series gives us the conditions for minimum points based on both first-order items and the second-order item. And first-order item approximation of a performance index function produced a powerful algorithm for locating the minimum points which we call ‘steepest descent algorithm’.
Now we want to have an insight into the second-order approximation of a function to find out whether there is an algorithm that can also work as a guide to the minimum points.</description>
    </item>
    
    <item>
      <title>Quadratic Functions</title>
      <link>https://anthony-tan.com/Quadratic-Functions/</link>
      <pubDate>Thu, 19 Dec 2019 15:45:37 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Quadratic-Functions/</guid>
      <description>Preliminaries Linear algebra Calculus 1,2 Taylor series  Quadratic Functions1 Quadratic function, a type of performance index, is universal. One of its key properties is that it can be represented in a second-order Taylor series precisely.
\[ F(\mathbf{x})=\frac{1}{2}\mathbf{x}^TA\mathbf{x}+\mathbf{d}\mathbf{x}+c\tag{1} \]
where \(A\) is a symmetric matrix(if it is not symmetric, it can be easily converted into symmetric). And recall the property of gradient:
\[ \nabla (\mathbf{h}^T\mathbf{x})=\nabla (\mathbf{x}^T\mathbf{h})=\mathbf{h}\tag{2} \]
and
\[ \nabla (\mathbf{x}^TQ\mathbf{x})=Q\mathbf{x}+Q^T\mathbf{x}=2Q\mathbf{x}\tag{3} \]</description>
    </item>
    
    <item>
      <title>Performance Surfaces and Optimum Points</title>
      <link>https://anthony-tan.com/Performance-Surfaces-and-Optimum-Points/</link>
      <pubDate>Thu, 19 Dec 2019 08:57:53 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Performance-Surfaces-and-Optimum-Points/</guid>
      <description>Preliminaries Perceptron learning algorithm Hebbian learning algorithm Linear algebra  Neural Network Training Technique1 Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:</description>
    </item>
    
  </channel>
</rss>
