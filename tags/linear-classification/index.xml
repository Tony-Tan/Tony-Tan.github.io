<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>linear classification on Anthony&#39;s Blogs</title>
    <link>https://anthony-tan.com/tags/linear-classification/</link>
    <description>Recent content in linear classification on Anthony&#39;s Blogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 20 Feb 2020 16:13:30 +0000</lastBuildDate><atom:link href="https://anthony-tan.com/tags/linear-classification/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>An Introduction to Probabilistic Generative Models</title>
      <link>https://anthony-tan.com/An-Introduction-to-Probabilistic-Generative-Models/</link>
      <pubDate>Thu, 20 Feb 2020 16:13:30 +0000</pubDate>
      
      <guid>https://anthony-tan.com/An-Introduction-to-Probabilistic-Generative-Models/</guid>
      <description>Preliminaries Probability   Bayesian Formular  Calculus  Probabilistic Generative Models1 The generative model used for making decisions contains an inference step and a decision step:
Inference step is to calculate \(\Pr(\mathcal{C}_k|\mathbf{x})\) which means the probability of \(\mathbf{x}\) belonging to the class \(\mathcal{C}_k\) given \(\mathbf{x}\) Decision step is to make a decision based on \(\Pr(\mathcal{C}_k|\mathbf{x})\) which was calculated in step 1  In this post, we just give an introduction and a framework for the probabilistic generative model in classification.</description>
    </item>
    
    <item>
      <title>Fisher Linear Discriminant(LDA)</title>
      <link>https://anthony-tan.com/Fisher-Linear-Discriminant/</link>
      <pubDate>Wed, 19 Feb 2020 17:01:38 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Fisher-Linear-Discriminant/</guid>
      <description>Preliminaries linear algebra   inner multiplication projection  Idea of Fisher linear discriminant1 ‘Least-square method’ in classification can only deal with a small set of tasks. That is because it was designed for the regression task. Then we come to the famous Fisher linear discriminant. This method is also discriminative for it gives directly the class to which the input \(\mathbf{x}\) belongs. Assuming that the linear function
\[ y=\mathbf{w}^T\mathbf{x}+w_0\tag{1} \]</description>
    </item>
    
    <item>
      <title>Discriminant Functions and Decision Boundary</title>
      <link>https://anthony-tan.com/Discriminant-Functions-and-Decision-Boundary/</link>
      <pubDate>Mon, 17 Feb 2020 16:15:28 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Discriminant-Functions-and-Decision-Boundary/</guid>
      <description>Preliminaries convex definition linear algebra   vector length vector direction  Discriminant Function in Classification The discriminant function or discriminant model is on the other side of the generative model. And we, here, have a look at the behavior of the discriminant function in linear classification.1
In the post ‘Least Squares Classification’, we have seen, in a linear classification task, the decision boundary is a line or hyperplane by which we separate two classes.</description>
    </item>
    
    <item>
      <title>Least Squares in Classification</title>
      <link>https://anthony-tan.com/Least-Squares-in-Classification/</link>
      <pubDate>Mon, 17 Feb 2020 12:39:31 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Least-Squares-in-Classification/</guid>
      <description>Preliminaries A Simple Linear Regression Least Squares Estimation From Linear Regression to Linear Classification pseudo-inverse  Least Squares for Classification1 Least-squares for linear regression had been talked about in ‘Simple Linear Regression’. And in this post, we want to find out whether this powerful algorithm can be used in classification.
Recalling the distinction between the properties of classification and regression, two points need to be emphasized again(‘From Linear Regression to Linear Classification’):</description>
    </item>
    
    <item>
      <title>From Linear Regression to Linear Classification</title>
      <link>https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/</link>
      <pubDate>Mon, 17 Feb 2020 11:20:11 +0000</pubDate>
      
      <guid>https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/</guid>
      <description>Preliminaries An Introduction to Linear Regression A Simple Linear Regression Bayesian theorem Feature extraction  Recall Linear Regression The goal of a regression problem is to find out a function or hypothesis that given an input \(\mathbf{x}\), it can make a prediction \(\hat{y}\) to estimate the target. Both the target \(y\) and prediction \(\hat{y}\) here are continuous. They have the properties of numbers1:
 Consider 3 inputs \(\mathbf{x}_1\), \(\mathbf{x}_2\) and \(\mathbf{x}_3\) and their coresponding targets are \(y_1=0\), \(y_2=1\) and \(y_3=2\).</description>
    </item>
    
  </channel>
</rss>
