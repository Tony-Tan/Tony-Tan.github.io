<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>bagging on Anthony&#39;s Blogs</title>
    <link>https://anthony-tan.com/tags/bagging/</link>
    <description>Recent content in bagging on Anthony&#39;s Blogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 07 Mar 2020 12:04:00 +0000</lastBuildDate><atom:link href="https://anthony-tan.com/tags/bagging/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>An Introduction to Combining Models</title>
      <link>https://anthony-tan.com/An-Introduction-to-Combining-Models/</link>
      <pubDate>Sat, 07 Mar 2020 12:04:00 +0000</pubDate>
      
      <guid>https://anthony-tan.com/An-Introduction-to-Combining-Models/</guid>
      <description>Preliminaries ‘Mixtures of Gaussians’ Basic machine learning concepts  Combining Models1 The mixture of Gaussians had been discussed in the post ‘Mixtures of Gaussians’. It was used to introduce the ‘EM algorithm’ but it gave us the inspiration of improving model performance.
All models we have studied, besides neural networks, are all single-distribution models. That is just like that, to solve a problem we invite an expert who is very good at this kind of problem, then we just do whatever the expert said.</description>
    </item>
    
  </channel>
</rss>
