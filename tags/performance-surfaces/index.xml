<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>performance surfaces on Anthony&#39;s Blogs</title>
    <link>https://anthony-tan.com/tags/performance-surfaces/</link>
    <description>Recent content in performance surfaces on Anthony&#39;s Blogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 19 Dec 2019 08:57:53 +0000</lastBuildDate><atom:link href="https://anthony-tan.com/tags/performance-surfaces/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Performance Surfaces and Optimum Points</title>
      <link>https://anthony-tan.com/Performance-Surfaces-and-Optimum-Points/</link>
      <pubDate>Thu, 19 Dec 2019 08:57:53 +0000</pubDate>
      
      <guid>https://anthony-tan.com/Performance-Surfaces-and-Optimum-Points/</guid>
      <description>Preliminaries Perceptron learning algorithm Hebbian learning algorithm Linear algebra  Neural Network Training Technique1 Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:</description>
    </item>
    
  </channel>
</rss>
