[{"content":"\n\n\n## Preliminaries\n\n1. Linear Algebra(the concepts of space, vector)\n2. Calculus \n\n## What is Linear Regression\n\nLinear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors). \n\n### Machine Learning and Statistical Learning \n\nMachine learning and statistical learning are similar but have some distinctions. In machine learning, models, regression models, or classification models, are used to predict the outputs of the new incoming inputs. \n\nIn contrast, in statistical learning, regression and classification are employed to model the data to find out the hidden relations among the inputs. In other words, the models of data, no matter they are regression or classification, or something else. They are used to analyze the mechanism behind the data.\n\n\n## \"Linear\"\n\nLinear is a property of the operation $f(\\cdot)$, which has the following two properties:\n\n1. $f(a\\mathbf{x})=af(\\mathbf{x})$\n2. $f(\\mathbf{x}+\\mathbf{y})=f(\\mathbf{x})+f(\\mathbf{y})$\n\n\nwhere $a$ is a scalar. Then we say $f(\\cdot)$ is linear or $f(\\cdot)$ has a linearity property.\n\nThe linear operation can be represented as a matrix. And when a 2-dimensional linear operation was drawn on the paper, it is a line. Maybe that is why it is named linear, I guess.\n\n## \"Regression\"\n\nIn statistical or machine learning, regression is a crucial part of the whole field. And, the other part is the well-known classification. If we have a close look at the outputs data type, one distinction between them is that the output of regression is continuous but the output of classification is discrete.\n\n\n\n## What is linear regression\n\nLinear regression is a regression model. All parameters in the model are linear, like:\n\n$$\nf(\\mathbf{x})=w_1x_1+w_2x_2+w_3x_3\\tag{1}\n$$\n\nwhere the $w_n$ where $n=1,2,3$ are the parameters of the model, the output $f(\\mathbf{x})$ can be written as $t$ for 1-deminsional outputs (or $\\mathbf{t}$ for multi-deminsional outputs). \n\n$f(\\mathbf{w})$ is linear:\n\n$$\n\\begin{aligned}\nf(a\\cdot\\mathbf{w})\u0026=aw_1x_1+aw_2x_2+aw_3x_3=a\\cdot f(\\mathbf{w}) \\\\\nf(\\mathbf{w}+\\mathbf{v})\u0026=(w_1+v_1)x_1+(w_2+v_2)x_2+(w_3+v_3)x_3\\\\\n\u0026=w_1x_1+v_1x_1+w_2x_2+v_2x_2+w_3x_3+v_3x_3\\\\\n\u0026=f(\\mathbf{w})+f(\\mathbf{v})\n\\end{aligned}\\tag{2}\n$$\n\nwhere $a$ is a scalar, and $\\mathbf{v}$ is in the same space with $\\mathbf{w}$\n\nQ.E.D\n\nThere is also another view that the linear property of models is also for $\\mathbf{x}$, the input. \n\nBut the following model\n\n$$\nt=f(\\mathbf{x})=w_1\\log(x_1)+w_2\\sin(x_2)\\tag{3}\n$$\n\nis a case of linear regression problem in our definition. But from the second point of view, it is not linear for inputs $\\mathbf{x}$. However, this is not an unsolvable contradiction. If we use:\n\n$$\ny_1= \\log(x_1)\\\\\ny_2= \\sin(x_2)\\tag{4}\n$$\n\nto replace the $\\log$ and $\\sin$ in equation (3), we get again\n\n$$\nt=f(\\mathbf{y})=w_1y_1+w_2y_2\\tag{5}\n$$\n\na linear operation for both input $\\mathbf{y}=\\begin{bmatrix}y_1\\;y_2\\end{bmatrix}^T$ and parameters $\\mathbf{z}$ .\n\nThe tranformation, equation(4), is called **feature extraction**. $\\mathbf{y}$ is called features, and $\\log$ and $\\sin$ are called **basis function**s\n\n\n## An Example \nThis example is taken from (James2013[^1]), It is about the sale between different kinds of advertisements. I downloaded the data set from [http://faculty.marshall.usc.edu/gareth-james/ISL/data.html](http://faculty.marshall.usc.edu/gareth-james/ISL/data.html). It's a CSV file, including 200 rows.\nHere I draw 3 pictures using 'matplotlib' to make the data more visible. They are advertisements for 'TV', 'Radio', 'Newspaper' to 'Sales' respectively.\n\n![](https://raw.githubusercontent.com/Tony-Tan/picgo_images_bed/master/2022_04_23_21_Introduction-to-Linear-Regression_total)\n\nFrom these figures, we can find TV ads and Sales looks like having a stronger relationship than radio ads and sales. However, the Newspaper ads and Sales look independent.\n\nFor statistical learning, we should take statistical methods to investigate the relation in the data. And in machine learning, to a certain input, predicting an output is what we are concerned.\n\n\n\n\n## Why Linear Regression \n\nLinear regression has been used for more than 200 years, and it's always been our first class of statistical learning or machine learning. Here we list 3 practical elements of linear regression, which are essential for the whole subject:\n\n1. It is still working in some areas. Although more complicated models have been built, they could not be replaced totally.\n2. It is a good jump-off point to the other more feasible and adorable models, which may be an extension or generation of naive linear regression\n3. Linear regression is easy, so it is possible to be analyzed mathematically.\n\nThis is why linear regression is always our first step to learn machine learning and statistical learning. And by now, this works pretty well.\n\n\n## A Probabilistic View\n\nMachine learning or statistical learning can be described from two different views - Bayesian and Frequentist. They both worked well for some different instances, but they also have their limitations. The Bayesian view of the linear regression will be talked about as well later.\n\nBayesian statisticians thought the input $\\mathbf{x}$, the output $t$, and the parameter $\\mathbf{w}$ are all random variables, while the frequentist does not think so. Bayesian statisticians predict the unknown input $\\mathbf{x}_0$ by forming the distribution $\\mathbb{P}(t_0|\\mathbf{x}_0)$ and then sampling from it. To achieve this goal, we must build the $\\mathbb{P}(t|\\mathbf{x})$ firstly. This is the modeling progress, or we can call it learning progress.\n\n## References\n\n[^1]: James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.","permalink":"https://anthony-tan.com/deep_learning/introduction-to-linear-regression/","summary":"## Preliminaries\n\n1. Linear Algebra(the concepts of space, vector)\n2. Calculus \n\n## What is Linear Regression\n\nLinear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors). \n\n### Machine Learning and Statistical Learning \n\nMachine learning and statistical learning are similar but have some distinctions.","title":"An Introduction to Linear Regression"},{"content":"About Me Hi, I\u0026rsquo;m Anthony Tan(Ë∞≠Âçá). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years. So far, I\u0026rsquo;ve learned and reviewed some papers that are necessary for reinforcement learning research, and some mathematics, like calculus, linear algebra, probability, and so on.\nHowever the bigger one in the picture is me, Tony, and the smaller one is my dog, Potato(He was too young to take a shower when we take the picture, and now he is no more a dirty puppyüòÄ)\nWhy the blogs These blogs here are used to simply explain what I have learned, according to the Feyman Technique. And blogging what I\u0026rsquo;ve just learned is the most important part of learning. The whole process is:\n Choosing a concept or theory I would like to know(collecting necessary materials )  Outlining what prior knowledge of this concept or theory Taking note of this prior knowledge   Try to explain the new theory to readers of the website without any new words and concepts in the theory (draft) Go back to the source and fill in the gap in understanding Simplify the explaining(rewrite and post)  What in blogs These blogs contain:\n Mathematics Neuroscience Algorithms  Deep Learning Reinforcement Learning    And some of these posts might also be represented by videos on my YouTube channel.\n","permalink":"https://anthony-tan.com/about/","summary":"About Me Hi, I\u0026rsquo;m Anthony Tan(Ë∞≠Âçá). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years.","title":""}]