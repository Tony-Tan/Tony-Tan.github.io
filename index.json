[{"content":"Preliminaries Linear Algebra(the concepts of space, vector) Calculus  Notations of Linear Regression1 We have already created a simple linear model in the post “An Introduction to Linear Regression”. According to the definition of linearity, we can develop the simplest linear regression model:\n\\[ Y\\sim w_1X+w_0\\tag{1} \\]\nwhere the symbol \\(\\sim\\) is read as “is approximately modeled as”. Equation (1) can also be described as “regressing \\(Y\\) on \\(X\\)(or \\(Y\\) onto \\(X\\))”.\nGo back to the example that was given in “An Introduction to Linear Regression”. Combining with the equation (1), we get a model of the budget for TV advertisement and sales:\n\\[ \\text{Sales}=w_1\\times \\text{TV}+ w_0\\tag{2} \\]\nAssuming we have a machine here, which can turn grain into flour, the input is the grain, \\(X\\) in equation (1), and the output is flour, \\(Y\\). Accordingly, \\(\\mathbf{w}\\) is the gears in the machine.\nThen the mathematically model is:\n\\[ y=\\hat{w_1}x+\\hat{w_0}\\tag{3} \\]\nThe hat symbol “\\(\\;\\hat{}\\;\\)” is used to present that this variable is a prediction, which means it is not the true value of the variable but a conjecture through certain mathematical strategies or methods else.\nThen, a new input \\(x_i\\) has its prediction:\n\\[ \\hat{y}_i=\\hat{w_1}x_i+\\hat{w_0}\\tag{4} \\]\nStatistical learning mainly studies \\(\\begin{bmatrix}\\hat{w_0}\\\\\\hat{w_1}\\end{bmatrix}\\) but machine learning concerns more about \\(\\hat{y}\\) All of them were based on the observed data.\nOnce we got this model, what we do next is estimating the parameters\nEstimating the Parameters For the advertisement task, what we have are a linear regression model equation(2) and a set of observations:\n\\[ \\{(x_1,y_1),(x_2,y_2),(x_3,y_3),\\dots,(x_n,y_n)\\}\\tag{5} \\]\nwhich is also known as training set. By the way, \\(x_i\\) in equation (5) is a sample of \\(X\\) and so is \\(y_i\\) of \\(Y\\). \\(n\\) is the size of the training set, the number of observations pairs.\nThe method we employed here is based on a measure of the “closeness” of the outputs of the model to the observed target (\\(y\\)s in set (5)). By far, the most used method is the “least squares criterion”.\nThe outputs \\(\\hat{y}_i\\) of current model(parameters) to every input \\(x_i\\) are:\n\\[ \\{(x_1,\\hat{y}_1),(x_2,\\hat{y}_2),(x_3,\\hat{y}_3),\\dots,(x_n,\\hat{y}_n)\\}\\tag{6} \\]\nand the difference between \\(\\hat{y}_i\\) and \\(y_i\\) is called residual and written as \\(e_i\\):\n\\[ e_i=y_i-\\hat{y}_i\\tag{7} \\]\n\\(y_i\\) is the target, which is the value our model is trying to achieve. So, the smaller the \\(|e_i|\\) is, the better the model is. Because the absolute operation is not a good analytic operation, we replace it with the quadratic operation:\n\\[ \\mathcal{L}_\\text{RSS}=e_1^2+e_2^2+\\dots+e_n^2\\tag{8} \\]\n\\(\\mathcal{L}_\\text{RSS}\\) means “Residual Sum of Squares”, the sum of total square residual. And to find a better model, we need to minimize the sum of the total residual. In machine learning, this is called loss function.\nNow we take equations (4),(7) into (8):\n\\[ \\begin{aligned} \\mathcal{L}_\\text{RSS}=\u0026amp;(y_1-\\hat{w_1}x_1-\\hat{w_0})^2+(y_2-\\hat{w_1}x_2-\\hat{w_0})^2+\\\\ \u0026amp;\\dots+(y_n-\\hat{w_1}x_n-\\hat{w_0})^2\\\\ =\u0026amp;\\sum_{i=1}^n(y_i-\\hat{w_1}x_i-\\hat{w_0})^2 \\end{aligned}\\tag{9} \\]\nTo minimize the function “\\(\\mathcal{L}_\\text{RSS}\\)”, the calculus told us the possible minimum(maximum) points always stay at stationary points. And the stationary points are the points where the derivative of the function is zero. Remember that the minimum(maximum) points must be stationary points, but the stationary point is not necessary to be a minimum(maximum) point. For more information, ‘Numerical Optimization’ is a good book.\nSince the ‘\\(\\mathcal{L}_\\text{RSS}\\)’ is a function of a vector \\(\\begin{bmatrix}w_0\u0026amp;w_1\\end{bmatrix}^T\\), the derivative is replaced by partial derivative. As the ‘\\(\\mathcal{L}_\\text{RSS}\\)’ is just a simple quadric surface, the minimum or maximum exists, and there is one and only one stationary point.\nThen our mission to find the best parameters for the regression has been converted to calculus the solution of the function system that the derivative(partial derivative) is set to zero.\nThe partial derivative of \\(\\hat{w_1}\\) is\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}=\u0026amp;-2\\sum_{i=1}^nx_i(y_i-\\hat{w_1}x_i-\\hat{w_0})\\\\ =\u0026amp;-2(\\sum_{i=1}^nx_iy_i-\\hat{w_1}\\sum_{i=1}^nx_i^2-\\hat{w_0}\\sum_{i=1}^nx_i) \\end{aligned}\\tag{10} \\]\nand derivative of \\(\\hat{w_0}\\) is:\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_0}}}=\u0026amp;-2\\sum_{i=1}^n(y_i-\\hat{w_1}x_i-\\hat{w_0})\\\\ =\u0026amp;-2(\\sum_{i=1}^ny_i-\\hat{w_1}\\sum_{i=1}^nx_i-\\sum_{i=1}^n\\hat{w_0}) \\end{aligned}\\tag{11} \\]\nSet both of them to zero and we can get:\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_0}}}\u0026amp;=0\\\\ \\hat{w_0} \u0026amp;=\\frac{\\sum_{i=1}^ny_i-\\hat{w_1}\\sum_{i=1}^nx_i}{n}\\\\ \u0026amp;=\\bar{y}-\\hat{w_1}\\bar{x} \\end{aligned}\\tag{12} \\]\nand\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}\u0026amp;=0\\\\ \\hat{w_1}\u0026amp;=\\frac{\\sum_{i=1}^nx_iy_i-\\hat{w_0}\\sum_{i=1}^nx_i}{\\sum_{i=1}^nx_i^2} \\end{aligned}\\tag{13} \\]\nTo get a equation of \\(\\hat{w_1}\\) independently, we take equation(13) to equation(12):\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}\u0026amp;=0\\\\ \\hat{w_1}\u0026amp;=\\frac{\\sum_{i=1}^nx_i(y_i-\\bar{y})}{\\sum_{i=1}^nx_i(x_i-\\bar{x})} \\end{aligned}\\tag{14} \\]\nwhere \\(\\bar{x}=\\frac{\\sum_{i=1}^nx_i}{n}\\) and \\(\\bar{y}=\\frac{\\sum_{i=1}^ny_i}{n}\\)\nBy the way, equation (14) has another form:\n\\[ \\hat{w_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})(x_i-\\bar{x})}\\tag{15} \\]\nand they are equal.\nDiagrams and Code Using python to demonstrate our result Equ. (12)(14) is correct:\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt  # load data from csv file by pandas AdvertisingFilepath=\u0026#39;./data/Advertising.csv\u0026#39; data=pd.read_csv(AdvertisingFilepath)  # convert original data to numpy array data_TV=np.array(data[\u0026#39;TV\u0026#39;]) data_sale=np.array(data[\u0026#39;sales\u0026#39;])  # calculate mean of x and y y_sum=0 y_mean=0 x_sum=0 x_mean=0 for x,y in zip(data_TV,data_sale):  y_sum+=y  x_sum+=x if len(data_sale)!=0:  y_mean=y_sum/len(data_sale) if len(data_TV)!=0:  x_mean=x_sum/len(data_TV)  # calculate w_1 w_1=0 a=0 b=0 for x,y in zip(data_TV,data_sale):  a += x*(y-y_mean)  b += x*(x-x_mean) if b!=0:  w_1=a/b  # calculate w_0 w_0=y_mean-w_1*x_mean  # draw a picture plt.xlabel(\u0026#39;TV\u0026#39;) plt.ylabel(\u0026#39;Sales\u0026#39;) plt.title(\u0026#39;TV and Sales\u0026#39;) plt.scatter(data_TV,data_sale,s=8,c=\u0026#39;g\u0026#39;, alpha=0.5) x=np.arange(-10,350,0.1) plt.plot(x,w_1*x+w_0,\u0026#39;r-\u0026#39;) plt.show() After running the code, we got:\nReference  James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.↩︎\n   ","permalink":"https://anthony-tan.com/A-Simple-Linear-Regression/","summary":"Preliminaries Linear Algebra(the concepts of space, vector) Calculus  Notations of Linear Regression1 We have already created a simple linear model in the post “An Introduction to Linear Regression”. According to the definition of linearity, we can develop the simplest linear regression model:\n\\[ Y\\sim w_1X+w_0\\tag{1} \\]\nwhere the symbol \\(\\sim\\) is read as “is approximately modeled as”. Equation (1) can also be described as “regressing \\(Y\\) on \\(X\\)(or \\(Y\\) onto \\(X\\))”.","title":"A Simple Linear Regression"},{"content":"Preliminariess Linear Algebra(the concepts of space, vector) Calculus  What is Linear Regression Linear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors).\nMachine Learning and Statistical Learning Machine learning and statistical learning are similar but have some distinctions. In machine learning, models, regression models, or classification models, are used to predict the outputs of the new incoming inputs.\nIn contrast, in statistical learning, regression and classification are employed to model the data to find out the hidden relations among the inputs. In other words, the models of data, no matter they are regression or classification, or something else. They are used to analyze the mechanism behind the data.\n“Linear” Linear is a property of the operation \\(f(\\cdot)\\), which has the following two properties:\n\\(f(a\\mathbf{x})=af(\\mathbf{x})\\) \\(f(\\mathbf{x}+\\mathbf{y})=f(\\mathbf{x})+f(\\mathbf{y})\\)  where \\(a\\) is a scalar. Then we say \\(f(\\cdot)\\) is linear or \\(f(\\cdot)\\) has a linearity property.\nThe linear operation can be represented as a matrix. And when a 2-dimensional linear operation was drawn on the paper, it is a line. Maybe that is why it is named linear, I guess.\n“Regression” In statistical or machine learning, regression is a crucial part of the whole field. And, the other part is the well-known classification. If we have a close look at the outputs data type, one distinction between them is that the output of regression is continuous but the output of classification is discrete.\nWhat is linear regression Linear regression is a regression model. All parameters in the model are linear, like:\n\\[ f(\\mathbf{x})=w_1x_1+w_2x_2+w_3x_3\\tag{1} \\]\nwhere the \\(w_n\\) where \\(n=1,2,3\\) are the parameters of the model, the output \\(f(\\mathbf{x})\\) can be written as \\(t\\) for 1-deminsional outputs (or \\(\\mathbf{t}\\) for multi-deminsional outputs).\n\\(f(\\mathbf{w})\\) is linear:\n\\[ \\begin{aligned} f(a\\cdot\\mathbf{w})\u0026amp;=aw_1x_1+aw_2x_2+aw_3x_3=a\\cdot f(\\mathbf{w}) \\\\ f(\\mathbf{w}+\\mathbf{v})\u0026amp;=(w_1+v_1)x_1+(w_2+v_2)x_2+(w_3+v_3)x_3\\\\ \u0026amp;=w_1x_1+v_1x_1+w_2x_2+v_2x_2+w_3x_3+v_3x_3\\\\ \u0026amp;=f(\\mathbf{w})+f(\\mathbf{v}) \\end{aligned}\\tag{2} \\]\nwhere \\(a\\) is a scalar, and \\(\\mathbf{v}\\) is in the same space with \\(\\mathbf{w}\\)\nQ.E.D\nThere is also another view that the linear property of models is also for \\(\\mathbf{x}\\), the input.\nBut the following model\n\\[ t=f(\\mathbf{x})=w_1\\log(x_1)+w_2\\sin(x_2)\\tag{3} \\]\nis a case of linear regression problem in our definition. But from the second point of view, it is not linear for inputs \\(\\mathbf{x}\\). However, this is not an unsolvable contradiction. If we use:\n\\[ y_1= \\log(x_1)\\\\ y_2= \\sin(x_2)\\tag{4} \\]\nto replace the \\(\\log\\) and \\(\\sin\\) in equation (3), we get again\n\\[ t=f(\\mathbf{y})=w_1y_1+w_2y_2\\tag{5} \\]\na linear operation for both input \\(\\mathbf{y}=\\begin{bmatrix}y_1\\;y_2\\end{bmatrix}^T\\) and parameters \\(\\mathbf{z}\\) .\nThe tranformation, equation(4), is called feature extraction. \\(\\mathbf{y}\\) is called features, and \\(\\log\\) and \\(\\sin\\) are called basis functions\nAn Example This example is taken from (James20131), It is about the sale between different kinds of advertisements. I downloaded the data set from http://faculty.marshall.usc.edu/gareth-james/ISL/data.html. It’s a CSV file, including 200 rows. Here I draw 3 pictures using ‘matplotlib’ to make the data more visible. They are advertisements for ‘TV’, ‘Radio’, ‘Newspaper’ to ‘Sales’ respectively.\nFrom these figures, we can find TV ads and Sales looks like having a stronger relationship than radio ads and sales. However, the Newspaper ads and Sales look independent.\nFor statistical learning, we should take statistical methods to investigate the relation in the data. And in machine learning, to a certain input, predicting an output is what we are concerned.\nWhy Linear Regression Linear regression has been used for more than 200 years, and it’s always been our first class of statistical learning or machine learning. Here we list 3 practical elements of linear regression, which are essential for the whole subject:\nIt is still working in some areas. Although more complicated models have been built, they could not be replaced totally. It is a good jump-off point to the other more feasible and adorable models, which may be an extension or generation of naive linear regression Linear regression is easy, so it is possible to be analyzed mathematically.  This is why linear regression is always our first step to learn machine learning and statistical learning. And by now, this works pretty well.\nA Probabilistic View Machine learning or statistical learning can be described from two different views - Bayesian and Frequentist. They both worked well for some different instances, but they also have their limitations. The Bayesian view of the linear regression will be talked about as well later.\nBayesian statisticians thought the input \\(\\mathbf{x}\\), the output \\(t\\), and the parameter \\(\\mathbf{w}\\) are all random variables, while the frequentist does not think so. Bayesian statisticians predict the unknown input \\(\\mathbf{x}_0\\) by forming the distribution \\(\\mathbb{P}(t_0|\\mathbf{x}_0)\\) and then sampling from it. To achieve this goal, we must build the \\(\\mathbb{P}(t|\\mathbf{x})\\) firstly. This is the modeling progress, or we can call it learning progress.\nReferences  James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Linear-Regression/","summary":"Preliminariess Linear Algebra(the concepts of space, vector) Calculus  What is Linear Regression Linear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors).\nMachine Learning and Statistical Learning Machine learning and statistical learning are similar but have some distinctions. In machine learning, models, regression models, or classification models, are used to predict the outputs of the new incoming inputs.","title":"An Introduction to Linear Regression"},{"content":"About Me Hi, I\u0026rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years. So far, I\u0026rsquo;ve learned and reviewed some papers that are necessary for reinforcement learning research, and some mathematics, like calculus, linear algebra, probability, and so on.\nHowever the bigger one in the picture is me, Tony, and the smaller one is my dog, Potato(He was too young to take a shower when we take the picture, and now he is no more a dirty puppy😀)\nWhy the blogs These blogs here are used to simply explain what I have learned, according to the Feyman Technique. And blogging what I\u0026rsquo;ve just learned is the most important part of learning. The whole process is:\n Choosing a concept or theory I would like to know(collecting necessary materials )  Outlining what prior knowledge of this concept or theory Taking note of this prior knowledge   Try to explain the new theory to readers of the website without any new words and concepts in the theory (draft) Go back to the source and fill in the gap in understanding Simplify the explaining(rewrite and post)  What in blogs These blogs contain:\n Mathematics Neuroscience Algorithms  Deep Learning Reinforcement Learning    And some of these posts might also be represented by videos on my YouTube channel.\n","permalink":"https://anthony-tan.com/about/","summary":"About Me Hi, I\u0026rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years.","title":""}]