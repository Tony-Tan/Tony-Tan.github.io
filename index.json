[{"content":"Preliminaries Committee  Boosting The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:\nperformances better than any of the base classifiers and each base classifier has a different weight factor  Adaboost Adaboost is short for adaptive boosting. It is a method combining several weak classifiers which are just better than random guesses and it gives a better performance than the committee. The base classifiers in AdaBoost are trained sequentially, and their training set is the same but with different weights for each sample. So when we consider the distribution of training data, every weak classifier was trained on different sample distribution. This might be an important reason for the improvement of AdaBoost from the committee. And the weights for weak classifiers are generated depending on the performance of the previous classifier.\nDuring the prediction process, the input data flows from classifier to classifier and the final result is some kind of combination of all output of weak classifiers.\nImportant ideas in the AdaBoost algorithm are:\nthe data points are predicted incorrectly in the current classifier giving a greater weight once the algorithm was trained, the prediction of each classifier is combined through a weighted majority voting scheme as:  where \\(w_n^{(1)}\\) is the initial weights of input data of the \\(1\\) st weak classifier, \\(y_1(x)\\) is the prediction of the \\(1\\) st weak classifier, \\(\\alpha_m\\) is the weight of each prediction(notably this weight works to \\(y_m(x)\\) and \\(w_n^{(1)}\\) is the weight of input data of the first classifier.). And the final output is the sign function of the weighted sum of all predictions.\nThe procedure of the algorithm is:\n Initial data weighting coefficients \\(\\{\\boldsymbol{w}_n\\}\\) by \\(w_n^{(1)}=\\frac{1}{N}\\) for \\(n=1,2,\\cdots,N\\) For \\(m=1,\\dots,M\\):   Fit a classifier \\(y_m(\\boldsymbol{x})\\) to training set by minimizing the weighted error function: \\[J_m=\\sum_{}^{}w_n^{(m)}I(y_m(\\boldsymbol{x}_n)\\neq t_n)\\] where \\(I(y_m(\\boldsymbol{x})\\neq t_n)\\) is the indicator function and equals 1 when \\(y_m(\\boldsymbol{x})\\neq t_n\\) and 0 otherwise Evaluate the quatities: \\[\\epsilon_m=\\frac{\\sum_{n=1}^Nw_n^{(m)}I(y_m(\\boldsymbol{x})\\neq t_n)}{\\sum_{n=1}^{N}w_n^{(m)}}\\] and then use this to evaluate \\(\\alpha_m=\\ln \\{\\frac{1-\\epsilon_m}{\\epsilon_m}\\}\\) Updata the data weighting coefficients: \\[w_n^{(m+1)}=w_n^{(m)}\\exp\\{\\alpha_mI(y_m(\\boldsymbol{x})\\neq t_n)\\}\\]  Make predictions using the final model, which is given by: \\[Y_M = \\mathrm{sign} (\\sum_{m=1}^{M}\\alpha_my_m(x))\\]   This procedure comes from ‘Pattern recognition and machine learning’1\nPython Code of Adaboost # weak classifier # test each dimension and each value and each direction to find a # best threshold and direction(\u0026#39;\u0026lt;\u0026#39; or \u0026#39;\u0026gt;\u0026#39;) class Stump():  def __init__(self):  self.feature = 0  self.threshold = 0  self.direction = \u0026#39;\u0026lt;\u0026#39;   def loss(self,y_hat, y, weights):  \u0026quot;\u0026quot;\u0026quot; :param y_hat: prediction :param y: target :param weights: weight of each data :return: loss \u0026quot;\u0026quot;\u0026quot;  sum = 0  example_size = y.shape[0]  for i in range(example_size):  if y_hat[i] != y[i]:  sum += weights[i]  return sum   def test_in_traing(self, x, feature, threshold, direction=\u0026#39;\u0026lt;\u0026#39;):  \u0026quot;\u0026quot;\u0026quot; test during training :param x: input data :param feature: classification on which dimension :param threshold: threshold :param direction: \u0026#39;\u0026lt;\u0026#39; or \u0026#39;\u0026gt;\u0026#39; to threshold :return: classification result \u0026quot;\u0026quot;\u0026quot;  example_size = x.shape[0]  classification_result = -np.ones(example_size)  for i in range(example_size):  if direction == \u0026#39;\u0026lt;\u0026#39;:  if x[i][feature] \u0026lt; threshold:  classification_result[i] = 1  else:  if x[i][feature] \u0026gt; threshold:  classification_result[i] = 1  return classification_result   def test(self,x):  \u0026quot;\u0026quot;\u0026quot; test during prediction :param x: input :return: classification result \u0026quot;\u0026quot;\u0026quot;  return self.test_in_traing(x, self.feature, self.threshold, self.direction)   def training(self, x, y, weights):  \u0026quot;\u0026quot;\u0026quot; main training process :param x: input :param y: target :param weights: weights :return: none \u0026quot;\u0026quot;\u0026quot;  example_size = x.shape[0]  example_dimension = x.shape[1]  loss_matrix_less = np.zeros(np.shape(x))  loss_matrix_more = np.zeros(np.shape(x))  for i in range(example_dimension):  for j in range(example_size):  results_ji_less = self.test_in_traing(x, i, x[j][i], \u0026#39;\u0026lt;\u0026#39;)  results_ji_more = self.test_in_traing(x, i, x[j][i], \u0026#39;\u0026gt;\u0026#39;)  loss_matrix_less[j][i] = self.loss(results_ji_less, y, weights)  loss_matrix_more[j][i] = self.loss(results_ji_more, y, weights)  loss_matrix_less_min = np.min(loss_matrix_less)  loss_matrix_more_min = np.min(loss_matrix_more)  if loss_matrix_less_min \u0026gt; loss_matrix_more_min:  minimum_position = np.where(loss_matrix_more == loss_matrix_more_min)  self.threshold = x[minimum_position[0][0]][minimum_position[1][0]]  self.feature = minimum_position[1][0]  self.direction = \u0026#39;\u0026gt;\u0026#39;  else:  minimum_position = np.where(loss_matrix_less == loss_matrix_less_min)  self.threshold = x[minimum_position[0][0]][minimum_position[1][0]]  self.feature = minimum_position[1][0]  self.direction = \u0026#39;\u0026lt;\u0026#39;   class Adaboost():  def __init__(self, maximum_classifier_size):  self.max_classifier_size = maximum_classifier_size  self.classifiers = []  self.alpha = np.ones(self.max_classifier_size)   def training(self, x, y, classifier_class):  \u0026quot;\u0026quot;\u0026quot; training adaboost main steps :param x: input :param y: target :param classifier_class: what can classifier would be used, here we use stump above :return: none \u0026quot;\u0026quot;\u0026quot;  example_size = x.shape[0]  weights = np.ones(example_size)/example_size   for i in range(self.max_classifier_size):  classifier = classifier_class()  classifier.training(x, y, weights)  test_res = classifier.test(x)  indicator = np.zeros(len(weights))  for j in range(len(indicator)):  if test_res[j] != y[j]:  indicator[j] = 1   cost_function = np.sum(weights*indicator)  epsilon = cost_function/np.sum(weights)  self.alpha[i] = np.log((1-epsilon)/epsilon)  self.classifiers.append(classifier)  weights = weights * np.exp(self.alpha[i]*indicator)   def predictor(self, x):  \u0026quot;\u0026quot;\u0026quot; prediction :param x: input data :return: prediction result \u0026quot;\u0026quot;\u0026quot;  example_size = x.shape[0]  results = np.zeros(example_size)  for i in range(example_size):  y = np.zeros(self.max_classifier_size)  for j in range(self.max_classifier_size):  y[j] = self.classifiers[j].test(x[i].reshape(1,-1))  results[i] = np.sign(np.sum(self.alpha*y))  return results the entire project can be found https://github.com/Tony-Tan/ML. And please star me! Thanks!\nWhen we use different numbers of classifiers, the results of the algorithm are like this:\nwhere the blue circles are the correct classification of class 1 and red circles are the correct classification of class 2. And the blue crosses belong to class 2 but were classified into class 1, and so do the red crosses.\nA 40-classifiers AdaBoost gives a relatively good prediction:\nwhere there is only one misclassified point.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Boosting-and-AdaBoost/","summary":"Preliminaries Committee  Boosting The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:\nperformances better than any of the base classifiers and each base classifier has a different weight factor  Adaboost Adaboost is short for adaptive boosting.","title":"Boosting and AdaBoost"},{"content":"Preliminaries Basic machine learning concepts Probability Theory concepts   expectation correlated random variable  Analysis of Committees1 The committee is a native inspiration for how to combine several models(or we can say how to combine the outputs of several models). For example, we can combine all the models by:\n\\[ y_{COM}(X)=\\frac{1}{M}\\sum_{m=1}^My_m(X)\\tag{1} \\]\nThen we want to find out whether this average prediction of models is better than every one of them.\nTo compare the committee and a single model, we need first to build a criterion depending on which we can distinguish which result is better. Assuming that the true generator of the training data \\(x\\) is:\n\\[ h(x)\\tag{2} \\]\nSo our prediction of the \\(m\\)th model for \\(m=1,2\\cdots,M\\) can be represented as:\n\\[ y_m(x) = h(x) +\\epsilon_m(x)\\tag{3} \\] where \\(\\epsilon_m(x)\\) is the error of \\(m\\) th model.Then the average sum-of-squares of error can be a nice criterion.\nThe criterion of a single model is:\n\\[ \\mathbb{E}_x[(y_m(x)-h(x))^2] = \\mathbb{E}_x[\\epsilon_m(x)^2] \\tag{4} \\]\nwhere the \\(\\mathbb{E}[\\cdot]\\) is the frequentist expectation(or average for usual saying).\nNow we consider the average of the error over \\(M\\) models:\n\\[ E_{AV} = \\frac{1}{M}\\sum_{m=1}^M\\mathbb{E}_x[\\epsilon_m(x)^2]\\tag{5} \\]\nAnd on the other hand, the committees have the error given by equations (1), (3), and (4):\n\\[ \\begin{aligned} E_{COM}\u0026amp;=\\mathbb{E}_x[(\\frac{1}{M}\\sum_{m=1}^My_m(x)-h(x))^2] \\\\ \u0026amp;=\\mathbb{E}_x[\\{\\frac{1}{M}\\sum_{m=1}^M\\epsilon_m(x)\\}^2] \\end{aligned} \\tag{6} \\]\nNow we assume that the random variables \\(\\epsilon_i(x)\\) for \\(i=1,2,\\cdots,M\\) have mean 0 and uncorrelated, so that:\n\\[ \\begin{aligned} \\mathbb{E}_x[\\epsilon_m(x)]\u0026amp;=0 \u0026amp;\\\\ \\mathbb{E}_x[\\epsilon_m(x)\\epsilon_l(x)]\u0026amp;=0,\u0026amp;m\\neq l \\end{aligned} \\tag{7} \\]\nThen substitute equation (7) into equation (6), we can get:\n\\[ E_{COM}=\\frac{1}{M^2}\\mathbb{E}_x[\\epsilon_m(x)]\\tag{8} \\]\nAccording to the equation (5) and (8):\n\\[ E_{AV}=\\frac{1}{M}E_{COM}\\tag{9} \\]\nAll the mathematics above is based on the assumption that the error of each model is uncorrelated. However, most time they are highly correlated and the reduction of error is generally small. But the relation:\n\\[ E_{COM}\\leq E_{AV}\\tag{10} \\]\nexists definitely which means committees can produce better predictions than a single model.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Committees/","summary":"Preliminaries Basic machine learning concepts Probability Theory concepts   expectation correlated random variable  Analysis of Committees1 The committee is a native inspiration for how to combine several models(or we can say how to combine the outputs of several models). For example, we can combine all the models by:\n\\[ y_{COM}(X)=\\frac{1}{M}\\sum_{m=1}^My_m(X)\\tag{1} \\]\nThen we want to find out whether this average prediction of models is better than every one of them.","title":"Committees"},{"content":"Preliminaries Bayesian Theorem  Bayesian Model Averaging(BMA)1 Bayesian model averaging(BMA) is another wildly used method that is very like a combining model. However, the difference between BMA and combining models is also significant.\nA Bayesian model averaging is a Bayesian formula in which the random variable are models(hypothesizes) \\(h=1,2,\\cdots,H\\) with prior probability \\(\\Pr(h)\\), then the marginal distribution over data \\(X\\) is:\n\\[ \\Pr(X)=\\sum_{h=1}^{H}\\Pr(X|h)\\Pr(h) \\]\nAnd the MBA is used to select a model(hypothesis) that can model the data best through Bayesian theory. When we have a larger size of \\(X\\), the posterior probability\n\\[ \\Pr(h|X)=\\frac{\\Pr(X|h)\\Pr(h)}{\\sum_{i=1}^{H}\\Pr(X|i)\\Pr(i)} \\]\nbecome sharper. Then we got a good hypothesis.\nMixture of Gaussian(Combining Models) In post ‘Mixtures of Gaussians’, we have seen how a mixture of Gaussians works. Then the joint distribution of input data \\(\\mathbf{x}\\) and latent variable \\(\\mathbf{z}\\) is:\n\\[ \\Pr(\\mathbf{x},\\mathbf{z}) \\]\nand the margin distribution of \\(\\mathbf{x}\\) is\n\\[ \\Pr(\\mathbf{x})=\\sum_{\\mathbf{z}}\\Pr(\\mathbf{x},\\mathbf{z}) \\]\nFor the mixture of Gaussians: \\[ \\Pr(\\mathbf{x})=\\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k) \\] the latent variable \\(\\mathbf{z}\\) is designed: \\[ \\Pr(z_k) = \\pi_k \\] for \\(k=\\{1,2,\\cdots,K\\}\\). And \\(z_k\\in\\{0,1\\}\\) is a \\(1\\)-of-\\(K\\) representation.\nThis mixture of Gaussians is a kind of combining models. Each time, only one \\(k\\) is selected(for \\(\\mathbf{z}\\) is \\(1\\)-of-\\(K\\) representation). An example of a mixture of Gaussians, and its original curve is like:\nAnd the latent variables \\(\\mathbf{z}\\) separate the whole distribution into several Gaussian distributions:\nThis is the simplest model of combining models where each expert is a Gaussian model. And during the voting, only one model was selected by \\(\\mathbf{z}\\) to make the final decision.\nDistinction between BMA and Combining Methods A combining model method contains several models and predicts by voting or other rules. However, Bayesian model averaging can be used to generate a hypothesis from several candidates.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Bayesian-Model-Averaging-and-Combining-Models/","summary":"Preliminaries Bayesian Theorem  Bayesian Model Averaging(BMA)1 Bayesian model averaging(BMA) is another wildly used method that is very like a combining model. However, the difference between BMA and combining models is also significant.\nA Bayesian model averaging is a Bayesian formula in which the random variable are models(hypothesizes) \\(h=1,2,\\cdots,H\\) with prior probability \\(\\Pr(h)\\), then the marginal distribution over data \\(X\\) is:\n\\[ \\Pr(X)=\\sum_{h=1}^{H}\\Pr(X|h)\\Pr(h) \\]\nAnd the MBA is used to select a model(hypothesis) that can model the data best through Bayesian theory.","title":"Bayesian Model Averaging(BMA) and Combining Models"},{"content":"Preliminaries ‘Mixtures of Gaussians’ Basic machine learning concepts  Combining Models1 The mixture of Gaussians had been discussed in the post ‘Mixtures of Gaussians’. It was used to introduce the ‘EM algorithm’ but it gave us the inspiration of improving model performance.\nAll models we have studied, besides neural networks, are all single-distribution models. That is just like that, to solve a problem we invite an expert who is very good at this kind of problem, then we just do whatever the expert said. However, if our problem is too hard that no expert can solve it completely by himself, inviting more experts is a good choice. This inspiration gives a new way to improve performance by combining multiple models but not just by improving the performance of a single model.\nOrganising Models A naive idea is voting by several models equally, which means averaging the prediction of all models. However, different models may have different abilities, and voting equally is not a good idea. Then boosting and other methods were introduced.\nIn some combining methods, such as AdaBoost(boosting), bootstrap, bagging, and e.t.c, the input data has an identical distribution with the training set. However, in some methods, the training set is cut into several subsets with different distributions from the original training set. The decision tree is such a method. A decision tree is a sequence of binary selection and it worked well in both regression and classification tasks.\nWe will briefly discuss: - committees - boosting - decision tree\nin the following posts.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Combining-Models/","summary":"Preliminaries ‘Mixtures of Gaussians’ Basic machine learning concepts  Combining Models1 The mixture of Gaussians had been discussed in the post ‘Mixtures of Gaussians’. It was used to introduce the ‘EM algorithm’ but it gave us the inspiration of improving model performance.\nAll models we have studied, besides neural networks, are all single-distribution models. That is just like that, to solve a problem we invite an expert who is very good at this kind of problem, then we just do whatever the expert said.","title":"An Introduction to Combining Models"},{"content":"Preliminaries Gaussian distribution log-likelihood Calculus  partial derivative Lagrange multiplier   EM Algorithm for Gaussian Mixture1 Analysis Maximizing likelihood could not be used in the Gaussian mixture model directly, because of its severe defects which we have come across at ‘Maximum Likelihood of Gaussian Mixtures’. With the inspiration of K-means, a two-step algorithm was developed.\nThe objective function is the log-likelihood function:\n\\[ \\begin{aligned} \\ln \\Pr(\\mathbf{x}|\\mathbf{\\pi},\\mathbf{\\mu},\\Sigma)\u0026amp;=\\ln (\\Pi_{n=1}^N\\sum_{j=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k))\\\\ \u0026amp;=\\sum_{n=1}^{N}\\ln \\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)\\\\ \\end{aligned}\\tag{1} \\]\n\\(\\mu_k\\) The condition that must be satisfied at a maximum of log-likelihood is the derivative(partial derivative) of parameters are \\(0\\). So we should calculate the partial derivatives of \\(\\mu_k\\):\n\\[ \\begin{aligned} \\frac{\\partial \\ln \\Pr(X|\\pi,\\mu,\\Sigma)}{\\partial \\mu_k}\u0026amp;=\\sum_{n=1}^N\\frac{-\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)\\Sigma_k^{-1}(\\mathbf{x}_n-\\mathbf{\\mu}_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}\\\\ \u0026amp;=-\\sum_{n=1}^N\\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}\\Sigma_k^{-1}(\\mathbf{x}_n-\\mathbf{\\mu}_k) \\end{aligned}\\tag{2} \\]\nand then set equation (2) equal to 0 and rearrange it as:\n\\[ \\begin{aligned} \\sum_{n=1}^N\\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}\\mathbf{x}_n\u0026amp;=\\sum_{n=1}^N\\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}\\mathbf{\\mu}_k \\end{aligned}\\tag{3} \\]\nIn the post ‘Mixtures of Gaussians’, we had defined:\n\\[ \\gamma_{nk}=\\Pr(k=1|\\mathbf{x}_n)=\\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}\\tag{4} \\] as responsibility. And substitute equation(4) into equation(3):\n\\[ \\begin{aligned} \\sum_{n=1}^N\\gamma_{nk}\\mathbf{x}_n\u0026amp;=\\sum_{n=1}^N\\gamma_{nk}\\mathbf{\\mu}_k\\\\ \\sum_{n=1}^N\\gamma_{nk}\\mathbf{x}_n\u0026amp;=\\mathbf{\\mu}_k\\sum_{n=1}^N\\gamma_{nk}\\\\ {\\mu}_k\u0026amp;=\\frac{\\sum_{n=1}^N\\gamma_{nk}\\mathbf{x}_n}{\\sum_{n=1}^N\\gamma_{nk}} \\end{aligned}\\tag{5} \\]\nand to simplify equation (5) we define:\n\\[ N_k = \\sum_{n=1}^N\\gamma_{nk}\\tag{6} \\]\nThen the equation (5) can be simplified as:\n\\[ {\\mu}_k=\\frac{1}{N_k}\\sum_{n=1}^N\\gamma_{nk}\\mathbf{x}_n\\tag{7} \\]\n\\(\\Sigma_k\\) The same calcualtion would be done to \\(\\frac{\\partial \\ln \\Pr(X|\\pi,\\mu,\\Sigma)}{\\partial \\Sigma_k}=0\\) :\n\\[ \\Sigma_k = \\frac{1}{N_k}\\sum_{n=1}^N\\gamma_{nk}(\\mathbf{x}_n - \\mathbf{\\mu_k})(\\mathbf{x}_n - \\mathbf{\\mu_k})^T\\tag{8} \\]\n\\(\\pi_k\\) However, the situation of \\(\\pi_k\\) is a little complex, for it has a constrain:\n\\[ \\sum_k^K \\pi_k = 1 \\tag{9} \\]\nthen Lagrange multiplier is employed and the objective function is:\n\\[ \\ln \\Pr(X|\\mathbf{\\pi},\\mathbf{\\mu},\\Sigma)+\\lambda (\\sum_k^K \\pi_k-1)\\tag{10} \\]\nand set the partial derivative of equation (10) to \\(\\pi_k\\) to 0:\n\\[ 0 = \\sum_{n=1}^N\\frac{\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}+\\lambda\\tag{11} \\]\nAnd multiply both sides by \\(\\pi_k\\) and sum over \\(k\\):\n\\[ \\begin{aligned} 0 \u0026amp;= \\sum_{k=1}^K(\\sum_{n=1}^N\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}+\\lambda\\pi_k)\\\\ 0\u0026amp;=\\sum_{k=1}^K\\sum_{n=1}^N\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}+\\sum_{k=1}^K\\lambda\\pi_k\\\\ 0\u0026amp;=\\sum_{n=1}^N\\sum_{k=1}^K\\gamma_{nk}+\\lambda\\sum_{k=1}^K\\pi_k\\\\ \\lambda \u0026amp;= -N \\end{aligned}\\tag{12} \\]\nthe last step of equation(12) is because \\(\\sum_{k=1}^K\\pi_k=1\\) and \\(\\sum_{k=1}^K\\gamma_{nk}=1\\)\nThen we substitute equation(12) into eqa(11): \\[ \\begin{aligned} 0 \u0026amp;= \\sum_{n=1}^N\\frac{\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}-N\\\\ N \u0026amp;= \\frac{1}{\\pi_k}\\sum_{n=1}^N\\gamma_{nk}\\\\ \\pi_k\u0026amp;=\\frac{N_k}{N} \\end{aligned}\\tag{13} \\]\nthe last step of equation (13) is because of the definition of equation (6).\nAlgorithm Equations (5), (8), and (13) could not construct a closed-form solution. The reason is that for example in equation (5), both side of the equation contains parameter \\(\\mu_k\\).\nHowever, the equations suggest an iterative scheme for finding a solution which includes two-step: expectation and maximization:\nE step: calculating the posterior probability of equation (4) with the current parameter M step: update parameters by equations (5), (8), and (13)  The initial value of the parameters could be randomly selected. But some other tricks are always used, such as K-means. And the stop conditions can be one of:\nincrease of log-likelihood falls below some threshold change of parameters less than some threshold.  Python Code for EM The input data should be normalized as what we did in ‘K-means algorithm’\ndef Gaussian( x, u, variance):  k = len(x)  return np.power(2*np.pi, -k/2.)*np.power(np.linalg.det(variance),  -1/2)*np.exp(-0.5*(x-u).dot(np.linalg.inv(variance)).dot((x-u).transpose()))   class EM():  def mixed_Gaussian(self,x,pi,u,covariance):  res = 0  for i in range(len(pi)):  res += pi[i]*Gaussian(x,u[i],covariance[i])  return res   def clusturing(self, x, d, initial_method=\u0026#39;K_Means\u0026#39;):  data_dimension = x.shape[1]  data_size = x.shape[0]  if initial_method == \u0026#39;K_Means\u0026#39;:  km = k_means.K_Means()  # k_means initial mean vector, each row is a mean vector\u0026#39;s transpose  centers, cluster_for_each_point = km.clusturing(x, d)  # initial latent variable pi  pi = np.ones(d)/d  # initial covariance  covariance = np.zeros((d,data_dimension,data_dimension))  for i in range(d):  covariance[i] = np.identity(data_dimension)/10.0  # calculate responsibility  responsibility = np.zeros((data_size,d))  log_likelihood = 0  log_likelihood_last_time = 0  for dummy in range(1,1000):  log_likelihood_last_time = log_likelihood  # E step:  # points in each class  k_class_dict = {i: [] for i in range(d)}  for i in range(data_size):  responsibility_numerator = np.zeros(d)  responsibility_denominator = 0  for j in range(d):  responsibility_numerator[j] = pi[j]*Gaussian(x[i],centers[j],covariance[j])  responsibility_denominator += responsibility_numerator[j]  for j in range(d):  responsibility[i][j] = responsibility_numerator[j]/responsibility_denominator   # M step:  N_k = np.zeros(d)  for j in range(d):  for i in range(data_size):  N_k[j] += responsibility[i][j]  for i in range(d):  # calculate mean  # sum of responsibility multiply x  sum_r_x = 0  for j in range(data_size):  sum_r_x += responsibility[j][i]*x[j]  if N_k[i] != 0:  centers[i] = 1/N_k[i]*sum_r_x  # covariance  # sum of responsibility multiply variance  sum_r_v = np.zeros((data_dimension,data_dimension))  for j in range(data_size):  temp = (x[j]-centers[i]).reshape(1,-1)  temp_T = (x[j]-centers[i]).reshape(-1,1)  sum_r_v += responsibility[j][i]*(temp_T.dot(temp))  if N_k[i] != 0:  covariance[i] = 1 / N_k[i] * sum_r_v  # latent pi  pi[i] = N_k[i]/data_size  log_likelihood = 0  for i in range(data_size):  log_likelihood += np.log(self.mixed_Gaussian(x[i], pi, centers, covariance))   if np.abs(log_likelihood - log_likelihood_last_time)\u0026lt;0.001:  break  print(log_likelihood_last_time)  return pi,centers,covariance The entire project can be found https://github.com/Tony-Tan/ML and please star me.\nThe progress of EM(initial with K-means):\nand the final result is:\nwhere the ellipse represents the covariance matrix and the axes of the ellipse are in the direction of eigenvectors of the covariance matrix, and their length is corresponding eigenvalues.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/EM-Algorithm/","summary":"Preliminaries Gaussian distribution log-likelihood Calculus  partial derivative Lagrange multiplier   EM Algorithm for Gaussian Mixture1 Analysis Maximizing likelihood could not be used in the Gaussian mixture model directly, because of its severe defects which we have come across at ‘Maximum Likelihood of Gaussian Mixtures’. With the inspiration of K-means, a two-step algorithm was developed.\nThe objective function is the log-likelihood function:\n\\[ \\begin{aligned} \\ln \\Pr(\\mathbf{x}|\\mathbf{\\pi},\\mathbf{\\mu},\\Sigma)\u0026amp;=\\ln (\\Pi_{n=1}^N\\sum_{j=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k))\\\\ \u0026amp;=\\sum_{n=1}^{N}\\ln \\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)\\\\ \\end{aligned}\\tag{1} \\]","title":"EM Algorithm"},{"content":"Preliminaries Probability Theory   multiplication principle joint distribution the Bayesian theory Gaussian distribution log-likelihood function  ‘Maximum Likelihood Estimation’  Maximum Likelihood1 Gaussian mixtures had been discussed in ‘Mixtures of Gaussians’. And once we have a training data set and a certain hypothesis, what we should do next is estimate the parameters of the model. Both kinds of parameters from a mixture of Gaussians \\(\\Pr(\\mathbf{x})= \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\\): - the parameters of Gaussian: \\(\\mathbf{\\mu}_k,\\Sigma_k\\) - and latent variables: \\(\\mathbf{z}\\)\nneed to be estimated. When we investigated the linear regression problems, ‘Maximum Likelihood Estimation’ method assumed the output of the linear model also has a Gaussian distribution. So, we could try the maximum likelihood again in the Gaussian mixture task, and find whether it could solve the problem.\nNotations: - input data: \\(\\{\\mathbf{x}_1,\\cdots,\\mathbf{x}_N\\}\\) for \\(\\mathbf{x}_i\\in \\mathbb{R}^D\\) and \\(i=\\{1,2,\\cdots,N\\}\\) and assuming they are i.i.d. Rearranging them in a matrix: \\[ X = \\begin{bmatrix} -\u0026amp;\\mathbf{x}_1^T\u0026amp;-\\\\ -\u0026amp;\\mathbf{x}_2^T\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\mathbf{x}_N^T\u0026amp;-\\\\ \\end{bmatrix}\\tag{1} \\]\n Latent varibales \\(\\mathbf{z}_i\\), for \\(i\\in\\{1,\\cdots,N\\}\\). And similar to matrix (3), the matrix of latent varibales is \\[ Z = \\begin{bmatrix} -\u0026amp;\\mathbf{z}_1^T\u0026amp;-\\\\ -\u0026amp;\\mathbf{z}_2^T\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\mathbf{z}_N^T\u0026amp;-\\\\ \\end{bmatrix}\\tag{2} \\]  Once we got these two matrices, based on the definition of ‘Mixtures of Gaussians’:\n\\[ \\Pr(\\mathbf{x})= \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\\tag{3} \\]\nthe log-likelihood function is given by:\n\\[ \\begin{aligned} \\ln \\Pr(\\mathbf{x}|\\mathbf{\\pi},\\mathbf{\\mu},\\Sigma)\u0026amp;=\\ln (\\Pi_{n=1}^N\\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k))\\\\ \u0026amp;=\\sum_{n=1}^{N}\\ln \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)\\\\ \\end{aligned}\\tag{4} \\]\nThis looks different from the single Gaussian model where the logarithm operates directly on \\(\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\\) who is an exponential function. The existence of summation in the logarithm makes the problem hard to solve.\nBecause the combination order of the Gaussian mixture could be arbitrary, we could have \\(K!\\) equivalent solutions of \\(\\mathbf{z}\\). So which one we get did not affect our model.\nMaximum Likelihood Could Fail Covariance Matrix \\(\\Sigma\\) is not Singular In the Gaussian distribution, the covariance matrix must be able to be inverted. So in our following discussion, we assume all the covariance matrice are invisible. For simplicity we take \\(\\Sigma_k=\\delta_k^2 I\\) where \\(I\\) is identity matrix.\nWhen \\(\\mathbf{x}_n=\\mathbf{\\mu}_j\\) When a point in the sample accidentally equals the mean \\(\\mu_j\\), the Gaussian distribution of the random variable \\(\\mathbf{x}_n\\) is:\n\\[ \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\delta_j^2I)=\\frac{1}{2\\pi^{\\frac{1}{2}}}\\frac{1}{\\delta_j}\\tag{5} \\]\nWhen the variance \\(\\delta_j\\to 0\\), this part goes to infinity and the whole algorithm failed.\nThis problem does not exist in a single Gaussian model, for the \\(\\mathbf{x}_n-\\mathbf{\\mu}_j=0\\) is not an exponent in its log-likelihood.\nSummary The maximum likelihood method is not suited for a Gaussian mixture model. Then we introduce the EM algorithm in the next post.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Maximum-Likelihood-of-Gaussian-Mixtures/","summary":"Preliminaries Probability Theory   multiplication principle joint distribution the Bayesian theory Gaussian distribution log-likelihood function  ‘Maximum Likelihood Estimation’  Maximum Likelihood1 Gaussian mixtures had been discussed in ‘Mixtures of Gaussians’. And once we have a training data set and a certain hypothesis, what we should do next is estimate the parameters of the model. Both kinds of parameters from a mixture of Gaussians \\(\\Pr(\\mathbf{x})= \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\\): - the parameters of Gaussian: \\(\\mathbf{\\mu}_k,\\Sigma_k\\) - and latent variables: \\(\\mathbf{z}\\)","title":"Maximum Likelihood of Gaussian Mixtures"},{"content":"Preliminaries Probability Theory   multiplication principle joint distribution the Bayesian theory Gaussian distribution  Calculus 1,2  A Formal Introduction to Mixtures of Gaussians1 We have introduced a mixture distribution in the post ‘An Introduction to Mixture Models’. And the example in that post was just two components Gaussian Mixture. However, in this post, we would like to talk about Gaussian mixtures formally. And it severs to motivate the development of the expectation-maximization(EM) algorithm.\nGaussian mixture distribution can be written as:\n\\[ \\Pr(\\mathbf{x})= \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\\tag{1} \\]\nwhere \\(\\sum_{k=1}^K \\pi_k =1\\) and \\(0\\leq \\pi_k\\leq 1\\).\nAnd then we introduce a random variable(vector) called latent variable(vector) \\(\\mathbf{z}\\), that each component of \\(\\mathbf{z}\\):\n\\[ z_k\\in\\{0,1\\}\\tag{2} \\]\nand \\(\\mathbf{z}\\) is a \\(1\\)-of-\\(K\\) representation, which means there is one and only one component is \\(1\\) and others are \\(0\\).\nTo build a joint distribution \\(\\Pr(\\mathbf{x},\\mathbf{z})\\), we should build \\(\\Pr(\\mathbf{x}|\\mathbf{z})\\) and \\(\\Pr(\\mathbf{z})\\) firstly. We define the distribution of \\(\\mathbf{z}\\):\n\\[ \\Pr(z_k=1)=\\pi_k\\tag{3} \\]\nfor \\(\\{\\pi_k\\}\\) for \\(k=1,\\cdots,K\\). And equation (3) is now written as:\n\\[ \\Pr(\\mathbf{z}) = \\Pi_{k=1}^K \\pi_k^{z_k}\\tag{4} \\]\nAnd according to the definition of \\(\\Pr(\\mathbf{z})\\) we can get the condition distribution of \\(\\mathbf{x}\\) given \\(\\mathbf{z}\\). Under the condition \\(z_k=1\\), we have:\n\\[ \\Pr(\\mathbf{x}|z_k=1)=\\mathcal{N}(\\mathbf{x}|\\mu_k,\\Sigma_k)\\tag{5} \\]\nand then we can derive the vector form of conditional distribution:\n\\[ \\Pr(\\mathbf{x}|\\mathbf{z})=\\Pi_{k=1}^{K}\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)^{z_k}\\tag{6} \\]\nOnce we have both the probability distribution of \\(\\mathbf{z}\\), \\(\\Pr(\\mathbf{z})\\), and conditional distribution, \\(\\Pr(\\mathbf{x}|\\mathbf{z})\\), we can build joint distribution by multiplication principle:\n\\[ \\Pr(x,z) = \\Pr(\\mathbf{z})\\cdot \\Pr(\\mathbf{x}|\\mathbf{z})\\tag{7} \\]\nHowever, what we are concerning is still the distribution of \\(\\mathbf{x}\\). We can calculate the probability of \\(\\mathbf{x}\\) by:\n\\[ \\Pr(\\mathbf{x}) = \\sum_{j}\\Pr(x,\\mathbf{z}_j) = \\sum_{j}\\Pr(\\mathbf{z}_j)\\cdot \\Pr(\\mathbf{x}|\\mathbf{z_j})\\tag{8} \\]\nwhere \\(\\mathbf{z_j}\\) is every possible value of random variable \\(\\mathbf{z}\\).\nThis is how latent variables construct mixture Gaussians. And this form is easy for us to analyze the distribution of a mixture model.\n‘Responsibility’ of Gaussian Mixtures The Bayesian formula can help us produce posterior. And the posterior probability of latent variable \\(\\mathbf{z}\\) by equation (7) can be calculated:\n\\[ \\Pr(z_k=1|\\mathbf{x})=\\frac{\\Pr(z_k=1)\\Pr(\\mathbf{x}|z_k=1)}{\\sum_j^K \\Pr(z_j=1)\\Pr(\\mathbf{x}|z_j=1)}\\tag{9} \\]\nand substitute equation (3),(5) into equation (9) and we get:\n\\[ \\Pr(z_k=1|\\mathbf{x})=\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k,\\Sigma_k)}{\\sum^K_j \\pi_j\\mathcal{N}(\\mathbf{x}|\\mu_j,\\Sigma_j)}\\tag{10} \\]\nAnd \\(\\Pr(z_k=1|\\mathbf{x})\\) is also called reponsibility, and denoted as:\n\\[ \\gamma(z_k)=\\Pr(z_k=1|\\mathbf{x})\\tag{11} \\]\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Mixtures-of-Gaussians/","summary":"Preliminaries Probability Theory   multiplication principle joint distribution the Bayesian theory Gaussian distribution  Calculus 1,2  A Formal Introduction to Mixtures of Gaussians1 We have introduced a mixture distribution in the post ‘An Introduction to Mixture Models’. And the example in that post was just two components Gaussian Mixture. However, in this post, we would like to talk about Gaussian mixtures formally. And it severs to motivate the development of the expectation-maximization(EM) algorithm.","title":"Mixtures of Gaussians"},{"content":"Preliminaries Numerical Optimization  necessary conditions for maximum  K-means algorithm Fisher Linear Discriminant  Clustering Problem1 The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.\nClustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete. However, during training classifiers, we always have a certain target corresponding to every input. On the contrary, clustering has no target at all, and what we have is only\n\\[ \\{x_1,\\cdots, x_N\\}\\tag{1} \\]\nwhere \\(x_i\\in\\Re^D\\) for \\(i=1,2,\\cdots,N\\). And our mission is to separate the dataset into \\(K\\) groups where \\(K\\) has been given before task\nAn intuitive strategy of clustering is based on two considerations: 1. the distance between data points in the same group should be as small as possible. 2. the distance between data points in the different groups should be as large as possible.\nThis is a little like Fisher Linear Discriminant. Based on these two points, some concepts could be formed.\nThe first one is how to represent a group. We take\n\\[ \\mu_i:i\\in\\{1,2,\\cdots, K\\}\\tag{2} \\]\nas the prototype associated with \\(i\\) th group. A group always contains several points, and a spontaneous idea is using the center of all the points belonging to one group as its prototype. To represent which group \\(\\mathbf{x}_i\\) in equation (1) belongs to, an indicator is necessary, and a 1-of-K coding scheme is used:\n\\[ r_{nk}\\in\\{0,1\\}\\tag{3} \\]\nfor \\(k=1,2,\\cdots,K\\) representing the group number and \\(n = 1,2,\\cdots,N\\) denoting the number of sample point, and where \\(r_{nk}=1\\) then \\(r_{nj}=0\\) for all \\(j\\neq k\\).\nObjective Function A loss function is a good way to measure the quantity of our model during both the training and testing stages. And in the clustering task loss function could not be used because we have no idea about what is correct. However, we can build another function that plays the same role as the loss function and it is also the target of what we want to optimize.\nAccording to the two base points above, we build our objective function:\n\\[ J=\\sum_{n=1}^{N}\\sum_{k=1}^{K}r_{nk}||\\mathbf{x}_n-\\mu_k||^2\\tag{4} \\]\nIn this objective function, the distance is defined as Euclidean distance(However, other measurements of similarity could also be used). Then the mission is to minimize \\(J\\) by finding some certain \\(\\{r_{nk}\\}\\) and \\(\\{\\mu_k\\}\\)\nK-Means Algorithm Now, let’s represent the famous K-Means algorithm. The method includes two steps:\nMinimising \\(J\\) respect to \\(r_{nk}\\) keeping \\(\\mu_k\\) fixed Minimising \\(J\\) respect to \\(\\mu_k\\) keeping \\(r_{nk}\\) fixed  In the first step, according to equation (4), the objective function is linear of \\(r_{nk}\\). So there is a close solution. Then we set:\n\\[ r_{nk}=\\begin{cases} 1\u0026amp;\\text{ if } k=\\arg\\min_{j}||x_n-\\mu_j||^2\\\\ 0\u0026amp;\\text{otherwise} \\end{cases}\\tag{5} \\]\nAnd in the second step, \\(r_{nk}\\) is fixed and we minimize objective function \\(J\\). For it is quadratic, the minimum point is on the stationary point where:\n\\[ \\frac{\\partial J}{\\partial \\mu_k}=-\\sum_{n=1}^{N}r_{nk}(x_n-\\mu_k)=0\\tag{6} \\]\nand we get:\n\\[ \\mu_k = \\frac{\\sum_{n=1}^{N}r_{nk}x_n}{\\sum_{n=1}^{N}r_{nk}}\\tag{7} \\]\n\\(\\sum_{n=1}^{N} r_{nk}\\) is the total number of points from the sample \\(\\{x_1,\\cdots, x_N\\}\\) who belong to prototype \\(\\mu_k\\) or group \\(k\\) at current step. And \\(\\mu_k\\) is just the average of all the points in the group \\(k\\).\nThis two-step, which was calculated by equation (5),(7), would repeat until \\(r_{nk}\\) and \\(\\mu_k\\) not change.\nThe K-means algorithm guarantees to converge because at every step the objective function \\(J\\) is reduced. So when there is only one minimum, the global minimum, the algorithm must converge.\nInput Data Preprocessing before K-means Most algorithms need their input data to obey some rules. To the K-means algorithm, we rescale the input data to mean 0 and variance 1. This is always done by\n\\[ x_n^{(i)} = \\frac{x_n^{(i)}- \\bar{x}^{(i)}}{\\delta^{i}} \\]\nwhere \\(x_n^{(i)}\\) is the \\(i\\) th component of the \\(n\\) th data point, and \\(x_n\\) comes from equation (1), \\(\\bar{x}^{(i)}\\) and \\(\\delta^{i}\\) is the \\(i\\) th mean and standard deviation repectively\nPython code of K-means  class K_Means():  \u0026quot;\u0026quot;\u0026quot; input data should be normalized: mean 0, variance 1 \u0026quot;\u0026quot;\u0026quot;  def clustering(self, x, K):  \u0026quot;\u0026quot;\u0026quot; :param x: inputs :param K: how many groups :return: prototype(center of each group), r_nk, which group k does the n th point belong to \u0026quot;\u0026quot;\u0026quot;  data_point_dimension = x.shape[1]  data_point_size = x.shape[0]  center_matrix = np.zeros((K, data_point_dimension))  for i in range(len(center_matrix)):  center_matrix[i] = x[np.random.randint(0, len(x)-1)]   center_matrix_last_time = np.zeros((K, data_point_dimension))  cluster_for_each_point = np.zeros(data_point_size, dtype=np.int32)  # -----------------------------------visualization-----------------------------------  # the part can be deleted  center_color = np.random.randint(0,1000, (K, 3))/1000.  plt.scatter(x[:, 0], x[:, 1], color=\u0026#39;green\u0026#39;, s=30, marker=\u0026#39;o\u0026#39;, alpha=0.3)  for i in range(len(center_matrix)):  plt.scatter(center_matrix[i][0], center_matrix[i][1], marker=\u0026#39;x\u0026#39;, s=65, color=center_color[i])  plt.show()  # -----------------------------------------------------------------------------------  while (center_matrix_last_time-center_matrix).all() != 0:  # E step  for i in range(len(x)):  distance_to_center = np.zeros(K)  for k in range(K):  distance_to_center[k] = (center_matrix[k]-x[i]).dot((center_matrix[k]-x[i]))  cluster_for_each_point[i] = int(np.argmin(distance_to_center))  # M step  number_of_point_in_k = np.zeros(K)  center_matrix_last_time = center_matrix  center_matrix = np.zeros((K, data_point_dimension))  for i in range(len(x)):  center_matrix[cluster_for_each_point[i]] += x[i]  number_of_point_in_k[cluster_for_each_point[i]] += 1   for i in range(len(center_matrix)):  if number_of_point_in_k[i] != 0:  center_matrix[i] /= number_of_point_in_k[i]  # -----------------------------------visualization-----------------------------------  # the part can be deleted  print(center_matrix)  plt.cla()  for i in range(len(center_matrix)):  plt.scatter(center_matrix[i][0], center_matrix[i][1], marker=\u0026#39;x\u0026#39;, s=65, color=center_color[i])  for i in range(len(x)):  plt.scatter(x[i][0], x[i][1], marker=\u0026#39;o\u0026#39;,s=30, color=center_color[cluster_for_each_point[i]],alpha=0.7)  plt.show()  # -----------------------------------------------------------------------------------  return center_matrix, cluster_for_each_point and the entire project can be found : https://github.com/Tony-Tan/ML and please star me(^_^).\nResults during K-means We use a tool https://github.com/Tony-Tan/2DRandomSampleGenerater to generate the input data from:\nThere are two classes the brown circle and the green circle. Then the K-means algorithm initial two prototypes, the centers of groups, randomly:\nthe two crosses represent the initial centers \\(\\mu_i\\). And then we iterate the two steps:\n Iteration 1\n  Iteration 2\n  Iteration 3\n  Iteration 4\n The result of iterations 3 and 4 do not vary for both objective function value \\(J\\) and parameters. Then the algorithm stopped.\nAnd different initial centers may have different convergence speeds, but they always have the same stop positions.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/K-means-Clustering/","summary":"Preliminaries Numerical Optimization  necessary conditions for maximum  K-means algorithm Fisher Linear Discriminant  Clustering Problem1 The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.\nClustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete.","title":"K-means Clustering"},{"content":"Preliminaries linear regression Maximum Likelihood Estimation Gaussian Distribution Conditional Distribution  From Supervised to Unsupervised Learning1 We have discussed many machine learning algorithms, including linear regression, linear classification, neural network models, and e.t.c, till now. However, most of them are supervised learning, which means a teacher is leading the models to bias toward a certain task. In these problems our attention was on the probability distribution of parameters given inputs, outputs, and models:\n\\[ \\Pr(\\mathbf{\\theta}|\\mathbf{x},\\mathbf{y},M)\\tag{1} \\]\nwhere \\(\\mathbf{\\theta}\\) is a vector of the parameters in the model \\(M\\) and \\(\\mathbf{x}\\), \\(\\mathbf{y}\\) are inputs vector and output vector respectively. As what Bayesian equation told us, we can maximize equation (1) by maximizing the likelihood. And the probability\n\\[ \\Pr(\\mathbf{y}|\\mathbf{x},\\mathbf{\\theta},M)\\tag{2} \\]\nis the important component of the method. More details about the maximum likelihood method can be found Maximum Likelihood Estimation.\nIn today’s discussion, another probability will come to our’s minds. If we have no information about the target in the training set, it is to say we have no teacher in the training stage. We concerned about:\n\\[ \\Pr(\\mathbf{x})\\tag{3} \\]\nOur task, now, could not be called classification or regression anymore. It is referred to as ‘cluster’ which is a progress of identifying which group the data point belongs to. What we have here is just a set of training points \\(\\mathbf{x}\\) without targets and the probability \\(\\Pr(\\mathbf{x})\\).\nAlthough this probability equation (3) is over one random variable, it can be arbitrarily complex. And sometimes, bringing in another random variable as assistance and combining them could get a new distribution that is relatively more tractable. That is to say, a joint distribution of observed variable \\(\\mathbf{x}\\) and another created random variable \\(\\mathbf{z}\\) could be more clear than the original distribution of \\(\\mathbf{x}\\). And sometimes under this combination, the conditional distribution of \\(\\mathbf{x}\\) given \\(\\mathbf{z}\\) is very clear, too.\nLet’s have look at a very simple example. \\(\\mathbf{x}\\) has a distribution:\n\\[ a\\cdot\\exp(-\\frac{(x-\\mu_1)^2}{\\delta_1})+b\\cdot\\exp(-\\frac{(x-\\mu_2)^2}{\\delta_2})\\tag{4} \\]\nwhere \\(a\\) and \\(b\\) are coeficients and \\(\\mu_1\\), \\(\\mu_2\\) are means of the Gaussian distributions and \\(\\delta_1\\) and \\(\\delta_2\\) are variances. It looks like this:\nThe random variable of equation (4) is just \\(x\\). However, now, we introduce another random variable \\(z\\in \\{0,1\\}\\) as assistance into the distribution, in which \\(z\\) has a uniform distribution. Then the distribution (4) can be rewritten in a joint form:\n\\[ \\Pr(x,z)=z\\cdot a\\cdot\\exp(-\\frac{(x-\\mu_1)^2}{\\delta_1})+(1-z)\\cdot b\\cdot\\exp(-\\frac{(x-\\mu_2)^2}{\\delta_2})\\tag{5} \\]\nfor \\(\\mathbf{z}\\) is discrete random variable vector who has a uniform distribution so \\(\\Pr(z=0)=\\Pr(z=1)=0.5\\) and the conditional distribution is \\(\\Pr(x|z=1)\\) is\n\\[ a\\cdot\\exp(-\\frac{(x-\\mu_1)^2}{\\delta_1})\\tag{6} \\]\nlooks like:\nAnd the conditional distribution is \\(\\Pr(x|z=0)\\) is \\[ b\\cdot\\exp(-\\frac{(x-\\mu_2)^2}{\\delta_2})\\tag{7} \\] looks like:\nAnd the margin distribution of \\(x\\) is still equation (4)\nand its 3D vision is:\nSo the conditional distribution of \\(x\\) given \\(z\\) is just a single-variable Gaussian model, which is much simple to deal with. And so is the margin \\(\\Pr(x)\\) by summing up all \\(z\\) (the rule of computing the margin distribution). Here the created random variable \\(z\\) is called a latent variable. It can be considered as an assistant input. However, it can also be considered as a kind of parameter of the model(we will discuss the details later). And the example above is the simplest Gaussian mixture which is wildly used in machine learning, statistics, and other fields. Here \\(z\\), the latent variable, is a discrete random variable, and the continuous latent variable will be introduced later as well.\nMixture distribution can be used to cluster data and what we are going to study are:\nNon probabilistic version: K-means algorithm Discrete latent variable and a relative algorithm which is known as EM algorithm  References  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Mixture-Models/","summary":"Preliminaries linear regression Maximum Likelihood Estimation Gaussian Distribution Conditional Distribution  From Supervised to Unsupervised Learning1 We have discussed many machine learning algorithms, including linear regression, linear classification, neural network models, and e.t.c, till now. However, most of them are supervised learning, which means a teacher is leading the models to bias toward a certain task. In these problems our attention was on the probability distribution of parameters given inputs, outputs, and models:","title":"An Introduction to Mixture Models"},{"content":"Preliminaries ‘An Introduction to Probabilistic Generative Models for Linear Classification’  Idea of logistic regression1 Logistic sigmoid function(logistic function for short) had been introduced in post ‘An Introduction to Probabilistic Generative Models for Linear Classification’. It has an elegant form:\n\\[ \\delta(a)=\\frac{1}{1+e^{-a}}\\tag{1} \\]\nand when \\(a=0\\), \\(\\delta(a)=\\frac{1}{2}\\) and this is just the half of the range of logistic function. This gives us a strong implication that we can set \\(a\\) equals to some functions \\(y(\\mathbf{x})\\), and then\n\\[ a=y(\\mathbf{x})=0\\tag{2} \\]\nbecomes a decision boundary. Here the logistic function plays the same role as the threshold function described in the post ‘From Linear Regression to Linear Classification’\nLogistic Regression of Linear Classification The easiest decision boundary is a constant corresponding to a 1-deminsional input. The dicision boundary of 1-deminsional input is a degenerated line, namely, a point. Here we consider a little complex function - a 2-deminsional input vector \\(\\begin{bmatrix}x_1\u0026amp;x_2\\end{bmatrix}^T\\) and the function \\(y(\\mathbf{x})\\) is:\n\\[ y(\\mathbf{x})=w_0+w_1x_1+w_2x_2=\\mathbf{w}^T\\mathbf{x}= \\begin{bmatrix}w_0\u0026amp;w_1\u0026amp;w_2\\end{bmatrix} \\begin{bmatrix} 1\\\\ x_1\\\\ x_2 \\end{bmatrix}\\tag{3} \\]\nThen we substitute this into equation (1), we got our linear logistic regression function:\n\\[ y(\\mathbf{x})=\\delta(\\mathbf{w}^T\\mathbf{x})=\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}}}\\tag{4} \\]\nThe output of the equation (4) is a real number, its range is \\((0,1)\\). So it can be used to represent a probability of the input belonging to \\(\\mathcal{C}_1\\) whose label is \\(1\\) or \\(\\mathcal{C}_0\\) whose label is \\(0\\) in the training set.\nEstimating the Parameters in Logistic Regression Although logistic regression is called regression, it acts as a classifier. Our mission, now, is to estimate the parameters in equation(4).\nRecalling that the output of equation(4) is \\(\\Pr(\\mathcal{C}_1|\\mathbf{x},M)\\) where \\(M\\) is the model we selected. And the model sometimes can be represented by its parameters. And the mission should you chose to accept it, is to build probability \\(\\Pr(\\mathbf{w}|\\mathbf{x},t)\\) where \\(\\mathbf{x}\\) is the input vector and \\(t\\in\\{0,1\\}\\) is the corresponding label and condition \\(\\mathbf{x}\\) is always been omitted. \\(t\\) is one of \\(\\mathcal{C}_1\\) or \\(\\mathcal{C}_2\\), so the Bayesian relation of \\(\\Pr(\\mathbf{w}|t)\\) is:\n\\[ \\Pr(\\mathbf{w}|t)=\\frac{\\Pr(t|\\mathbf{w})\\Pr(\\mathbf{w})}{\\Pr(t)}=\\frac{\\Pr(\\mathcal{C}_i|\\mathbf{w})\\Pr(\\mathbf{w})}{\\Pr(t)}=\\frac{\\Pr(\\mathcal{C}_i|\\mathbf{x},M)\\Pr(\\mathbf{w})}{\\Pr(t)}\\tag{5} \\]\nThen the maximum likelihood function is employed to estimate the parameters. And the likelihood is just:\n\\[ \\Pr(\\mathcal{C}_1|\\mathbf{x},M)=\\delta(\\mathbf{w}^T\\mathbf{x})=\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}}}\\tag{6} \\]\nWhen we have had the training set:\n\\[ \\{\\mathbf{x}_1,t_1\\},\\{\\mathbf{x}_2,t_2\\},\\cdots,\\{\\mathbf{x}_N,t_N\\}\\tag{7} \\]\nthe likelihood becomes:\n\\[ \\Pi_{t_i\\in \\mathcal{C}_1}\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\Pi_{t_i\\in \\mathcal{C}_0}(1-\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}})\\tag{8} \\]\nIn the second part, when \\(\\mathbf{x}\\) belongs to \\(\\mathcal{C}_0\\) the label is \\(0\\). The output of this class should approach to \\(0\\), so minimising \\(\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\) equals to maximising \\(1-\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\). For equation(8) is not convenient to optimise, we can use the property that \\(t_n\\in{0,1}\\) and we have:\n\\[ \\begin{aligned} \u0026amp;\\Pi_{t_i\\in \\mathcal{C}_1}\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\Pi_{t_i\\in \\mathcal{C}_0}(1-\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}})\\\\ =\u0026amp;\\Pi_i^N(\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}})^{t_i}(1-\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}})^{1-t_i} \\end{aligned} \\tag{9} \\]\nFrom now on, we turn to an optimization problem. Maximizing equation(9) equals to minimize its minus logarithm(we use \\(y_i\\) retpresent \\(\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\)):\n\\[ \\begin{aligned} E\u0026amp;=-\\mathrm{ln}\\;\\Pi^N_{i=1}y_i^{t_i}(1-y_i)^{1-t_i}\\\\ \u0026amp;=-\\sum^N_{i=1}(t_i\\mathrm{ln}y_i+(1-t_i)\\mathrm{ln}(1-y_i)) \\end{aligned} \\tag{10} \\]\nEquation(10) is called cross-entropy, which is a very important concept in information theory and is called cross-entropy error in machine learning which is also a very useful function.\nFor there is no closed-form solution to the optimization problem in equation(10), ‘steepest descent algorithm’ is employed. And what we need to calculate firstly is the derivative of equation(10). For we want to get the derivative of \\(\\mathbf{w}\\)of function \\(y_i(\\mathbf{x})\\), the chain rule is necessary:\n\\[ \\begin{aligned} \\frac{dE}{dw}\u0026amp;=-\\frac{d}{dw}\\sum^N_{i=1}(t_i\\mathrm{ln}y_i+(1-t_i)\\mathrm{ln}(1-y_i))\\\\ \u0026amp;=-\\sum^N_{i=1}\\frac{d}{dw}t_i\\mathrm{ln}y_i+\\frac{d}{dw}(1-t_i)\\mathrm{ln}(1-y_i)\\\\ \u0026amp;=-\\sum^N_{i=1}t_i\\frac{y_i\u0026#39;}{y_i}+(1-t_i)\\frac{-y\u0026#39;_i}{1-y_i} \\end{aligned} \\tag{11} \\]\nand\n\\[ \\begin{aligned} y_i\u0026#39;\u0026amp;=\\frac{d}{dw}\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\\\ \u0026amp;=\\frac{\\mathbf{x}e^{-\\mathbf{w}^T\\mathbf{x_i}}}{(1+e^{-\\mathbf{w}^T\\mathbf{x_i}})^2} \\end{aligned}\\tag{12} \\]\nSubstitute equation (12) into equation (11), we have:\n\\[ \\begin{aligned} \\frac{dE}{dw}\u0026amp;=-\\sum_{i=1}^Nt_i\\frac{\\frac{\\mathbf{x}e^{-\\mathbf{w}^T\\mathbf{x_i}}}{(1+e^{-\\mathbf{w}^T\\mathbf{x_i}})^2}}{\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}}-(1-t_i)\\frac{\\frac{\\mathbf{x}e^{-\\mathbf{w}^T\\mathbf{x_i}}}{(1+e^{-\\mathbf{w}^T\\mathbf{x_i}})^2}}{\\frac{e^{-\\mathbf{w}^T\\mathbf{x_i}}}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}}\\\\ \u0026amp;=-\\sum_{i=1}^Nt_i\\frac{\\mathbf{x}e^{-\\mathbf{w}^T\\mathbf{x_i}}}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}-(1-t_i)\\frac{\\mathbf{x}}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\\\ \u0026amp;=-\\sum_{i=1}^N(t_i\\frac{e^{-\\mathbf{w}^T\\mathbf{x_i}}}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}-(1-t_i)\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}})\\mathbf{x}\\\\ \u0026amp;=-\\sum_{i=1}^N(t_i(1-y_i)-(1-t_i)y_i)\\mathbf{x}\\\\ \u0026amp;=-\\sum_{i=1}^N(t_i-y_i)\\mathbf{x} \\end{aligned}\\tag{13} \\]\nThen we can update \\(\\mathbf{w}\\) :\n\\[ \\mathbf{w} = \\mathbf{w} - \\mathrm{learning\\; rate} \\times (-\\frac{1}{N}\\sum_{i=1}^N(t_i-y_i)\\mathbf{x})\\tag{14} \\]\nCode for logistic regression class LogisticRegression():   def logistic_sigmoid(self, a):  return 1./(1.0 + np.exp(-a))   def fit(self, x, y, e_threshold, lr=1):  x = np.array(x)/320.-1  # augment the input  x_dim = x.shape[0]  x = np.c_[np.ones(x_dim), x]  # initial parameters in 0.01 to 1  w = np.random.randint(1, 100, x.shape[1])/100.  number_of_points = np.size(y)  for dummy in range(1000):  y_output = self.logistic_sigmoid(w.dot(x.transpose()))  # gradient calculation  e_gradient = np.zeros(x.shape[1])  for i in range(number_of_points):  e_gradient += (y_output[i]-y[i])*x[i]  e_gradient = e_gradient / number_of_points  # update parameter  w += -e_gradient*lr  e = 0  for i in range(number_of_points):  e += -(y[i] * np.log(y_output[i]) + (1 - y[i]) * np.log(1 - y_output[i]))  e /= number_of_points  if e \u0026lt;= e_threshold:  break  return w The entire project can be found The entire project can be found https://github.com/Tony-Tan/ML and please star me.\nTwo hyperparameters are the learning rate and the stop condition. When the error is lower than the stopping threshold, the algorithm stops.\nExperiment 1 with the learning rate 1, and different learning rates lead to different convergence rates:\nExperiment 2 with the learning rate 1, and different learning rates lead to different convergence rates:\nExperiment 3 with the learning rate 1, and different learning rates lead to different convergence rates:\nSeveral Traps in Logistic Regression Input value should be normalized and centered at 0 Learning rate is chosen corroding to equation (14) but not \\(-\\sum_{i=1}^N(t_i-y_i)\\mathbf{x}\\) because the uncertain coefficient \\(N\\) parameter vector \\(\\mathbf{w}\\) identifies the direction, so its margin can be arbitrarily large. And to a large \\(\\mathbf{w}\\) , \\(y_i(\\mathbf{x})\\) is very close to \\(1\\) or \\(0\\), but it can never be \\(1\\) or \\(0\\). So there is no optimization position, and the equation(13) can never be \\(0\\) which means the algorithm can never stop by himself. The more large margin, more steppen the curve  Considering \\(\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}}}\\), when the margin of \\(\\mathbf{w}\\) grows, we can write it in a combination of margin \\(M\\) and direction vector \\(\\mathbf{w}_d\\): \\(\\frac{1}{1+e^{-M(\\mathbf{w_d}^T\\mathbf{x}})}\\). And the function \\(\\frac{1}{1+e^{-M(x)}}\\) varies according \\(M\\) is like(when \\(M\\) grows the logistic function is more like Heaviside step function):    References  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Logistic-Regression/","summary":"Preliminaries ‘An Introduction to Probabilistic Generative Models for Linear Classification’  Idea of logistic regression1 Logistic sigmoid function(logistic function for short) had been introduced in post ‘An Introduction to Probabilistic Generative Models for Linear Classification’. It has an elegant form:\n\\[ \\delta(a)=\\frac{1}{1+e^{-a}}\\tag{1} \\]\nand when \\(a=0\\), \\(\\delta(a)=\\frac{1}{2}\\) and this is just the half of the range of logistic function. This gives us a strong implication that we can set \\(a\\) equals to some functions \\(y(\\mathbf{x})\\), and then","title":"Logistic Regression"},{"content":"Preliminaries Probability   Bayesian Formular  Calculus  Probabilistic Generative Models1 The generative model used for making decisions contains an inference step and a decision step:\nInference step is to calculate \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) which means the probability of \\(\\mathbf{x}\\) belonging to the class \\(\\mathcal{C}_k\\) given \\(\\mathbf{x}\\) Decision step is to make a decision based on \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) which was calculated in step 1  In this post, we just give an introduction and a framework for the probabilistic generative model in classification. But the details of how to estimate the parameters in the model will not be introduced.\nFrom Bayesian Formular to Logistic Sigmoid Function To build \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\), we can start from Bayesian formula. To the class \\(\\mathcal{C}_1\\) of a two-classes problem, the posterior probability:\n\\[ \\begin{aligned} \\Pr(\\mathcal{C}_1|\\mathbf{x})\u0026amp;=\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)}{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)+\\Pr(\\mathbf{x}|\\mathcal{C}_2)\\Pr(\\mathcal{C}_2)}\\\\ \u0026amp;=\\frac{1}{1+\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_2)\\Pr(\\mathcal{C}_2)}{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)}} \\end{aligned}\\tag{1} \\]\nrepresents a new function:\n\\[ \\begin{aligned} \\Pr(\\mathcal{C}_1|\\mathbf{x})\u0026amp;=\\delta(a)\\\\ \u0026amp;=\\frac{1}{1+e^{-a}} \\end{aligned}\\tag{2} \\]\nwhere:\n\\[ a=\\ln\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)}{\\Pr(\\mathbf{x}|\\mathcal{C}_2)\\Pr(\\mathcal{C}_2)}\\tag{3} \\]\nAn usual question is why we set \\(a=\\ln\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)}{\\Pr(\\mathbf{x}|\\mathcal{C}_2)\\Pr(\\mathcal{C}_2)}\\) but not \\(a=\\ln\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_2)\\Pr(\\mathcal{C}_2)}{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)}\\). In my opinion, this \\(a\\) just determine the graph of function \\(\\delta(a)\\). However, we perfer monotone-increasing function and \\(\\frac{1}{1+e^{-a}}\\) is just a monotone-increasing function but \\(\\frac{1}{1+e^{a}}\\) is not.\n\\(\\delta(\\cdot)\\) is called logistic sigmoid function or squashing function, because it maps any number into interval \\((0,1)\\). The range of the function is just within the range of probability. So it is a good way to represent some kinds of probability, such as the \\(\\Pr(\\mathcal{C}_1|\\mathbf{x})\\). the shape of the logistic sigmoid function is:\nSome Properties of Logistic Sigmoid For the logistic sigmoid function is symmetrical, then:\n\\[ 1-\\delta(a)=\\frac{e^{-a}}{1+e^{-a}}=\\frac{1}{e^a+1}\\tag{4} \\]\nand:\n\\[ \\delta(-a)=\\frac{1}{1+e^a}\\tag{5} \\]\nSo, we have an important equation:\n\\[ 1-\\delta(a)=\\delta(-a)\\tag{6} \\]\nThe inverse function of \\(y=\\delta(a)\\) is: \\[ \\begin{aligned} y\u0026amp;=\\frac{1}{1+e^{-a}}\\\\ e^{-a}\u0026amp;=\\frac{1}{y}-1\\\\ a\u0026amp;=-\\ln(\\frac{1-y}{y})\\\\ a\u0026amp;=\\ln(\\frac{y}{1-y}) \\end{aligned}\\tag{7} \\]\nThe derivative of the logistic sigmoid function is: \\[ \\frac{d\\delta(a)}{d a}=\\frac{e^{-a}}{(1+e^{-a})^2}=(1-\\delta(a))\\delta(a)\\tag{8} \\]\nMultiple Classes Problems We, now, extend the logistic sigmoid function into multiple classes condition. And we also start from the Bayesian formula:\n\\[ \\Pr(\\mathcal{C}_k|\\mathbf{x})=\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}{\\sum_i\\Pr(\\mathbf{x}|\\mathcal{C}_i)\\Pr(\\mathcal{C}_i)}\\tag{9} \\]\nIn this condition,if we set \\(a_i=\\ln\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}{\\Pr(\\mathbf{x}|\\mathcal{C}_i)\\Pr(\\mathcal{C}_i)}\\), the whole fomular will be too complecated. To simplify the equation, we just set:\n\\[ a_i=\\ln \\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)\\tag{10} \\]\nand we get a function of posterior probability: \\[ \\Pr(\\mathcal{C}_k|\\mathbf{x})=\\frac{e^{a_k}}{\\sum_i e^{a_i}}\\tag{11} \\] And according to the property of probability, we get the value of function: \\[ y(a)=\\frac{e^{a_k}}{\\sum_i e^{a_i}}\\tag{12} \\] belongs to interval \\([0,1]\\). And it is called the softmax function. Although according to equation (10), the domain of the definition of softmax function is \\((-\\infty,0]\\), \\(a\\) can be any real number. It’s called softmax because it is a smooth version of the max function.\nWhen \\(a_k\\gg a_j\\) for \\(k\\neq j\\), we have:\n\\[ \\begin{aligned} \\Pr(\\mathbf{x}|\\mathcal{C}_k)\u0026amp;\\simeq1\\\\ \\Pr(\\mathbf{x}|\\mathcal{C}_j)\u0026amp;\\simeq0 \\end{aligned} \\]\nSo both the logistic sigmoid function and softmax function can be used to form generative classifiers, which gives a value to the decision step.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Probabilistic-Generative-Models/","summary":"Preliminaries Probability   Bayesian Formular  Calculus  Probabilistic Generative Models1 The generative model used for making decisions contains an inference step and a decision step:\nInference step is to calculate \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) which means the probability of \\(\\mathbf{x}\\) belonging to the class \\(\\mathcal{C}_k\\) given \\(\\mathbf{x}\\) Decision step is to make a decision based on \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) which was calculated in step 1  In this post, we just give an introduction and a framework for the probabilistic generative model in classification.","title":"An Introduction to Probabilistic Generative Models"},{"content":"Preliminaries linear algebra   inner multiplication projection  Idea of Fisher linear discriminant1 ‘Least-square method’ in classification can only deal with a small set of tasks. That is because it was designed for the regression task. Then we come to the famous Fisher linear discriminant. This method is also discriminative for it gives directly the class to which the input \\(\\mathbf{x}\\) belongs. Assuming that the linear function\n\\[ y=\\mathbf{w}^T\\mathbf{x}+w_0\\tag{1} \\]\nis employed as before. Then the threshold function \\(f(\\mathbf{y})=\\begin{cases}1 \u0026amp;\\text{ if } y\\leq 0\\\\0 \u0026amp;\\text{ otherwise }\\end{cases}\\) was employed. If \\(y\u0026lt;0\\) or equivalenttly \\(\\mathbf{w}^T\\mathbf{x}\\leq -w_0\\) , \\(\\mathbf{x}\\) belongs to \\(\\mathcal{C}_1\\), or it belongs to \\(\\mathcal{C}_2\\).\nThis is an intuitive classification framework, but if such kinds of parameters exist and how to find them out is still a hard problem.\nFrom the linear algebra view, when we set \\(w_0=0\\) the equation (1) can be viewed as the vector \\(\\mathbf{x}\\) projecting on vector \\(\\mathbf{w}\\).\nAnd a good parameter vector direction and a threshold may solve this problem. And the measurement of how good the parameter vector direction is has some different candidates.\nDistance Between Class Center The first strategy that comes to us is to maximize the distance between the projections of the centers of different classes.\nThe first step is to get the center of a class \\(\\mathcal{C}_k\\) whose size is \\(N_k\\) by:\n\\[ \\mathbf{m}_k=\\frac{1}{N_k}\\sum_{x_i\\in \\mathcal{C}_k}\\mathbf{x}_i\\tag{2} \\]\nSo the distance between the projections \\(m_1\\), \\(m_2\\) of centers of the two classes, \\(\\mathbf{m}_1\\), \\(\\mathbf{m}_2\\) is:\n\\[ m_1-m_2=\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2)\\tag{3} \\]\nAnd for the \\(\\mathbf{w}\\) here is referred to as the direction vector, its margin is \\(1\\):\n\\[ ||\\mathbf{w}||=1\\tag{4} \\]\nWhen \\(\\mathbf{w}\\) has the same direction with \\(\\mathbf{m}_1-\\mathbf{m}_2\\), equation(3) get its maximum value.\nThen the result looks like this:\nThe blue star and blue circle are the center of red stars and green circles, respectively. and the blue arrow is the direction we want in which the projections of center points have the longest distance.\nWith our observation, this line does not give the optimum solution. Because, although the projections of centers have the longest distance on this line, the projections of all sample points scatter into a relatively large region and some of them from different classes have been mixed up.\nThis phenomenon exists in our daily life. For example, when two seats are closed, the people sitting on them do not have enough space(this is the original condition). Then we move the seats and make them a little farther from each other(make the projections of centers far from each other). But this time, two big guys come to sit on them(the projection has a big variance), and the space is still not enough.\nIn our problem, the projections of the points of a class are the big guys. We need to make the projection of centers far away from each other and make the projections of points in one class slender (which means a lower variance) at the same time.\nThe variance of the projected points of class \\(k\\) can be calculated by:\n\\[ s^2_k=\\sum_{n\\in \\mathcal{C}_k}(y_n - m_k)^2\\tag{5} \\]\nand it is also called within-class variance.\nTo make the seat comfortable for people who sit on them, we need to make the seat as far as possible from each other(maximize \\((m_2-m_1)^2\\)) and only allow children to sit(minimize the sum of within-class variance).\nFisher criterion satisfies this requirement:\n\\[ J(\\mathbf{w})=\\frac{(m_2-m_1)^2}{s_1^2+s_2^2}\\tag{6} \\]\nAnd \\(J(\\mathbf{w})\\) in details is:\n\\[ \\begin{aligned} J(\\mathbf{w})\u0026amp;=\\frac{(m_2-m_1)^2}{s_1^2+s_2^2}\\\\ \u0026amp;=\\frac{(\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2))^T(\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2))}{\\sum_{n\\in \\mathcal{C}_1}(y_n - m_1)^2+\\sum_{n\\in \\mathcal{C}_2}(y_n - m_2)^2}\\\\ \u0026amp;=\\frac{(\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2))^T(\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2))} {\\sum_{n\\in \\mathcal{C}_1}(\\mathbf{w}^T\\mathbf{x}_n - \\mathbf{w}^T\\mathbf{m}_1)^2+\\sum_{n\\in \\mathcal{C}_2}(\\mathbf{w}^T\\mathbf{x}_n - \\mathbf{w}^T\\mathbf{m}_2)^2}\\\\ \u0026amp;=\\frac{\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2)(\\mathbf{m}_1-\\mathbf{m}_2)^T\\mathbf{w}} {\\mathbf{w}^T(\\sum_{n\\in \\mathcal{C}_1}(\\mathbf{x}_n - \\mathbf{m}_1)(\\mathbf{x}_n - \\mathbf{m}_1)^T+\\sum_{n\\in \\mathcal{C}_2}(\\mathbf{x}_n - \\mathbf{m}_2)(\\mathbf{x}_n - \\mathbf{m}_2)^T)\\mathbf{w}} \\end{aligned}\\tag{7} \\]\nAnd we can set:\n\\[ \\begin{aligned} S_B \u0026amp;= (\\mathbf{m}_1-\\mathbf{m}_2)(\\mathbf{m}_1-\\mathbf{m}_2)^T\\\\ S_W \u0026amp;= \\sum_{n\\in \\mathcal{C}_1}(\\mathbf{x}_n - \\mathbf{m}_1)(\\mathbf{x}_n - \\mathbf{m}_1)^T+\\sum_{n\\in \\mathcal{C}_2}(\\mathbf{x}_n - \\mathbf{m}_2)(\\mathbf{x}_n - \\mathbf{m}_2)^T \\end{aligned}\\tag{8} \\]\nwhere \\(S_B\\) represents the covariance matrix Between classes, and \\(S_W\\) represents the Within classes covariance. Then the equation(8) becomes:\n\\[ \\begin{aligned} J(\\mathbf{w})\u0026amp;=\\frac{\\mathbf{w}^TS_B\\mathbf{w}}{\\mathbf{w}^TS_W\\mathbf{w}} \\end{aligned}\\tag{9} \\]\nTo maximise the \\(J(\\mathbf{w})\\), we should differentiat equation (9) with respect to \\(\\mathbf{w}\\) firstly:\n\\[ \\begin{aligned} \\frac{\\partial }{\\partial \\mathbf{w}}J(\\mathbf{w})\u0026amp;=\\frac{\\partial }{\\partial \\mathbf{w}}\\frac{\\mathbf{w}^TS_B\\mathbf{w}}{\\mathbf{w}^TS_W\\mathbf{w}}\\\\ \u0026amp;=\\frac{(S_B+S_B^T)\\mathbf{w}(\\mathbf{w}^TS_W\\mathbf{w})-(\\mathbf{w}^TS_B\\mathbf{w})(S_W+S_W^T)\\mathbf{w}}{(\\mathbf{w}^TS_W\\mathbf{w})^T(\\mathbf{w}^TS_W\\mathbf{w})} \\end{aligned}\\tag{10} \\]\nand set it to zero:\n\\[ \\frac{(S_B+S_B^T)\\mathbf{w}(\\mathbf{w}^TS_W\\mathbf{w})-(\\mathbf{w}^TS_B\\mathbf{w})(S_W+S_W^T)\\mathbf{w}}{(\\mathbf{w}^TS_W\\mathbf{w})^T(\\mathbf{w}^TS_W\\mathbf{w})}=0\\tag{11} \\]\nand then:\n\\[ \\begin{aligned} (S_B+S_B^T)\\mathbf{w}(\\mathbf{w}^TS_W\\mathbf{w})\u0026amp;=(\\mathbf{w}^TS_B\\mathbf{w})(S_W+S_W^T)\\mathbf{w}\\\\ (\\mathbf{w}^TS_W\\mathbf{w})S_B\\mathbf{w}\u0026amp;=(\\mathbf{w}^TS_B\\mathbf{w})S_W\\mathbf{w} \\end{aligned}\\tag{12} \\]\nBecause \\((\\mathbf{w}^TS_W\\mathbf{w})\\) and \\((\\mathbf{w}^TS_B\\mathbf{w})\\) are scalars and according equation (8) when we multiply both sides by \\(\\mathbf{w}\\) and we have\n\\[ S_B \\mathbf{w}= (\\mathbf{m}_1-\\mathbf{m}_2)((\\mathbf{m}_1-\\mathbf{m}_2)^T\\mathbf{w})\\tag{13} \\]\nand \\((\\mathbf{m}_1-\\mathbf{m}_2)^T\\mathbf{w}\\) is a scalar and \\(S_B\\mathbf{w}\\) have the same direction with \\((\\mathbf{m}_1-\\mathbf{m}_2)\\)\nso the equation (12) can be written as:\n\\[ \\begin{aligned} \\mathbf{w}\u0026amp;\\propto S^{-1}_WS_B\\mathbf{w}\\\\ \\mathbf{w}\u0026amp;\\propto S^{-1}_W(\\mathbf{m}_1-\\mathbf{m}_2) \\end{aligned}\\tag{14} \\]\nSo, up to now, we have had a result parameter vector \\(\\mathbf{w}\\) based on maximizing Fisher Criterion.\nCode The code of the process is relatively easy:\nclass LinearClassifier():   def fisher(self, x, y):  x = np.array(x)  x_dim = x.shape[1]  m_1 = np.zeros(x_dim)  m_1_size = 0  m_2 = np.zeros(x_dim)  m_2_size = 0  for i in range(len(y)):  if y[i] == 0:  m_1 = m_1 + x[i]  m_1_size += 1  else:  m_2 = m_2 + x[i]  m_2_size += 1  if m_1_size != 0 and m_2_size != 0:  m_1 = (m_1/m_1_size).reshape(-1, 1)  m_2 = (m_2/m_2_size).reshape(-1, 1)  s_c_1 = np.zeros([x_dim, x_dim])  s_c_2 = np.zeros([x_dim, x_dim])  for i in range(len(y)):  if y[i] == 0:  s_c_1 += (x[i] - m_1).dot((x[i] - m_1).transpose())  else:  s_c_2 += (x[i] - m_2).dot((x[i] - m_2).transpose())  s_w = s_c_1 + s_c_2   return np.linalg.inv(s_w).dot(m_2-m_1) The entire project can be found https://github.com/Tony-Tan/ML and please star me.\nThe results of the code(where the line is the one to which the points are projected):\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006. And what we should do is estimate the parameters of the model.↩︎\n   ","permalink":"https://anthony-tan.com/Fisher-Linear-Discriminant/","summary":"Preliminaries linear algebra   inner multiplication projection  Idea of Fisher linear discriminant1 ‘Least-square method’ in classification can only deal with a small set of tasks. That is because it was designed for the regression task. Then we come to the famous Fisher linear discriminant. This method is also discriminative for it gives directly the class to which the input \\(\\mathbf{x}\\) belongs. Assuming that the linear function\n\\[ y=\\mathbf{w}^T\\mathbf{x}+w_0\\tag{1} \\]","title":"Fisher Linear Discriminant(LDA)"},{"content":"Preliminaries convex definition linear algebra   vector length vector direction  Discriminant Function in Classification The discriminant function or discriminant model is on the other side of the generative model. And we, here, have a look at the behavior of the discriminant function in linear classification.1\nIn the post ‘Least Squares Classification’, we have seen, in a linear classification task, the decision boundary is a line or hyperplane by which we separate two classes. And if our model is based on the decision boundary or, in other words, we separate inputs by a function and a threshold, the model is a discriminant model and the decision boundary is formed by the function and a threshold.\nNow, we are going to talk about what the decision boundaries look like in the \\(K\\)-classes problem when \\(K=2\\) and \\(K\u0026gt;2\\). To illustrate the boundaries, we only consider the 2D(two dimensional) input vector \\(\\mathbf{x}\\) who has only two components.\nTwo classes The easiest decision boundary comes from 2-dimensional input space which is separated into 2 regions:\nwhose decision boundary is:\n\\[ \\mathbf{w}^T\\mathbf{x}+w_0=\\text{ constant }\\tag{1} \\]\nThis equation is equal to \\(\\mathbf{w}^T\\mathbf{x}+w_0=0\\) because \\(w_0\\) is also a constant, so it can be merged with the r.h.s. constant. Of course, the 1-dimensional input space is easier than 2-dimensional, and its decision boundary is a point.\nLet’s go back to the line, and it has the following properties:\nThe vector \\(\\mathbf{w}\\) always points to a certain region and is perpendicular to the line. \\(w_0\\) decides the location of the boundary relative to the origin. The perpendicular distance \\(r\\) to the line of a point \\(\\mathbf{x}\\) can be calculated by \\(r=\\frac{y(\\mathbf{x})}{||\\mathbf{w}||}\\) where \\(y(\\mathbf{x})=\\mathbf{w}^T\\mathbf{x}+w_0\\)  Because these three properties are all basic concepts of a line, we just prove the third point roughly:\nproof: We set \\(\\mathbf{x}_{\\perp}\\) is the projection of \\(\\mathbf{x}\\) on the line.\nWe using the first point that \\(\\mathbf{w}\\) is perpendicular to the line and \\(\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\) is the union vector:\n\\[ \\mathbf{x}=\\mathbf{x}_{\\perp}+r\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\tag{2} \\]\nand we substitute equation (2) to the line function \\(y(\\mathbf{x})=\\mathbf{w}^T\\mathbf{x}+w_0\\) :\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;=\\mathbf{w}^T(\\mathbf{x}_{\\perp}+r\\frac{\\mathbf{w}}{||\\mathbf{w}||})+w_0\\\\ \u0026amp;=\\mathbf{w}^T\\mathbf{x}_{\\perp}+\\mathbf{w}^Tr\\frac{\\mathbf{w}}{||\\mathbf{w}||}+w_0\\\\ \u0026amp;=\\mathbf{w}^Tr\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\\\ \u0026amp;=r\\frac{||\\mathbf{w}||^2}{||\\mathbf{w}||}\\\\ \\end{aligned}\\tag{3} \\]\nSo we have\n\\[ r=\\frac{y(\\mathbf{x})}{||\\mathbf{w}||}\\tag{4} \\]\nQ.E.D.\nHowever, augmented vectors \\(\\mathbf{w}= \\begin{bmatrix}w_0\u0026amp;w_1\u0026amp; \\cdots\u0026amp;w_d\\end{bmatrix}^T\\) and \\(\\mathbf{x}= \\begin{bmatrix}1\u0026amp;x_1\u0026amp; \\cdots\u0026amp;x_d\\end{bmatrix}^T\\) can cancel \\(w_0\\) of the original boundary equation. So a \\(d+1\\)-dimensional hyperplane that went through the origin could be instea replaced by an \\(d\\)-dimensional hyperplane.\nMultiple Classes Things changed when we consider more than 2 classes. Their boundaries become more complicated, and we have 3 different strategies for this problem intuitively:\n1-versus-the-rest Classifier This strategy needs at least \\(K-1\\) classifiers(boundaries). Each classifier \\(k\\) just decides which side belongs to class \\(k\\) and the other side does not belong to \\(k\\). So when we have two boundaries, like:\nwhere the region \\(R_4\\) is embarrassed, based on the properties of the decision boundary, and the definition of classification in the post‘From Linear Regression to Linear Classification’, region \\(R_4\\) can not belong to \\(\\mathcal{C}_1\\) and \\(\\mathbb{C}_2\\) simultaneously.\nSo the first strategy can work for some regions, but there are some black whole regions where the input \\(\\mathbf{x}\\) belongs to more than one class and some white whole regions where the input \\(\\mathbf{x}\\) belongs to no classes(region \\(R_3\\) could be such a region)\n1-versus-1 classifier Another kind of multiple class boundary is the combination of several 1-versus-1 linear decision boundaries. Both sides of a decision boundary belong to a certain class, not like the 1-versus-rest classifier. And to a \\(K\\) class task, it needs \\(K(K-1)/2\\) binary discriminant functions.\nHowever, the contradiction still exists. Region \\(R_4\\) belongs to class \\(\\mathcal{C}_1\\), \\(\\mathcal{C}_2\\), and \\(\\mathcal{C}_3\\) simultaneously.\nSo this is also not good for all situations.\n\\(K\\) Linear functions We use a set of \\(K\\) linear functions: \\[ \\begin{aligned} y_1(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_1\\mathbf{x}+w_{10}\\\\ y_2(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_2\\mathbf{x}+w_{20}\\\\ \u0026amp;\\vdots \\\\ y_K(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_K\\mathbf{x}+w_{K0}\\\\ \\end{aligned}\\tag{5} \\]\nand an input belongs to \\(k\\) when \\(y_k(\\mathbf{x})\u0026gt;y_j(\\mathbf{x})\\) where \\(j\\in \\{1,2,\\cdots,K\\}\\) that \\(j\\neq k\\). According to this definition, the decision boundary between class \\(k\\) and class \\(j\\) is \\(y_k(\\mathbf{x})=y_j(\\mathbf{x})\\) where \\(k,j\\in\\{1,2,\\cdots,K\\}\\) and \\(j\\neq k\\). Then a decision hyperplane is defined as:\n\\[ (\\mathbf{w}_k-\\mathbf{w}_j)^T\\mathbf{x}+(w_{k0}-w_{j0})=0\\tag{6} \\]\nThese decision boundaries separate the input spaces into \\(K\\) single connect, convex regions.\nproof: choose two points in the region \\(k\\) that \\(k\\in \\{1,2,\\cdots,K\\}\\). \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) are two points in the region. An arbitrary point on the line between \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) can be written as \\(\\mathbf{x}\u0026#39;=\\lambda \\mathbf{x}_A + (1-\\lambda)\\mathbf{x}_B\\) where \\(0\\leq\\lambda\\leq1\\). For the linearity of \\(y_k(\\mathbf{x})\\) we have:\n\\[ y_k(\\mathbf{x}\u0026#39;)=\\lambda y_k(\\mathbf{x}_A) + (1-\\lambda)y_k(\\mathbf{x}_B)\\tag{7} \\]\nBecause \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) belong to class \\(k\\), \\(y_k(\\mathbf{x}_A)\u0026gt;y_j(\\mathbf{x}_A)\\) and \\(y_k(\\mathbf{x}_B)\u0026gt;y_j(\\mathbf{x}_B)\\) where \\(j\\neq k\\). Then \\(y_k(\\mathbf{x}\u0026#39;)\u0026gt;y_j(\\mathbf{x}\u0026#39;)\\) and the region of class \\(k\\) is convex.\nQ.E.D\nThe last strategy seems good. And what we should do is estimate the parameters of the model. The most famous approaches that will study are: 1. Least square 2. Fisher’s linear discriminant 3. Perceptron algorithm\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Discriminant-Functions-and-Decision-Boundary/","summary":"Preliminaries convex definition linear algebra   vector length vector direction  Discriminant Function in Classification The discriminant function or discriminant model is on the other side of the generative model. And we, here, have a look at the behavior of the discriminant function in linear classification.1\nIn the post ‘Least Squares Classification’, we have seen, in a linear classification task, the decision boundary is a line or hyperplane by which we separate two classes.","title":"Discriminant Functions and Decision Boundary"},{"content":"Preliminaries A Simple Linear Regression Least Squares Estimation From Linear Regression to Linear Classification pseudo-inverse  Least Squares for Classification1 Least-squares for linear regression had been talked about in ‘Simple Linear Regression’. And in this post, we want to find out whether this powerful algorithm can be used in classification.\nRecalling the distinction between the properties of classification and regression, two points need to be emphasized again(‘From Linear Regression to Linear Classification’):\nthe targets of regression are continuous but the targets of classification are discrete. the output of the classification hypothesis could be \\(\\mathbb{P}(\\mathcal{C}_k|\\mathbf{x})\\) generatively or the output is just a class label \\(\\mathcal{C}_k\\) discriminatively.  The generative model will be talked about in other posts. And we focus on discriminative models in these posts which means our hypothesis directly gives which class the input belongs to.\nWe want to use least-squares methods which had been designed and proved for linear regression. And what we could do to extend the least-squares method to classification are:\nmodifying the type of output and designing a discriminative model  Modifying the type of output is to convert the class label into a number, like ‘apple’ to \\(1\\), ‘orange’ to 0. And when we use the 1-of-K label scheme(https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/), we could build the model with \\(K\\) linear functions:\n\\[ \\begin{aligned} y_1(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_1\\mathbf{x}\\\\ y_2(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_2\\mathbf{x}\\\\ \\vdots\u0026amp;\\\\ y_K(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_K\\mathbf{x}\\\\ \\end{aligned}\\tag{1} \\]\nwhere \\(\\mathbf{x}=\\begin{bmatrix}1\u0026amp;x_1\u0026amp;x_2\u0026amp;\\cdots\u0026amp;x_n\\end{bmatrix}^T\\) and \\(\\mathbf{w}_i=\\begin{bmatrix}w_0\u0026amp;w_1\u0026amp;w_2\u0026amp;\\cdots\u0026amp;w_n\\end{bmatrix}^T\\) for \\(i=1,2,\\cdots,K\\). And \\(y_i\\) is the \\(i\\) th component of 1-of-K output for \\(i=1,2,\\cdots,K\\). Clearly, the output of each \\(y_i(\\mathbf{x})\\) is continuous and could not be just \\(0\\) or \\(1\\). So we set the largest value to be 1 and others 0.\nWe had discussed the linear regression with the least squares in a ‘single-target’ regression problem. And that idea can also be employed in the multiple targets regression. And these \\(K\\) parameter vectors \\(\\mathbf{w}_i\\) can be calculated simultaneously. We can rewrite the equation (1) into the matrix form:\n\\[ \\mathbf{y}(\\mathbf{x})=W^T\\mathbf{x}\\tag{2} \\]\nwhere the \\(i\\)th column of \\(W\\) is \\(\\mathbf{w}_i\\)\nThen we employ the least square method for a sample:\n\\[ \\{(\\mathbf{x}_1,\\mathbf{t}_1),(\\mathbf{x}_2,\\mathbf{t}_2),\\cdots,(\\mathbf{x}_m,\\mathbf{t}_m)\\} \\tag{3} \\]\nwhere \\(\\mathbf{t}\\) is a \\(K\\)-dimensional target consisting of \\(k-1\\) 0’s and one ‘1’. And each diminsion of output \\(\\mathbf{y}(\\mathbf{x})_i\\) is the regression result of the corresponding dimension of target \\(t_i\\). And we build up the input matrix \\(X\\) of all \\(m\\) input consisting of \\(\\mathbf{x}^T\\) as rows:\n\\[ X=\\begin{bmatrix} -\u0026amp;\\mathbf{x}^T_1\u0026amp;-\\\\ -\u0026amp;\\mathbf{x}^T_2\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\mathbf{x}^T_K\u0026amp;- \\end{bmatrix}\\tag{4} \\]\nThe sum of square errors is:\n\\[ E(W)=\\frac{1}{2}\\mathrm{Tr}\\{(XW-T)^T(XW-T)\\} \\tag{5} \\]\nwhere the matrix \\(T\\) is the target matrix whose \\(i\\) th row in target vevtor \\(\\mathbf{t}^T_i\\). The trace operation is employed because the only the value \\((W\\mathbf{x}^T_i-\\mathbf{t}_i)^T(W\\mathbf{x}_i^T-\\mathbf{t}_i)\\) for \\(i=1,2,\\cdots,m\\) is meaningful, but \\((W\\mathbf{x}^T_i-\\mathbf{t}_i)^T(W\\mathbf{x}_j^T-\\mathbf{t}_j)\\) where \\(i\\neq j\\) and \\(i,j = 1,2,\\cdots,m\\) is useless.\nTo minimize the linear equation in equation(5), we can get its derivative\n\\[ \\begin{aligned} \\frac{dE(W)}{dW}\u0026amp;=\\frac{d}{dW}(\\frac{1}{2}\\mathrm{Tr}\\{(XW-T)^T(XW-T)\\})\\\\ \u0026amp;=\\frac{1}{2}\\frac{d}{dW}(\\mathrm{Tr}\\{W^TX^TXW-T^TXW-W^TX^TT+T^TT\\})\\\\ \u0026amp;=\\frac{1}{2}\\frac{d}{dW}(\\mathrm{Tr}\\{W^TX^TXW\\}-\\mathrm{Tr}\\{T^TXW\\}\\\\ \u0026amp;-\\mathrm{Tr}\\{W^TX^TT\\}+\\mathrm{Tr}\\{T^TT\\})\\\\ \u0026amp;=\\frac{1}{2}\\frac{d}{dW}(\\mathrm{Tr}\\{W^TX^TXW\\}-2\\mathrm{Tr}\\{T^TXW\\}+\\mathrm{Tr}\\{T^TT\\})\\\\ \u0026amp;=\\frac{1}{2}(X^TXW-X^TT) \\end{aligned}\\tag{6} \\]\nand set it equal to \\(\\mathbf{0}\\):\n\\[ \\begin{aligned} \\frac{1}{2}(X^TXW-X^TT )\u0026amp;= \\mathbf{0}\\\\ W\u0026amp;=(X^TX)^{-1}X^TT \\end{aligned}\\tag{7} \\]\nwhere we assume \\(X^TX\\) can be inverted. The component \\((X^TX)^{-1}X^T\\) is also called pseudo-inverse of the matrix \\(X\\) and it is always denoted as \\(X^{\\dagger}\\).\nCode and Result The code of this algorithm is relatively simple because we have programmed the linear regression before which has the same form of equation (7).\nWhat we should care about is the formation of these matrices \\(W\\), \\(X\\), and \\(T\\).\nwe should first convert the target value into the 1-of-K form:\ndef label_convert(y, method =\u0026#39;1-of-K\u0026#39;):  if method == \u0026#39;1-of-K\u0026#39;:  label_dict = {}  number_of_label = 0  for i in y:  if i not in label_dict:  label_dict[i] = number_of_label  number_of_label += 1  y_ = np.zeros([len(y),number_of_label])  for i in range(len(y)):  y_[i][label_dict[y[i]]] = 1  return y_,number_of_label what we do is count the total number of labels(\\(K\\))and we set the \\(i\\) th component of the 1-of-K target to 1 and other components to 0.\nThe kernel of the algorithm is:\nclass LinearClassifier():  def least_square(self, x, y):  x = np.array(x)  x_dim = x.shape[0]  x = np.c_[np.ones(x_dim), x]  w = np.linalg.pinv(x.transpose().dot(x)).dot(x.transpose()).dot(y)  return w.transpose() the line x = np.c_[np.ones(x_dim), x] is to augment the input vector \\(\\mathbf{x}\\) with a dummy value \\(1\\). And the transpose of the result is to make each row represent a weight vector of eqation (2). The entire project can be found The entire project can be found https://github.com/Tony-Tan/ML and please star me 🥰.\nI have tested the algorithm in several training sets, and the result is like the following figures:\nProblems of Least Squares Lack of robustness if outliers (Figure 2 illustrates this problem) Sum of squares error penalizes the predictions that are too correct(the decision boundary will be tracked to the outlinear as the points at right bottom corner in figure 2) Least-squares workes for regression when we assume the target data has a Gaussian distribution and then the least-squares method maximizes the likelihood function. The distribution of targets in these classification tasks is not Gaussian.  References  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Least-Squares-in-Classification/","summary":"Preliminaries A Simple Linear Regression Least Squares Estimation From Linear Regression to Linear Classification pseudo-inverse  Least Squares for Classification1 Least-squares for linear regression had been talked about in ‘Simple Linear Regression’. And in this post, we want to find out whether this powerful algorithm can be used in classification.\nRecalling the distinction between the properties of classification and regression, two points need to be emphasized again(‘From Linear Regression to Linear Classification’):","title":"Least Squares in Classification"},{"content":"Preliminaries An Introduction to Linear Regression A Simple Linear Regression Bayesian theorem Feature extraction  Recall Linear Regression The goal of a regression problem is to find out a function or hypothesis that given an input \\(\\mathbf{x}\\), it can make a prediction \\(\\hat{y}\\) to estimate the target. Both the target \\(y\\) and prediction \\(\\hat{y}\\) here are continuous. They have the properties of numbers1:\n Consider 3 inputs \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) and \\(\\mathbf{x}_3\\) and their coresponding targets are \\(y_1=0\\), \\(y_2=1\\) and \\(y_3=2\\). Then a good predictor should give the predictions \\(\\hat{y}_1\\), \\(\\hat{y}_2\\) and \\(\\hat{y}_3\\) where the distance between \\(\\hat{y}_1\\) and \\(\\hat{y}_2\\) is larger than the one between \\(\\hat{y}_1\\) and \\(\\hat{y}_3\\)\n Some properties of regression tasks we should pay attention to are:\nThe goal of regression is to produce a hypothesis that can give a prediction as close to the target as possible The output of the hypothesis and target are continuous numbers and have numerical meanings, like distance, velocity, weights, and so on.  General Classification On the other side, we met more classification tasks in our life than regression. Such as in the supermarket we can tell the apple and the orange apart easily. And we can even verify whether this apple is tasty or not.\nThen the goal of classification is clear:\n Assign input \\(\\mathbf{x}\\) to a certain class of \\(K\\) available classes. And \\(\\mathbf{x}\\) must belong to one and only one class.\n The input \\(\\mathbf{x}\\), like the input of regression, can be a feature or basis function and can be continuous or discrete. However, its output is discrete. Let’s go back to the example that we can tell apple, orange, and pineapple apart. The difference between apple and orange and the difference between apple and pineapple can not be compared, because the distance(it is the mathematical name of difference) itself had no means.\nA Binary Code Scheme we can not calculate apple and orange directly. So a usual first step in the classification task is mapping the target or labels of an example into a number, like \\(1\\) for the apple and \\(2\\) for the orange.\nA binary code scheme is another way to code targets.\nFor a two classes mission, the numerical labels can be:\n\\[ \\mathcal{C}_1=0 \\text{ and }\\mathcal{C}_2=1\\tag{1} \\]\nIt’s equal to:\n\\[ \\mathcal{C}_1=1 \\text{ and }\\mathcal{C}_2=0\\tag{2} \\]\nAnd to a \\(K\\) classes target, the binary code scheme is:\n\\[ \\begin{aligned} \\mathcal{C}_1 \u0026amp;= \\{1,0,\\cdots,0\\}\\\\ \\mathcal{C}_2 \u0026amp;= \\{0,1,\\cdots,0\\}\\\\ \\vdots \u0026amp; \\\\ \\mathcal{C}_K \u0026amp;= \\{0,0,\\cdots,1\\}\\\\ \\end{aligned}\\tag{3} \\]\nThe \\(n\\)-dimensional input \\(\\mathbf{x}\\in \\mathbb{R}^n\\) and \\(\\mathbb{R}^n\\) is called the input space. In the classification task, the input points can be separated by the targets, and these parts of space are called decision regions and the boundaries between decision regions are called decision boundaries or decision surfaces. When the decision boundary is linear, the task is called linear classification.\nThere are roughly two kinds of procedures for classification:\nDiscriminant Function: assign input \\(\\mathbf{x}\\) to a certain class directly. We infer \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) firstly and then make a decision based on the posterior probability. Inference of \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) was calculated firstly \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) can also be calculated by Bayesian Theorem \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})=\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}{\\Pr(\\mathbf{x})}=\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}{\\sum_k \\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}\\)   They are the discriminate model and generative model, respectively.\nLinear Classification In the regression problem, the output of the linear function:\n\\[ \\mathbf{w}^T\\mathbf{x}+b\\tag{4} \\]\nis approximate of the target. But in the classification task, we want the output to be the class to which the input \\(\\mathbf{x}\\) belongs. However, the output of the linear function is always continuous. This output is more like the posterior probability, say \\(\\Pr({\\mathcal{C}_i|\\mathbf{x}})\\) rather than the discrete class label. To generate a class label output, function \\(f(\\cdot)\\) which is called ‘action function’ in machine learning was employed. For example, we can choose a threshold function as the active function:\n\\[ y(\\mathbf{x})=f(\\mathbf{w}^T\\mathbf{x}+b)\\tag{5} \\]\nwhere \\(f(\\cdot)\\) is the threshold function:\n\\[ f(x) = \\begin{cases}1\u0026amp;x\\geq c\\\\0\u0026amp;\\text{otherwise}\\end{cases}\\tag{6} \\] where \\(c\\) is a constant.\nIn this case, the boundary is \\(\\mathbf{w}^T\\mathbf{x}+b = c\\), and it is a line. So we call this kind of model ‘linear classification’. The input \\(\\mathbf{x}\\) can be replaced by a basis function \\(\\phi(\\mathbf{x})\\) as mentioned in the polynomial regression.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/","summary":"Preliminaries An Introduction to Linear Regression A Simple Linear Regression Bayesian theorem Feature extraction  Recall Linear Regression The goal of a regression problem is to find out a function or hypothesis that given an input \\(\\mathbf{x}\\), it can make a prediction \\(\\hat{y}\\) to estimate the target. Both the target \\(y\\) and prediction \\(\\hat{y}\\) here are continuous. They have the properties of numbers1:\n Consider 3 inputs \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) and \\(\\mathbf{x}_3\\) and their coresponding targets are \\(y_1=0\\), \\(y_2=1\\) and \\(y_3=2\\).","title":"From Linear Regression to Linear Classification"},{"content":"Priliminaries A Simple Linear Regression Least Squares Estimation  Extending Linear Regression with Features1 The original linear regression is in the form:\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;= b + \\mathbf{w}^T \\mathbf{x}\\\\ \u0026amp;=w_01 + w_1x_1+ w_2x_2+\\cdots + w_{m+1}x_{m+1} \\end{aligned}\\tag{1} \\]\nwhere the input vector \\(\\mathbf{x}\\) and parameter \\(\\mathbf{w}\\) are \\(m\\)-dimension vectors whose first components are \\(1\\) and bias \\(w_0=b\\) respectively. This equation is linear for both the input vector and parameter vector. Then an idea come to us, if we set \\(x_i=\\phi_i(\\mathbf{x})\\) then equation (1) convert to:\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;= b + \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x})\\\\ \u0026amp;=w_01 + w_1\\phi_1(\\mathbf{x})+\\cdots + w_{m+1}\\phi_{m+1}(\\mathbf{x}) \\end{aligned}\\tag{2} \\]\nwhere \\(\\phi(\\mathbf{x})=\\begin{bmatrix}\\phi_1(\\mathbf{x})\\\\\\phi_2(\\mathbf{x})\\\\ \\vdots\\\\\\phi_m(\\mathbf{x})\\end{bmatrix}\\) and \\(\\mathbf{w}=\\begin{bmatrix}w_1\\\\w_2\\\\ \\vdots\\\\ w_m\\end{bmatrix}\\) the function with input \\(\\mathbf{x}\\), \\(\\mathbf{\\phi}(\\cdot)\\) is called feature.\nThis feature function was used widely, especially in reducing dimensions of original input(such as in image processing) and increasing the flexibility of the predictor(such as in extending linear regression to polynomial regression).\nPolynomial Regression When we set the feature as:\n\\[ \\phi(x) = \\begin{bmatrix}x\\\\x^2\\end{bmatrix}\\tag{3} \\]\nthe linear regression converts to:\n\\[ y(\\mathbf{x})=b+ w_1x+w_2x^2\\tag{4} \\]\nHowever, the estimation of the parameter \\(\\mathbf{w}\\) is not changed by the extension of the feature function. Because in the least-squares or other optimization algorithms the parameters or random variables are \\(\\mathbf{w}\\), and we do not care about the change of input space. And when we use the algorithm described in ‘least squares estimation’:\n\\[ \\mathbf{w}=(X^TX)^{-1}X^T\\mathbf{y}\\tag{5} \\]\nto estimate the parameter, we got:\n\\[ \\mathbf{w}=(\\Phi^T\\Phi)^{-1}\\Phi^T\\mathbf{y}\\tag{6} \\]\nwhere \\[ \\Phi=\\begin{bmatrix} -\u0026amp;\\phi(\\mathbf{x_1})^T\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\phi(\\mathbf{x_m})^T\u0026amp;-\\end{bmatrix}\\tag{7} \\]\nCode for polynomial regression To the same task in the ‘least squares estimation’, regression of the weights of the newborn baby with days is like:\nThe linear regression result of a male baby is :\nAnd code of the least square polynomial regression with power \\(d\\) is\ndef fit_polynomial(self, x, y, d):  x_org = np.array(x).reshape(-1, 1)  # add a column which is all 1s to calculate bias  x = np.c_[np.ones(x.size).reshape(-1, 1), x_org]  x_org_d = x_org  # building polynomial with highest power d  for i in range(1, d):  x_org_d = x_org_d * x_org  x = np.c_[x, x_org_d]  y = np.array(y).reshape(-1, 1)  w = np.linalg.inv(x.transpose().dot(x)).dot(x.transpose()).dot(y)  return w The entire project can be found The entire project can be found https://github.com/Tony-Tan/ML and please star me.\nAnd the result of the regression is:\nThe blue regression line looks pretty well compared to the right line.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Polynomial-Regression-and-Features-Extension-of-Linear-Regression/","summary":"Priliminaries A Simple Linear Regression Least Squares Estimation  Extending Linear Regression with Features1 The original linear regression is in the form:\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;= b + \\mathbf{w}^T \\mathbf{x}\\\\ \u0026amp;=w_01 + w_1x_1+ w_2x_2+\\cdots + w_{m+1}x_{m+1} \\end{aligned}\\tag{1} \\]\nwhere the input vector \\(\\mathbf{x}\\) and parameter \\(\\mathbf{w}\\) are \\(m\\)-dimension vectors whose first components are \\(1\\) and bias \\(w_0=b\\) respectively. This equation is linear for both the input vector and parameter vector. Then an idea come to us, if we set \\(x_i=\\phi_i(\\mathbf{x})\\) then equation (1) convert to:","title":"Polynomial Regression and Features-Extension of Linear Regression"},{"content":"Priliminaries A Simple Linear Regression Least Squares Estimation linear algebra  Square Loss Function for Regression1 For any input \\(\\mathbf{x}\\), our goal in a regression task is to give a prediction \\(\\hat{y}=f(\\mathbf{x})\\) to approximate target \\(t\\) where the function \\(f(\\cdot)\\) is the chosen hypothesis or model as mentioned in the post https://anthony-tan.com/A-Simple-Linear-Regression/.\nThe difference between \\(t\\) and \\(\\hat{y}\\) can be called ‘error’ or more precisely ‘loss’. Because in an approximation task, ‘error’ occurs by chance and always exists, and ‘loss’ is a good word to represent the difference. The loss can be written generally as function \\(\\ell(f(\\mathbf{x}),t)\\). Intuitively, the smaller the loss, the better the approximation.\nSo the expectation of loss:\n\\[ \\mathbb E[\\ell]=\\int\\int \\ell(f(\\mathbf{x}),t)p(\\mathbf{x},t)d \\mathbf{x}dt\\tag{1} \\]\nshould be as small as possible.\nIn probability viewpoint, the input vector \\(\\mathbf{x}\\), target \\(t\\) and parameters in function(model) \\(f(\\cdot)\\) are all random variables. Then the expectation of loss function may exist.\nConsidering the square error loss function \\(e=(f(\\mathbf{x})-t)^2\\), it is a usual measure of the difference between the prediction and the target. And substitute the loss function into equation (1), we have:\n\\[ \\mathbb E[\\ell]=\\int\\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)d \\mathbf{x}dt\\tag{2} \\]\nTo minimize this function, we could use Euler-Lagrange equation, Fundamental theorem of calculus and Fubini’s theorem:\nFubini’s theorem told us that we can change the order of integration: \\[ \\begin{aligned} \\mathbb E[\\ell]\u0026amp;=\\int\\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)d \\mathbf{x}dt\\\\ \u0026amp;=\\int\\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)dtd \\mathbf{x} \\end{aligned}\\tag{3} \\]\nAccording to the Euler-Lagrange equation, we first create a new function \\(G(x,f,f\u0026#39;)\\): \\[ G(x,f,f\u0026#39;)= \\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)dt\\tag{4} \\]\nThe Euler-Lagrange equation is used to minimize the equation (2): \\[ \\frac{\\partial G}{\\partial f}-\\frac{d}{dx}\\frac{\\partial G}{\\partial f\u0026#39;}=0\\tag{5} \\]\nBecause there is no \\(y\u0026#39;\\) component in function \\(G()\\). Then the equation: \\[ \\frac{\\partial G}{\\partial f}=0\\tag{6} \\] becomes the necessary condition to minimize the equation (2):\n\\[ 2\\int (f(\\mathbf{x})-t)p(\\mathbf{x},t)dt=0 \\tag{7} \\]\nRearrange the equation (7), and we get a good predictor that can minimize the square loss function :\n\\[ \\begin{aligned} \\int (f(\\mathbf{x})-t)p(\\mathbf{x},t)dt\u0026amp;=0\\\\ \\int f(\\mathbf{x})p(\\mathbf{x},t)dt-\\int tp(\\mathbf{x},t)dt\u0026amp;=0\\\\ f(\\mathbf{x})\\int p(\\mathbf{x},t)dt\u0026amp;=\\int tp(\\mathbf{x},t)dt\\\\ f(\\mathbf{x})\u0026amp;=\\frac{\\int tp(\\mathbf{x},t)dt}{\\int p(\\mathbf{x},t)dt}\\\\ f(\\mathbf{x})\u0026amp;=\\frac{\\int tp(\\mathbf{x},t)dt}{p(\\mathbf{x})}\\\\ f(\\mathbf{x})\u0026amp;=\\int tp(t|\\mathbf{x})dt\\\\ f(\\mathbf{x})\u0026amp;= \\mathbb{E}_t[t|\\mathbf{x}] \\end{aligned}\\tag{8} \\]\nWe finally find the expectation of \\(t\\) given \\(\\mathbf{x}\\) is the optimum solution. The expectation of \\(t\\) given \\(\\mathbf{x}\\) is also called the regression function.\nA small summary: \\(\\mathbb{E}[t| \\mathbf{x}]\\) is a good estimate of \\(f(\\mathbf{x})\\)\nMaximum Likelihood Estimation Generally, we assume that there is a generator behind the data:\n\\[ t=g(\\mathbf{x},\\mathbf{w})+\\epsilon\\tag{9} \\]\nwhere the function \\(g(\\mathbf{x},\\mathbf{w})\\) is a deterministic function, \\(t\\) is the target variable and \\(\\epsilon\\) is zero mean Gaussian random variable with percision \\(\\beta\\) which is the inverse variance. Because of the property of Gaussian distribution, \\(t\\) has a Gaussian distribution, with mean(expectation) \\(g(\\mathbf{x},\\mathbf{w})\\) and percesion \\(\\beta\\). And recalling the standard form of Gaussian distribution:\n\\[ \\begin{aligned} \\Pr(t|\\mathbf{x},\\mathbf{w},\\beta)\u0026amp;=\\mathcal{N}(t|g(\\mathbf{x},\\mathbf{w}),\\beta^{-1})\\\\ \u0026amp;=\\frac{\\beta}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{1}{2}(\\beta(x-\\mu)^2)} \\end{aligned}\\tag{10} \\]\nOur task here is to approximate the generator in equation (9) with a linear function. Somehow, when we use the square loss function, the optimum solution for this task is \\(\\mathbb{E}[t|\\mathbf{x}]\\) to equation (8).\nthe solution to equation (10) is:\n\\[ \\mathbb{E}[t|\\mathbf{x}]=g(\\mathbf{x},\\mathbf{w})\\tag{11} \\]\nWe set the linear model as: \\[ f(x)=\\mathbf{w}^T\\mathbf{x}+b\\tag{12} \\]\nand this can be converted to:\n\\[ f(x)= \\begin{bmatrix} b\u0026amp;\\mathbf{w}^T \\end{bmatrix} \\begin{bmatrix} 1\\\\ \\mathbf{x} \\end{bmatrix}=\\mathbf{w}_a^T\\mathbf{x}_a \\tag{13} \\]\nfor short, we just write the \\(\\mathbf{w}_a\\) and \\(\\mathbf{x}_a\\) as \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\). Then the linear model becomes:\n\\[ f(x)=\\mathbf{w}^T\\mathbf{x}\\tag{14} \\]\nAs we mentioned above we consider all the parameter as a random variable, then the conditioned distribution of \\(\\mathbf{w}\\) is \\(\\Pr(\\mathbf{w}|\\mathbf{t},\\beta)\\). \\(X\\) or \\(\\mathbf{x}\\) was omitted in the condition because it does not affect the result at all. And the Bayesian theorem told us:\n\\[ \\Pr(\\mathbf{w}|\\mathbf{t},\\beta)=\\frac{\\Pr( \\mathbf{t}|\\mathbf{w},\\beta) \\Pr(\\mathbf{w})} {\\Pr(\\mathbf{t})}=\\frac{\\text{Likelihood}\\times \\text{Prior}}{\\text{Evidence}}\\tag{15} \\]\nWe want to find the \\(\\mathbf{w}^{\\star}\\) that maximise the posterior probability \\(\\Pr(\\mathbf{w}|\\mathbf{t},\\beta)\\). Because \\(\\Pr(\\mathbf{t})\\) and \\(\\Pr(\\mathbf{w})\\) are constant. Then the maximum of likelihood \\(\\Pr(\\mathbf{t}|\\mathbf{w},\\beta)\\) maximise the posterior probability.\n\\[ \\begin{aligned} \\Pr(\\mathbf{t}|\\mathbf{w},\\beta)\u0026amp;=\\Pi_{i=0}^{N}\\mathcal{N}(t_i|\\mathbf{w}^T\\mathbf{x}_i,\\beta^{-1})\\\\ \\ln \\Pr(\\mathbf{t}|\\mathbf{w},\\beta)\u0026amp;=\\sum_{i=0}^{N}\\ln \\mathcal{N}(t_i|\\mathbf{w}^T\\mathbf{x}_i,\\beta^{-1})\\\\ \u0026amp;=\\sum_{i=0}^{N}\\ln \\frac{\\beta}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{1}{2}(\\beta(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2)}\\\\ \u0026amp;=\\sum_{i=0}^{N} \\ln \\beta - \\sum_{i=0}^{N} \\ln \\sqrt{2\\pi} - \\frac{1}{2}\\beta\\sum_{i=0}^{N}(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2 \\end{aligned}\\tag{16} \\]\nThis gives us a wonderful result.\nWe can only control the component \\(\\frac{1}{2}\\beta\\sum_{i=0}^{N}(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2\\) of the last line of equation(16), because \\(\\sum_{i=0}^{N} \\ln \\beta\\) and \\(- \\sum_{i=0}^{N} \\ln \\sqrt{2\\pi}\\) were decided by the assumptions. In other words, to maximise the likelihood, we just need to minimise:\n\\[ \\sum_{i=0}^{N}(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2\\tag{17} \\]\nThis was just to minimize the sum of squares. Then this optimization problem went back to the least square problem.\nLeast Square Estimation and Maximum Likelihood Estimation When we assume there is a generator:\n\\[ t=g(\\mathbf{x},\\mathbf{w})+\\epsilon\\tag{18} \\]\nbehind the data, and \\(\\epsilon\\) has a zero-mean Gaussian distribution with any precision \\(\\beta\\), the maximum likelihood estimation finally converts to the least square estimation. This is not only worked for linear regression because we did not assume what \\(g(\\mathbf{x},\\mathbf{w})\\) is.\nHowever, when the \\(\\epsilon\\) has a different distribution but not Gaussian distribution, the least square estimation will not be the optimum solution for maximum likelihood estimation.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Maximum-Likelihood-Estimation/","summary":"Priliminaries A Simple Linear Regression Least Squares Estimation linear algebra  Square Loss Function for Regression1 For any input \\(\\mathbf{x}\\), our goal in a regression task is to give a prediction \\(\\hat{y}=f(\\mathbf{x})\\) to approximate target \\(t\\) where the function \\(f(\\cdot)\\) is the chosen hypothesis or model as mentioned in the post https://anthony-tan.com/A-Simple-Linear-Regression/.\nThe difference between \\(t\\) and \\(\\hat{y}\\) can be called ‘error’ or more precisely ‘loss’. Because in an approximation task, ‘error’ occurs by chance and always exists, and ‘loss’ is a good word to represent the difference.","title":"Maximum Likelihood Estimation"},{"content":"Priliminaries A Simple Linear Regression the column space  Another Example of Linear Regression 1 In the blog A Simple Linear Regression, squares of the difference between the output of a predictor and the target were used as a loss function in a regression problem. And it could be also written as:\n\\[ \\ell(\\hat{\\mathbf{y}}_i,\\mathbf{y}_i)=(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i)^T(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i) \\tag{1} \\]\nThe linear regression model in a matrix form is:\n\\[ y=\\mathbf{w}^T\\mathbf{x}+\\mathbf{b}\\tag{2} \\]\nWhat we do in this post is analyze the least-squares methods from two different viewpoints\nConsider a new training set, newborn weights, and time from the WHO:\n  Day Male(kg) Female(kg)    0 3.5 3.4  15 4.0 3.8  45 4.9 4.5  75 5.7 5.2  105 6.4 5.9  135 7.0 6.4  165 7.6 7.0  195 8.2 7.5  225 8.6 7.9  255 9.1 8.3  285 9.5 8.7  315 9.8 9.0  345 10.2 9.4  375 10.5 9.7    View of algebra This is just what the post A Simple Linear Regression did. The core idea of this view is that the loss function is quadratic so its stationary point is the minimum or maximum. Then what to do is just find the stationary point.\nAnd its result is: View of Geometric Such a simple example with just two parameters above had almost messed us up in calculation. However, the practical task may have more parameters, say hundreds or thousands of parameters. It is impossible for us to solve that in a calculus way.\nNow let’s review the linear relation in equation (2) and when we have a training set of \\(m\\) points : \\[ \\{(\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),\\dots,(\\mathbf{x}_m,y_m)\\}\\tag{3} \\]\nBecause they are sampled from an identity “machine”. They can be stacked together in a matrix form as:\n\\[ \\begin{bmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_m \\end{bmatrix}=\\begin{bmatrix} -\u0026amp;\\mathbf{x}_1^T\u0026amp;-\\\\ -\u0026amp;\\mathbf{x}_2^T\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\mathbf{x}_m^T\u0026amp;- \\end{bmatrix}\\mathbf{w}+I_m\\mathbf{b}\\tag{4} \\]\nwhere \\(I_m\\) is an identical matrix whose column and row is \\(m\\) and \\(\\mathbf{b}\\) is \\(b\\) repeating \\(m\\) times. To make the equation shorter and easier to calculate, we can put \\(b\\) into the vector \\(\\mathbf{w}\\) like:\n\\[ \\begin{bmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_m \\end{bmatrix}=\\begin{bmatrix} 1\u0026amp;-\u0026amp;\\mathbf{x}_1^T\u0026amp;-\\\\ 1\u0026amp;-\u0026amp;\\mathbf{x}_2^T\u0026amp;-\\\\ 1\u0026amp;\u0026amp;\\vdots\u0026amp;\\\\ 1\u0026amp;-\u0026amp;\\mathbf{x}_m^T\u0026amp;- \\end{bmatrix} \\begin{bmatrix} b\\\\ \\mathbf{w} \\end{bmatrix} \\tag{5} \\]\nWe use a simplified equation to represent the relation in equation(5): \\[ \\mathbf{y} = X\\mathbf{w}\\tag{6} \\]\nFrom the linear algebra points, equation(6) represents that \\(\\mathbf{y}\\) is in the column space of \\(X\\). However, when \\(\\mathbf{y}\\) isn’t, the equation (6) does not hold anymore. And what we need to do next is to find a vector \\(\\mathbf{\\hat{y}}\\) in the column space which is the closest one to the vector \\(\\mathbf{y}\\):\n\\[ \\arg\\min_{\\mathbf{\\hat{y}}=X\\mathbf{w}} ||\\mathbf{y}-\\mathbf{\\hat{y}}||\\tag{7} \\]\nAnd as we have known, the projection of \\(\\mathbf{y}\\) to the column space of \\(X\\) has the shortest distance to \\(\\mathbf{y}\\)\nAccording to linear algebra, the closest vector in a subspace to a vector is its projection in that subspace. Then our mission now is to find \\(\\mathbf{w}\\) to make:\n\\[ \\mathbf{\\hat{y}} = X\\mathbf{w}\\tag{8} \\]\nwhere \\(\\mathbf{\\hat{y}}\\) is the projection of \\(\\mathbf{y}\\) in the column space of \\(X\\).\nAccording to the projection equation in linear algebra:\n\\[ \\mathbf{\\hat{y}}=X(X^TX)^{-1}X^T\\mathbf{y}\\tag{22} \\]\nThen substitute equation (8) into equation (9) and assuming \\((X^TX)^{-1}\\) exists:\n\\[ \\begin{aligned} X\\mathbf{w}\u0026amp;=X(X^TX)^{-1}X^T\\mathbf{y}\\\\ X^TX\\mathbf{w}\u0026amp;=X^TX(X^TX)^{-1}X^T\\mathbf{y}\\\\ X^TX\\mathbf{w}\u0026amp;=X^T\\mathbf{y}\\\\ \\mathbf{w}\u0026amp;=(X^TX)^{-1}X^T\\mathbf{y} \\end{aligned}\\tag{10} \\]\nTo a thin and tall matrix, \\(X\\), which means that the number of sample points in the training set is far more than the dimension of a sample point, \\((X^TX)^{-1}\\) exists usually.\nCode of Linear Regression(Matrix Form) import pandas as pds import numpy as np import matplotlib.pyplot as plt   class LeastSquaresEstimation():  def __init__(self, method=\u0026#39;OLS\u0026#39;):  self.method = method   def fit(self, x, y):  x = np.array(x).reshape(-1, 1)  # add a column which is all 1s to calculate bias of linear function  x = np.c_[np.ones(x.size).reshape(-1, 1), x]  y = np.array(y).reshape(-1, 1)  if self.method == \u0026#39;OLS\u0026#39;:  w = np.linalg.inv(x.transpose().dot(x)).dot(x.transpose()).dot(y)  b = w[0][0]  w = w[1][0]  return w, b   if __name__ == \u0026#39;__main__\u0026#39;:  data_file = pds.read_csv(\u0026#39;./data/babys_weights_by_months.csv\u0026#39;)  lse = LeastSquaresEstimation()  weight_male, bias_male = lse.fit(data_file[\u0026#39;day\u0026#39;],data_file[\u0026#39;male\u0026#39;])  day_0 = data_file[\u0026#39;day\u0026#39;][0]  day_end = list(data_file[\u0026#39;day\u0026#39;])[-1]  days = np.array([day_0,day_end])  plt.scatter(data_file[\u0026#39;day\u0026#39;], data_file[\u0026#39;male\u0026#39;], c=\u0026#39;r\u0026#39;, label=\u0026#39;male\u0026#39;, alpha=0.5)  plt.scatter(data_file[\u0026#39;day\u0026#39;], data_file[\u0026#39;female\u0026#39;], c=\u0026#39;b\u0026#39;, label=\u0026#39;female\u0026#39;, alpha=0.5)  plt.xlabel(\u0026#39;days\u0026#39;)  plt.ylabel(\u0026#39;weight(kg)\u0026#39;)  plt.legend()  plt.show() the entire project can be found at https://github.com/Tony-Tan/ML and please star me 😀.\nIts output is also like:\nReference  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Least-Squares-Estimation/","summary":"Priliminaries A Simple Linear Regression the column space  Another Example of Linear Regression 1 In the blog A Simple Linear Regression, squares of the difference between the output of a predictor and the target were used as a loss function in a regression problem. And it could be also written as:\n\\[ \\ell(\\hat{\\mathbf{y}}_i,\\mathbf{y}_i)=(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i)^T(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i) \\tag{1} \\]\nThe linear regression model in a matrix form is:\n\\[ y=\\mathbf{w}^T\\mathbf{x}+\\mathbf{b}\\tag{2} \\]\nWhat we do in this post is analyze the least-squares methods from two different viewpoints","title":"Least Squares Estimation"},{"content":"Preliminaries ‘An Introduction to Backpropagation and Multilayer Perceptrons’ ‘The Backpropagation Algorithm’  Speed Backpropagation up 1 BP algorithm has been described in ‘An Introduction to Backpropagation and Multilayer Perceptrons’. And the implementation of the BP algorithm has been recorded at ‘The Backpropagation Algorithm’. BP has worked in many applications for many years, but there are too many drawbacks in the process. The basic BP algorithm is too slow for most practical applications that it might take days or even weeks in training. And the following posts are some investigations to make the BP algorithm more practical and speed it up.\nIn the post ‘Backpropagation, Batch Training, and Incremental Training’, the BP approximation example had shown that the algorithm converged very slowly. BP is a variation of LMS and LMS is a variation of ‘steepest descent’. So BP is a kind of steepest descent, and the difference between them is the calculation of derivatives. Steepest descent is the simplest and the slowest, while Newton and conjugate algorithms are faster. Then inspiration comes to us whether these algorithms can be used in speeding up the convergence of BP.\nResearch on faster algorithms falls on rough two categories and some aspects would be discussed:\nDevelopment of heuristic techniques  varying learning rate momentum rescaling variables  The standard numerical optimization technique  find a numerical optimization technique already exists. ‘Reinvent the wheel’ is a good and bad idea. Conjugate gradient algorithm Levenberg-Marquardt algorithm   Backpropagation is called ‘back’ because in the calculation of the sensitivities in the hidden layer are calculated by its next layer neurons that have connections with it. And the weights and biases updating process of BP is the same as the steepest descent. So we name the standard backpropagation algorithm steepest descent backpropagation and SDBP for short.\nDrawbacks of Backpropagation LMS guarantee to converge that minimize the MSE under the condition that the learning rate \\(\\alpha\\) is small enough. MSE is a quadratic function that has always a single stationary point and constant Hessian. Because Hessian matrices of quadratic functions do not change, so the curvature of functions would not change and their contours are elliptical.\nWhen BP is used in a layered network, it degenerates to the LMS algorithm. The MSE of the single-layer network is quadratic. So it has a single stationary point and constant curvature.\nBut when the network has more than one layer, its MSE of it is no more quadratic. MSE of multiple layers network has many local minimum points and curvature varies widely in different regions of parameter space. Now let go to look at the surface of MSE of multiple layers network. The simplest network 1-2-1 would be our example and its abbreviated notation is:\nthe transfer functions of both the hidden layer and the output layer are log-sigmoid functions. And in the following experiment the function to approximate has the same architecture as the 1-2-1 network and its parameters are:\n\\[ \\begin{aligned} \\{\u0026amp;\\\\ \u0026amp;w^1_{1,1}=10,b^1_{1}=-5,\\\\ \u0026amp;w^1_{1,2}=10,b^1_{2}=5,\\\\ \u0026amp;w^2_{1,1}=1,w^2_{1,2}=1,b^2=-1\\\\ \\}\u0026amp; \\end{aligned} \\tag{1} \\]\nThen in this task, the global minimum point is at equation(1). And in the interval \\([-2,2]\\) the target function looks like this:\nBecause the target function to be approximated has the same architecture as our model, the 1-2-1 network can have the correct approximation where MSE is 0. Although this experiment is humble compared to practical applications, it can illustrate some important concepts.\nTo approximate the target function, we generate the inputs:\n\\[ \\{-2.0,-1.9,\\cdot,1.9,2.0\\} \\]\nwith equivalent probability and the corresponding outputs. The performance index is the sum of square errors which equals MSE.\nThere are 7 parameters in this simple network model totally. However, we can not observe them all at one picture. We set \\(b^1_{1},w^1_{1,2},b^1_{2},w^2_{1,2},b^2\\) to their optimum values given by equation(1) and leave \\(w^1_{1,1}\\) and \\(w^2_{1,1}\\) as variables. Then we get the contour map:\nThe performance index is not quadratic. Then it has more than one minimum, for instance, \\(w^1_{1,1}=0.88\\) and \\(w^2_{1,1}= 38.6\\), and solution (1) is also a local minimum of the performance index. The curvature varies drastically over the parameter space and a constant learning rate can not suit the entire process. Because some regions are flat where a large learning rate is good, and some regions of curvature are steep where a small learning rate is necessary. The flat region is a common condition when the transfer function of the network is sigmoid. For example, when the inputs are large, the surface of the performance index of the sigmoid network is very flat.\nBecause the 1-2-1 network has a symmetrical architecture, the surface of \\(b^1_1\\) and \\(b^1_2\\) is symmetric as well. and between these two local minimums there must be a saddle point which is at \\(b^1_1=0,b^1_2=0\\):\nSo \\(b^1_1=0\\) and \\(b^1_2=0\\) are not good initial values. And if initial values of the parameters were large, the learning algorithm would start at a very flat region, this is also the nightmare for most algorithms. Trying several initial guesses is also a good idea but when the whole training process needs days or weeks this method is impractical. A common method for initial parameters of networks is using small random numbers as a random number between \\(-1\\) and \\(1\\) with uniform distribution.\nConvergence Example The batching method has been introduced in ‘Backpropagation, Batch Training, and Incremental Training’. It is a generalized method that uses the whole training set of the ‘The Backpropagation Algorithm’, which uses one point of the training set at a time. The following process is based on the batching method.\nNow let’s consider the parameter \\(w^1_{1,1}\\) and the \\(w^2_{1,1}\\) while other parameters are set to optimum solution as in equation(1).\nThe first example is with initial guesses \\(w^1_{1,1}=-4\\) and \\(w^2_{1,1}=-4\\) whose trajectory is labeled as ‘a’:\nIt takes a long time during the flat region and the entire process takes more than 300,000 epochs. And the sum of the square error of the performance index is:\nThe flat region takes a great part of the whole process.\nThe second example is with initial guesses \\(w^1_{1,1}=-4\\) and \\(w^2_{1,1}=10\\) whose trajectory is labeled as ‘b’:\nIt converges to another local minimum point but not the global minimum. And the sum of the squire error of the performance index is:\nwhose final point can not reduce error to 0.\nAs we have mentioned above, in the flat region a bigger learning rate is required. Now let’s start at \\(w^1_{1,1}=-4\\) and \\(w^2_{1,1}=-4\\) as well. But when we increase the learning rate from \\(1\\) to \\(100\\), the algorithm becomes unstable:\nand its error is not decrease after some iterations:\nAfter all the experiments above, we found that flat regions are everywhere then a larger learning rate is required. However, a large learning rate makes the algorithm unstable. What we do next is to make the algorithm stable and fast.\nReferences  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","permalink":"https://anthony-tan.com/Drawbacks-of-Backpropagation/","summary":"Preliminaries ‘An Introduction to Backpropagation and Multilayer Perceptrons’ ‘The Backpropagation Algorithm’  Speed Backpropagation up 1 BP algorithm has been described in ‘An Introduction to Backpropagation and Multilayer Perceptrons’. And the implementation of the BP algorithm has been recorded at ‘The Backpropagation Algorithm’. BP has worked in many applications for many years, but there are too many drawbacks in the process. The basic BP algorithm is too slow for most practical applications that it might take days or even weeks in training.","title":"Drawbacks of Backpropagation"},{"content":"Preliminaries Calculus 1,2 Linear Algebra  Batch v.s. Incremental Training1 In both LMS and BP algorithms, the error in each update process step is not MSE but SE \\(e=t_i-a_i\\) which is calculated just by a data point of the training set. This is called a stochastic gradient descent algorithm. And why it is called ‘stochastic’ is because error at every iterative step is approximated by randomly selected train data points but not the whole data set. It is also called ‘online’ learning when each time step a data point is used and ‘online’ data is always coming to us. And each one can be used independently by the algorithm. So incremental training is also a name for this process.\nWhen we use the whole data set to approximate the error, this is called batch training. This algorithm calculates gradient after all inputs are applied to the network before parameters are updated. For example, when all inputs have equal probability, the mean square error becomes:\n\\[ \\begin{aligned} F(\\mathbf{x})\u0026amp;=\\mathbb E[\\mathbf{e}^T\\mathbf{e}]\\\\ \u0026amp;=\\mathbb E[(\\mathbf{t}-\\mathbf{a})^T(\\mathbf{t}-\\mathbf{a})]\\\\ \u0026amp;=\\frac{1}{Q}\\sum^{Q}_{q=1}(\\mathbf{t}_q-\\mathbf{a}_q)^T(\\mathbf{t}_q-\\mathbf{a}_q) \\end{aligned}\\tag{1} \\]\nRather than changing MSE into SE, this just replaced the MSE with the average of the whole training set error. A statistics professor at MIT said: ‘what our statisticians do every day is replacing expectation with average’. This average is closer to the MSE than SE is. Then the total gradient is:\n\\[ \\begin{aligned} \\nabla F(\\mathbf{x})\u0026amp;=\\nabla\\{\\frac{1}{Q}\\sum^{Q}_{q=1}(\\mathbf{t}_q-\\mathbf{a}_q)^T(\\mathbf{t}_q-\\mathbf{a}_q)\\}\\\\ \u0026amp;=\\frac{1}{Q}\\sum^{Q}_{q=1}\\nabla\\{(\\mathbf{t}_q-\\mathbf{a}_q)^T(\\mathbf{t}_q-\\mathbf{a}_q)\\} \\end{aligned}\\tag{2} \\]\nThen the update step is converted to:\n\\[ W^m(k+1)=W^m(k)-\\frac{\\alpha}{Q}\\sum^{Q}_{q=1}\\mathbf{s}^m_q(\\mathbf{a}^{m-1}_q)^T\\\\ \\mathbf{b}^m(k+1)=\\mathbf{b}^m(k)-\\frac{\\alpha}{Q}\\sum^{Q}_{q=1}\\mathbf{s}^m_q\\cdot 1\\tag{3} \\]\nUsing Backpropagation Building a toy BP program is a good way to go deeper inside the algorithm. The details of the design of the algorithm could be found in ‘The Backpropagation Algorithm’. And the task is consist of three essential parts:\nChoice of network architecture The algorithm used to train to a network would convergent Generalization  Choice of Network Architecture How many layers and how many neurons are necessary for a certain task is the key point in designing a network. For instance, to approximate the target functions\n\\[ g(p)=1+\\sin(\\frac{i\\pi}{4}\\cdot p)\\tag{4} \\]\nwhere for \\(-2\\leq p \\leq 2\\) and \\(i=\\{1,2,4,8\\}\\). And these four different functions look like:\nat the interval of \\([-2,2]\\)\n1-3-1 Neural Network The architecture we used here to approximate four functions is a 1-3-1 net. And the BP algorithm is used.\nThen the process of changing of the curve for \\(g(p)=1+\\sin(\\frac{\\pi}{4}\\cdot p)\\) is like:\nAnd for the \\(g(p)=1+\\sin(\\frac{2\\pi}{4}\\cdot p)\\)\nAnd for the \\(g(p)=1+\\sin(\\frac{4\\pi}{4}\\cdot p)\\) And for the \\(g(p)=1+\\sin(\\frac{8\\pi}{4}\\cdot p)\\) The four final approximate results of these for the function are: Limit of 1-3-1 network has been illustrated above and the capacity of 1-3-1 network can only approximate \\(g(p)=1+\\sin(\\frac{i\\pi}{4}\\cdot p)\\) for \\(i=\\{1,2,3,4\\}\\).\n\\(g(p)=1+\\sin(\\frac{8\\pi}{4}\\cdot p)\\) can not be regressed by 1-3-1 for its flexibility is not enough for the target function. This can also be concluded by the property of these three neurons in the hidden layer whose transfer function is log-sigmoid. Because these three neurons have only three ‘steps’(which has been described in ‘An Introduction to Backpropagation and Multilayer Perceptrons’). These three steps are trained to approximate the three crests of the target functions. So when the target functions have more than 3 crests (including 3 crests), 1-3-1 can not regress the target function accurately.\n1-2-1, 1-3-1, 1-4-1, 1-5-1 networks for \\(g(p)=1+\\sin(\\frac{6\\pi}{4}\\cdot p)\\) The target function \\(g(p)=1+\\sin(\\frac{6\\pi}{4}\\cdot p)\\) has 3 crests at iterval \\([-2,2]\\) and 4 different types of network are used in the approximation:\nThe process of 1-2-1 neuron network for \\(g(p)=1+\\sin(\\frac{6\\pi}{4}\\cdot p)\\) is like:\nThe process of 1-3-1 neuron network for \\(g(p)=1+\\sin(\\frac{6\\pi}{4}\\cdot p)\\) is like:\nThe process of 1-4-1 neuron network for \\(g(p)=1+\\sin(\\frac{6\\pi}{4}\\cdot p)\\) is like:\nThe process of 1-5-1 neuron network for \\(g(p)=1+\\sin(\\frac{6\\pi}{4}\\cdot p)\\) is like:\nAnd the final results of these four networks are:\nSummary of the comparison of the results of the four different networks are:\nthe more neurons in hidden layers the more flexible the entire network is if the flexibility of the network is not sufficient for the target function, it is not a good model for the task. although the flexibility of the network is sufficient, the training algorithm may also not be able to converge to the global minimum  Convergence Analysis When the training algorithm did not converge to the global minimum, the responses of the network can not give an accurate approximation to the desired function. This is just because the BP algorithm used here is not like LMS it worked under the condition that the performance index function is quadratic and it has only had one minimum. The performance index of multiple layers network has a lot of local minimum and saddle points also affect the convergence of the algorithm.\nBP can not guarantee convergence to the global minimum. Many factors can affect the process. And now let’s observe the different initial values of the parameters of the network which lead to different local minimums of the performance index:\nConvergent to a local minimum Initial values: |layer|neuron|initial weights and bias| |:—:|:—:|:—:| |2|1|\\([0.42965179 -0.07152415]\\)| |2|2|\\([0.16361572 0.79774829]\\)| |2|3|\\([0.73702272 0.75144977]\\)| |3|1|\\([0.76338542 0.95722099 -0.11531554 0.20356626]\\)|\nand the process of the algorithm is:\nthe descent process of MSE is:\nand the final converged parameters:\n  layer neuron initial weights and bias    2 1 \\([0.93900718 -0.01797782]\\)  2 2 \\([-4.82660011 4.52635542]\\)  2 3 \\([4.80297794 4.51769764]\\)  3 1 \\([13.6902581 5.32639145 -5.37517373 -5.8557322]\\)    Convergent to another local minimum Initial values: |layer|neuron|initial weights and bias| |:—:|:—:|:—:| |2|1|\\([-18.61882866 12.96283924]\\)| |2|2|\\([ 11.35841636 -20.15384594]\\)| |2|3|\\([ 2.77601854 22.87956077]\\)| |3|1|\\([-44.15374244 -58.65710547 -34.40432363 -78.4400726 ]\\)|\nand the process of the algorithm is:\nthe descent process of MSE is:\nand the final converged parameters:\n  layer neuron initial weights and bias    2 1 \\([-19.84672298 -21.74627601]\\)  2 2 \\([-31.5629279 -67.20354561]\\)  2 3 \\([2.77601496 22.8795743 ]\\)  3 1 \\([0.98462573 -47.6630436 22.41340957 -21.62233846]\\)    These two examples imply that the initial values of parameters primarily affect the local minimum the algorithm would converge to. not only the initial values but also other parameters of the learning algorithm would affect the final results.\nGeneralization For we have only a finite number of training samples(examples of proper network behavior) which means our task is approximating the function that has more input/output pairs than we used in training. And the behavior of the model to the inputs which were not used in the training process is called generalization. For instance, the target function is:\n\\[ g(p)=1+\\sin(\\frac{\\pi}{4}\\cdot p)\\tag{5} \\]\nand the training set are the inputs \\(p=-2.0,-1.6,\\cdots,1.6,2.0\\) and their corresponding outputs. These 11 pairs are used in training the following the two networks:\n1-2-1 1-2-1 gives a good generalization that is when the points are not used in the train, the blue line is also close to the red line:\n1-9-1 However, a more powerful network, a 1-9-1 network gives a closer approximation than the 1-2-1 network at training points that the blue circles are close to red circles. But at other points, which are not used in training, represented by the blue line is far from the ground truth. This means 1-9-1 gives a bad generalization:\nThis is called ‘overfitting’.\nthe summary of these two experiments is: 1. usually, parameters in the model should be less than the number of points in the training set(this can also be described as the number of training data should be more than the parameter in the model) 2. Ockham’s Razor is a good rule in future work of neuron network design: when the smaller networks could work, a bigger network is not necessary.\nReferences  Demuth, Howard B., Mark H. Beale, Orlando De Jess, and Martin T. Hagan. Neural network design. Martin Hagan, 2014.↩︎\n   ","permalink":"https://anthony-tan.com/Backpropagation-Batch-Training-and-Incremental-Training/","summary":"Preliminaries Calculus 1,2 Linear Algebra  Batch v.s. Incremental Training1 In both LMS and BP algorithms, the error in each update process step is not MSE but SE \\(e=t_i-a_i\\) which is calculated just by a data point of the training set. This is called a stochastic gradient descent algorithm. And why it is called ‘stochastic’ is because error at every iterative step is approximated by randomly selected train data points but not the whole data set.","title":"Backpropagation, Batch Training, and Incremental Training"},{"content":"Preliminaries An Introduction to Backpropagation and Multilayer Perceptrons Culculus 1,2 Linear algebra Jacobian matrix  Architecture and Notations1 We have seen a three-layer network is flexible in approximating functions(An Introduction to Backpropagation and Multilayer Perceptrons). If we had a more-than-three-layer network, it could be used to approximate any functions as accurately as we want. However, another trouble that came to us is the learning rules. This problem almost killed neural networks in the 1970s. Until the backpropagation(BP for short) algorithm was found that it is an efficient algorithm in training multiple layers networks.\nA 3-layer network is also used in this post for it is the simplest multiple-layer network whose abbreviated notation is:\nand a more short way to represent its architecture is:\n\\[ R - S^1 - S^2 - S^3 \\tag{1} \\]\nFor the three-layer network has only three layers that are not too large to denote mathematically, then it can be written as:\n\\[ \\mathbf{a}=f^3(W^3\\cdot f^2(W^2\\cdot f^1(W^1\\cdot \\mathbf{p}+\\mathbf{b}^1)+\\mathbf{b}^2)+\\mathbf{b}^3)\\tag{2} \\]\nHowever, this mathematical equation is too complex to construct when we have a 10-layer network or a 100-layer network. Then we can use some short equations that describe the whole operation of the \\(M\\)-layer network:\n\\[ a^{m+1}=f^{m+1}(W^{m+1}\\mathbf{a}^{m}+\\mathbf{b}^{m+1})\\tag{3} \\]\nfor \\(m = 1, 2, 3, \\cdots M-1\\). \\(M\\) is the number of layers in the neural networks. And: - \\(\\mathbf{a}^0=\\mathbf{p}\\) is its input - \\(\\mathbf{a}=\\mathbf{a}^M\\) is its output\nPerformance Index We have had a network now. Then we need to definite a performance index for this 3-layer network.\nMSE is used here as the performance index the same as what the LMS algorithm did in post ‘Widrow-Hoff Learning’. And the training set is:\n\\[ \\{\\mathbf{p}_1,\\mathbf{t}_1\\},\\{\\mathbf{p}_2,\\mathbf{t}_2\\},\\cdots \\{\\mathbf{p}_Q,\\mathbf{t}_Q\\}\\tag{4} \\]\nwhere \\(\\mathbf{p}_i\\) is the input and \\(\\mathbf{t}_i\\) is the corresponding output(target).\nBP is the generation of LMS algorithms, and both of them try to minimize the mean square error. And what we finally get is a trained neural network that fits the training set. But this model may not be guaranteed to fit the original task where the training set is generated. So a good training set that can represent the original task accurately is necessary.\nTo make it easier to understand from the steepest descent algorithm to LMS and BP, we convert the weights and bias in the neural network form \\(w\\) and \\(b\\) into a vector \\(\\mathbf{x}\\). Then the performance index is:\n\\[ F(\\mathbf{x})=\\mathbb E[e^2]=\\mathbb E[(t-a)^2]\\tag{5} \\]\nWhen the network has multiple outputs this generalizes to:\n\\[ F(\\mathbf{x})=\\mathbb E[\\mathbf{e}^T\\mathbf{e}]=\\mathbb E[(\\mathbf{t}-\\mathbf{a})^T(\\mathbf{t}-\\mathbf{a})]\\tag{6} \\]\nDuring an iteration, in the LMS algorithm, MSE(mean square error) is approximated by SE(square error):\n\\[ \\hat{F}(\\mathbf{x})=(\\mathbf{t}-\\mathbf{a})^T(\\mathbf{t}-\\mathbf{a})=\\mathbf{e}^T\\mathbf{e}\\tag{7} \\]\nwhere the expectations are replaced by the calculation of current input, output, and target.\nReviewing the ‘steepest descent algorithm’, the gradient descent algorithm of approximate MSE is also called stochastic gradient descent:\n\\[ \\begin{aligned} w^m_{i,j}(k+1)\u0026amp;=w^m_{i,j}(k)-\\alpha \\frac{\\partial \\hat{F}}{\\partial w^m_{i,j}}\\\\ b^m_{i}(k+1)\u0026amp;=b^m_{i}(k)-\\alpha \\frac{\\partial \\hat{F}}{\\partial b^m_{i}} \\end{aligned}\\tag{8} \\]\nwhere \\(\\alpha\\) is the learning rate.\nHowever, the steep descent algorithm seems can not work on a multiple-layer network for we can not calculate the partial derivative in the hidden layer and input layer directly.\nWe were inspired by another mathematical tool - the chain rule.\nThe Chain Rule Calculus when \\(f\\) is explicit function of \\(\\mathbf{n}\\) and \\(\\mathbf{n}\\) is a explicit function of \\(\\mathbf{w}\\), we can calculate the partial derivative \\(\\frac{\\partial f}{\\partial w}\\) by:\n\\[ \\frac{\\partial f}{\\partial w}=\\frac{\\partial f}{\\partial n}\\frac{\\partial n}{\\partial w}\\tag{9} \\]\nThe whole process looks like a chain. And let’s look at a simple example: when we have \\(f(n)=e^n\\) and \\(n=2w\\), we have \\(f(n(w))=e^{2w}\\). We can easily calculate the direvative \\(\\frac{\\partial f}{\\partial w}=\\frac{\\partial e^2w}{\\partial w}=2e^{2w}\\). And when chain rule is used, we have:\n\\[ \\frac{\\partial f(n(w))}{\\partial w}=\\frac{\\partial e^n}{\\partial n}\\frac{\\partial n}{\\partial w}=\\frac{\\partial e^n}{\\partial n}\\frac{\\partial 2w}{\\partial w}=e^n\\cdot 2=2e^{2w}\\tag{10} \\]\nthat is the same as what we get directly.\nWhen the chain rule is used in the second part on the right of equation (8), we get the way to calculate the derivative of the weight of hidden layers:\n\\[ \\begin{aligned} \\frac{\\partial \\hat{F}}{\\partial w^m_{i,j}}\u0026amp;=\\frac{\\partial \\hat{F}}{\\partial n^m_i}\\cdot \\frac{\\partial n^m_i}{\\partial w^m_{i,j}}\\\\ \\frac{\\partial \\hat{F}}{\\partial b^m_{i}}\u0026amp;=\\frac{\\partial \\hat{F}}{\\partial n^m_i}\\cdot \\frac{\\partial n^m_i}{\\partial b^m_{i}} \\end{aligned}\\tag{11} \\]\nfrom the abbreviated notation, we know that \\(n^m_i=\\sum^{S^{m-1}}_{j=1}w^m_{i,j}a^{m-1}_{j}+b^m_i\\). Then equation (11) can be writen as:\n\\[ \\begin{aligned} \\frac{\\partial \\hat{F}}{\\partial w^m_{i,j}}\u0026amp;=\\frac{\\partial \\hat{F}}{\\partial n^m_i}\\cdot \\frac{\\partial \\sum^{S^{m-1}}_{j=1}w^m_{i,j}a^{m-1}_{j}+b^m_i}{\\partial w^m_{i,j}}=\\frac{\\partial \\hat{F}}{\\partial n^m_i}\\cdot a^{m-1}_j\\\\ \\frac{\\partial \\hat{F}}{\\partial b^m_{i}}\u0026amp;=\\frac{\\partial \\hat{F}}{\\partial n^m_i}\\cdot \\frac{\\partial \\sum^{S^{m-1}}_{j=1}w^m_{i,j}a^{m-1}_{j}+b^m_i}{\\partial b^m_{i}}=\\frac{\\partial \\hat{F}}{\\partial n^m_i}\\cdot 1 \\end{aligned}\\tag{12} \\]\nEquation (12) could also be simplified by defining a new concept: sensitivity.\nSensitivity We define sensitivity as \\(s^m_i\\equiv \\frac{\\partial \\hat{F}}{\\partial n^m_{i}}\\) that means the sensitivity of \\(\\hat{F}\\) to changes in the \\(i^{\\text{th}}\\) element of the net input at layer \\(m\\). Then equation (12) can be simplified as:\n\\[ \\begin{aligned} \\frac{\\partial \\hat{F}}{\\partial w^m_{i,j}}\u0026amp;=s^m_{i}\\cdot a^{m-1}_j\\\\ \\frac{\\partial \\hat{F}}{\\partial b^m_{i}}\u0026amp;=s^m_{i}\\cdot 1 \\end{aligned}\\tag{13} \\]\nThen the steepest descent algorithm is: \\[ \\begin{aligned} w^m_{i,j}(k+1)\u0026amp;=w^m_{i,j}(k)-\\alpha s^m_{i}\\cdot a^{m-1}_j\\\\ b^m_{i}(k+1)\u0026amp;=b^m_{i}(k)-\\alpha s^m_{i}\\cdot 1 \\end{aligned}\\tag{14} \\]\nThis can also be written in a matrix form:\n\\[ \\begin{aligned} W^m(k+1)\u0026amp;=W^m(k)-\\alpha \\mathbf{s}^m(\\mathbf{a}^{m-1})^T\\\\ \\mathbf{b}^m(k+1)\u0026amp;=\\mathbf{b}^m(k)-\\alpha \\mathbf{s}^m\\cdot 1 \\end{aligned}\\tag{15} \\]\nwhere: \\[ \\mathbf{s}^m=\\frac{\\partial \\hat{F}}{\\alpha \\mathbf{n}^m}=\\begin{bmatrix} \\frac{\\partial \\hat{F}}{\\partial n^m_1}\\\\ \\frac{\\partial \\hat{F}}{\\partial n^m_2}\\\\ \\vdots\\\\ \\frac{\\partial \\hat{F}}{\\partial n^m_{S^m}}\\\\ \\end{bmatrix}\\tag{16} \\]\nAnd be careful of the \\(\\mathbf{s}\\) which means the sensitivity and \\(S^m\\) which means the number of layers \\(m\\)\nBackpropagating the Sensitivities Equation (15) is our BP algorithm. But we can not calculate sensitivities yet. We can easily calculate the sensitivities of the last layer which is the same as LMS. And we have an inspiration that is we can use the relation between the latter layer and the current layer. So let’s observe the Jacobian matrix which represents the relation between the latter layer linear combination output \\(\\mathbf{n}^{m+1}\\) and the current layer linear combination output \\(\\mathbf{n}^m\\):\n\\[ \\frac{\\partial \\mathbf{n}^{m+1}}{\\partial \\mathbf{n}^{m}}= \\begin{bmatrix} \\frac{ \\partial n^{m+1}_1}{\\partial n^{m}_1} \u0026amp; \\frac{\\partial n^{m+1}_1}{\\partial n^{m}_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial n^{m+1}_1}{\\partial n^{m}_{S^m}}\\\\ \\frac{\\partial n^{m+1}_2}{\\partial n^{m}_1} \u0026amp; \\frac{\\partial n^{m+1}_2}{\\partial n^{m}_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial n^{m+1}_2}{\\partial n^{m}_{S^m}}\\\\ \\vdots\u0026amp;\\vdots\u0026amp;\u0026amp;\\vdots\\\\ \\frac{\\partial n^{m+1}_{S^{m+1}}}{\\partial n^{m}_1} \u0026amp; \\frac{\\partial n^{m+1}_{S^{m+1}}}{\\partial n^{m}_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial n^{m+1}_{S^{m+1}}}{\\partial n^{m}_{S^m}}\\\\ \\end{bmatrix}\\tag{17} \\]\nAnd the \\((i,j)^{\\text{th}}\\) element of the matrix is:\n\\[ \\begin{aligned} \\frac{\\partial n^{m+1}_i}{\\partial n^{m}_j}\u0026amp;=\\frac{\\partial (\\sum^{S^m}_{l=1}w^{m+1}_{i,l}a^m_l+b^{m+1}_i)}{\\partial n^m_j}\\\\ \u0026amp;= w^{m+1}_{i,j}\\frac{\\partial a^m_j}{\\partial n^m_j}\\\\ \u0026amp;= w^{m+1}_{i,j}\\frac{\\partial f^m(n^m_j)}{\\partial n^m_j}\\\\ \u0026amp;= w^{m+1}_{i,j}\\dot{f}^m(n^m_j) \\end{aligned}\\tag{18} \\]\nwhere \\(\\sum^{S^m}_{l=1}w^{m+1}_{i,l}a^m_l+b^{m+1}_i\\) is the linear combination output of layer \\(m+1\\) and \\(a^m\\) is the output of layer \\(m\\). And we can define \\(\\dot{f}^m(n^m_j)=\\frac{\\partial f^m(n^m_j)}{\\partial n^m_j}\\)\nTherefore the Jacobian matrix can be written as:\n\\[ \\begin{aligned} \u0026amp;\\frac{\\partial \\mathbf{n}^{m+1}}{\\partial \\mathbf{n}^{m}}\\\\ =\u0026amp;W^{m+1}\\dot{F}^m(\\mathbf{n}^m)\\\\ =\u0026amp;\\begin{bmatrix} w^{m+1}_{1,1}\\dot{f}^m(n^m_1) \u0026amp; w^{m+1}_{1,2}\\dot{f}^m(n^m_2) \u0026amp; \\cdots \u0026amp; w^{m+1}_{1,{S^m}}\\dot{f}^m(n^m_{S^m})\\\\ w^{m+1}_{2,1}\\dot{f}^m(n^m_1) \u0026amp; w^{m+1}_{2,2}\\dot{f}^m(n^m_2) \u0026amp; \\cdots \u0026amp; w^{m+1}_{2,{S^m}}\\dot{f}^m(n^m_{S^m})\\\\ \\vdots\u0026amp;\\vdots\u0026amp;\u0026amp;\\vdots\\\\ w^{m+1}_{S^{m+1},1}\\dot{f}^m(n^m_1) \u0026amp; w^{m+1}_{S^{m+1},2}\\dot{f}^m(n^m_2) \u0026amp; \\cdots \u0026amp; w^{m+1}_{S^{m+1},{S^m}}\\dot{f}^m(n^m_{S^m}) \\end{bmatrix} \\end{aligned} \\tag{19} \\]\nwhere we have:\n\\[ \\dot{F}^m(\\mathbf{n}^m)= \\begin{bmatrix} \\dot{f}(n^m_1)\u0026amp;0\u0026amp;\\cdots\u0026amp;0\\\\ 0\u0026amp;\\dot{f}(n^m_2)\u0026amp;\\cdots\u0026amp;0\\\\ \\vdots\u0026amp;\\vdots\u0026amp;\\ddots\u0026amp;\\vdots\\\\ 0\u0026amp;0\u0026amp;\\cdots\u0026amp;\\dot{f}(n^m_{S^m}) \\end{bmatrix}\\tag{20} \\]\nThen recurrence relation for the sensitivity by using the chain rule in matrix form is:\n\\[ \\begin{aligned} \\mathbf{s}^m\u0026amp;=\\frac{\\partial \\hat{F}}{\\partial n^m}\\\\ \u0026amp;=(\\frac{\\partial \\mathbf{n}^{m+1}}{\\partial \\mathbf{n}^{m}})^T\\cdot \\frac{\\partial \\hat{F}}{\\partial n^{m+1}}\\\\ \u0026amp;=\\dot{F}^m(\\mathbf{n}^m)W^{m+1}\\mathbf{s}^{m+1}\\\\ \\end{aligned}\\tag{21} \\]\nThis is why it is called backpropagation because the sensitivities of layer \\(m\\) are calculated by layer \\(m+1\\) :\n\\[ S^{M}\\to S^{M-1}\\to S^{M-2}\\to \\cdots \\to S^{1}\\tag{22} \\]\nSame to the LMS algorithm, BP is also an approximating algorithm of the steepest descent technique. And the start of BP \\(\\mathbf{s}^M_i\\) is:\n\\[ \\begin{aligned} \\mathbf{s}^M_i\u0026amp;=\\frac{\\partial \\hat{F}}{\\partial n^m_i}\\\\ \u0026amp;=\\frac{\\partial (\\mathbf{t}-\\mathbf{a})^T(\\mathbf{t}-\\mathbf{a})}{\\partial n^m_i}\\\\ \u0026amp;=\\frac{\\partial \\sum_{j=1}^{S^M}(t_j-a_j)^2}{\\partial n^M_i}\\\\ \u0026amp;=-2(t_i-a_i)\\frac{\\partial a_i}{\\partial n^M_{i}} \\end{aligned}\\tag{23} \\]\nand this is easy to understand because it is just a variation of the LMS algorithm. Since\n\\[ \\frac{\\partial a_i}{\\partial n^M_i}=\\frac{\\partial f^M(n^M_i)}{\\partial n^M_i}=\\dot{f}^M(n^M_j)\\tag{24} \\]\nwe can write:\n\\[ s^M_i=-2(t_i-a_i)\\dot{f}^M(n^M_i)\\tag{25} \\]\nand its matrix form is:\n\\[ \\mathbf{s}^M_i=-\\dot{F}^M(\\mathbf{n}^M)(\\mathbf{t}-\\mathbf{a})\\tag{26} \\]\nSummary of BP Propagate the input forward through the network  \\(\\mathbf{a}^0=\\mathbf{p}\\) \\(\\mathbf{a}^{m+1}=f^{m+1}(W^{m+1}\\mathbf{a}^m+\\mathbf{b}^{m+1})\\) for \\(m=0,1,2,\\cdots, M-1\\) \\(\\mathbf{a}=\\mathbf{a}^M\\)  Propagate the sensitivities backward through the network:  \\(\\mathbf{s}^M=-2\\dot{F}^M(\\mathbf{n}^M)(\\mathbf{t}-\\mathbf{a})\\) \\(\\mathbf{s}^m= \\dot{F}^m(\\mathbf{n}^m)(W^{m+1})^T\\mathbf{s}^{m+1})\\) for \\(m=M-1,\\cdots,2,1\\)  Finally, the weights and bias are updated using the approximate steepest descent rule:  \\(W^{m}(k+1)=W^{m}(k)-\\alpha \\mathbf{s}^m(\\mathbf{a}^{m-1})^T\\) \\(\\mathbf{b}^{m}(k+1)=\\mathbf{b}^{m}(k)-\\alpha \\mathbf{s}^m\\)   References  Demuth, Howard B., Mark H. Beale, Orlando De Jess, and Martin T. Hagan. Neural network design. Martin Hagan, 2014.↩︎\n   ","permalink":"https://anthony-tan.com/The-Backpropagation-Algorithm/","summary":"Preliminaries An Introduction to Backpropagation and Multilayer Perceptrons Culculus 1,2 Linear algebra Jacobian matrix  Architecture and Notations1 We have seen a three-layer network is flexible in approximating functions(An Introduction to Backpropagation and Multilayer Perceptrons). If we had a more-than-three-layer network, it could be used to approximate any functions as accurately as we want. However, another trouble that came to us is the learning rules. This problem almost killed neural networks in the 1970s.","title":"The Backpropagation Algorithm"},{"content":"Preliminaries Performance learning Perceptron learning rule Supervised Hebbian learning LMS  Form LMS to Backpropagation1 The LMS algorithm is a kind of ‘performance learning’. And we have studied several learning rules(algorithms) till now, such as ‘Perceptron learning rule’ and ‘Supervised Hebbian learning’. And they were based on the idea of the physical mechanism of biological neuron networks.\nThen performance learning was represented. Because of its outstanding performance, we go further and further away from natural intelligence into performance learning.\nLMS can only solve the classification task which is linear separable. And then backpropagation(BP for short) which is a generalization of the LMS algorithm was introduced for more complex problems. And backpropagation is also an approximation of the steepest descent algorithm. The performance index of the problem which was supposed to be solved by backpropagation was MSE.\nThe distinction between BP and LMS is how derivative is calculated: 1. 1-layer network: \\(\\frac{\\partial e}{\\partial w}\\) is relatively easy to compute. 2. multiple-layer network: \\(\\frac{\\partial e}{\\partial w_{i,j}}\\) is complex. The chain rule would be employed to deal with the multiple-layer network with nonlinear transfer functions.\nBrief History of BP Rosenblatt and Widrow found the disadvantage of a single-layer network is that it can only solve the linear separable tasks. So they brought up the multilayer network. However, they had not developed an efficient learning rule to train a multilayer network.\nIn 1974, the first procedure of training a multilayer network was introduced by Paul Werbos in his thesis. However, this thesis was not noticed by researchers. In 1985 and 1986, David Parker, Yann LeCun, and Geoffry Hilton proposed the BP algorithm respectively. And in 1986, the book David R. and James M. ‘Parallel Distributed Processing’ made the algorithm known widely.\nIn the following several posts we would like to investigate: 1. The capacity of the multilayer network 2. BP algorithm\nMultilayer Perceptrons Let’s consider the 3-layer network:\nwhose output is:\n\\[ \\mathbf{a}^3=\\mathbf{f}^3(W^3\\mathbf{f}^2(W^2\\mathbf{f}^1(W^1\\mathbf{p}+\\mathbf{b}^1)+\\mathbf{b}^2)+\\mathbf{b}^3) \\]\nand because all the outputs of one layer are inputs to the next layer and this makes it possible that the network can be notated as:\nwhere \\(R\\) representes number of input and \\(S^i\\) for \\(i=1,2,3\\) is the number of neurons of layer 1,2,3.\nWe have now had the architecture of the new model multilayer network. What we should do next is to investigate the capacity of the multilayer network in:\nPattern classification Function Approximation  Pattern Classification Firstly, let’s have a look at a famous logical problem ‘exclusive-or’ or ‘XOR’ for short. This problem was famous for it can not be solved by a single-layer network which is proposed by Minsky and Papert in 1969.\nA Multilayer network was then invented to solve the ‘XOR’ problem. The input/output pairs of ‘XOR’ are\n\\[ \\{\\mathbf{p}_1=\\begin{bmatrix} 0\\\\0 \\end{bmatrix},\\mathbf{t}_1=0\\}\\\\ \\{\\mathbf{p}_2=\\begin{bmatrix} 0\\\\1 \\end{bmatrix},\\mathbf{t}_2=1\\}\\\\ \\{\\mathbf{p}_3=\\begin{bmatrix} 1\\\\0 \\end{bmatrix},\\mathbf{t}_3=1\\}\\\\ \\{\\mathbf{p}_4=\\begin{bmatrix} 1\\\\1 \\end{bmatrix},\\mathbf{t}_4=0\\} \\]\nand these points are not linear separable:\nand if we use a 2-layer network, the XOR problem can be solved:\nwhere these two lines can be constructed by two neurons:\nThe blue line can be \\(y=-x+0.5\\) and its neuron model is:  The green line can be \\(y=-x+1.5\\) and its neuron model is:   And these two lines(neurons) can be mixed and constructed into a 2-layer network(2-2-1 network):\nThis gave a solution to the non-linear separable problem ‘XOR’. However, this is not a learning rule which means it could not be generalized to other more complex problems.\nFunction Approximation Besides classification, another task of the neuron network is function approximation. If we consider the intelligence of a model as an intricate function, the ability of the neuron network in approximating function should be studied. This capacity is also known as the model’s flexibility. Now let’s discuss the flexibility of a multilayer perceptron for implementing functions. A simple example is a good way to look inside of properties of a model without unnecessary details. So a ‘1-2-1’ network whose transfer function is logic-sigmoid in the first layer and linear function in the second layer is introduced:\nwhen \\(w^1_{1,1}=10\\), \\(w^1_{2,1}=10\\), \\(b^1_1=-10\\), \\(b^1_2=10\\), \\(w^2_{1,1}=1\\), \\(w^2_{2,1}=1\\), \\(b^2=0\\). It looks like:\nEach step can be changed by changing parameters. Because the steps are centered at\n\\(n^1_1=0\\) at \\(p=1\\) \\(n^1_2=0\\) at \\(p=-1\\)  and steps can be changed by changing weights. When \\(w^1_{1,1}=20\\), \\(w^1_{2,1}=20\\), \\(b^1_1=-10\\), \\(b^1_2=10\\), \\(w^2_{1,1}=1\\), \\(w^2_{2,1}=1\\), \\(b^2=0\\). It looks like the gray line in the figure:\nNow let’s have a look at how the curve of neuron networks looks like when one of the parameters is changing.\nReferences  Demuth, Howard B., Mark H. Beale, Orlando De Jess, and Martin T. Hagan. Neural network design. Martin Hagan, 2014.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Backpropagation-and-Multilayer-Perceptrons/","summary":"Preliminaries Performance learning Perceptron learning rule Supervised Hebbian learning LMS  Form LMS to Backpropagation1 The LMS algorithm is a kind of ‘performance learning’. And we have studied several learning rules(algorithms) till now, such as ‘Perceptron learning rule’ and ‘Supervised Hebbian learning’. And they were based on the idea of the physical mechanism of biological neuron networks.\nThen performance learning was represented. Because of its outstanding performance, we go further and further away from natural intelligence into performance learning.","title":"An Introduction to Backpropagation and Multilayer Perceptrons"},{"content":"Preliminaries ‘Performance Surfaces and Optimum Points’ Linear algebra stochastic approximation Probability Theory  ADALINE, LMS, and Widrow-Hoff learning1 Performance learning had been discussed. But we have not used it in any neural network. In this post, we talk about an important application of performance learning. And this new neural network was invented by Frank Widrow and his graduate student Marcian Hoff in 1960. It was almost the same time as Perceptron was developed which had been discussed in ‘Perceptron Learning Rule’.\nIt is called Widrow-Hoff Learning. This is an approximate steepest descent algorithm. And the performance index used in the learning rule is mean square error.\nPerceptron was discussed because it is still used in the current tasks. And it’s a kind of basic block of a current neural network as well. However, Widrow-Hoff learning was supposed to be discussed because it is:\nwidely used in many signal processing and precursor to the backpropagation algorithm which is a very important tool of current deep learning research.  ADALINE (Adaptive LInear NEuron) and a learning rule called LMS(Least Mean Square) algorithm were introduced in the paper ‘Adaptive switching circuits’. The only distinction between perceptron and ADALINE is only the transfer function which in perceptron is a hard-limiting but in ADALINE is linear. And they have the same inherent limitation: they can only deal with the linear separable problem.\nAnd the learning rule LMS algorithm is more powerful than the perceptron learning rule. The perceptron learning algorithm always gives a decision boundary through a training point(a sample of the training set)or near a training point as we have talked about in ‘Perceptron Learning Rule’. So this classification is not strong enough, and LMS can fix this problem. LMS had great success in signal processing but it did not work well in adapting the algorithm to the multilayer network. Backpropagation is a descendant of the LMS algorithm.\nADALINE Network It’s an abbreviated notation of ADALINE network is;\nIt can be notated mathematically as: \\[ \\begin{aligned} \\mathbf{a}\u0026amp;=\\text{pureline}(W\\mathbf{p} + \\mathbf{b})\\\\ \u0026amp;= W\\mathbf{p}+\\mathbf{b} \\end{aligned}\\tag{1} \\]\nThis is already a simple model, but we would like to use a more simplified model: a neuron with 2-inputs. Its abbreviated notation is:\nwhose mathematical notation is:\n\\[ \\begin{aligned} \\mathbf{a}\u0026amp;= W\\mathbf{p}+\\mathbf{b}\\\\ \u0026amp;=w_{1,1}p_1+w_{1,2}p_2+b \\end{aligned}\\tag{2} \\]\nand decision boundary of this 2-input neuron is:\n\\(\\mathbf{w}\\) is the vector consisting of the weights in neuron. And it always point to the region where \\(w_{1,1}p_1+w_{1,2}p_2+b\u0026gt;0\\)\nThis is the simplest ADALINE neural network architecture, and then we would like to investigate ‘how to modify its parameter’ - its learning rule.\nMean Square Error LMS is a kind of supervised learning algorithm and it needs a training set:\n\\[ \\{\\mathbf{p}_1,t_1\\},\\{\\mathbf{p}_2,t_2\\},\\cdots,\\{\\mathbf{p}_Q,t_Q\\}\\tag{3} \\]\nAnd we used the information of the interaction between the output of the neural network of a certain input \\(\\mathbf{p}_i\\) and its corresponding target \\(t_i\\)\nThe information used in LMS is the mean square error(MSE), which is the difference between output and target. This is the performance index of the ADALINE neural network.\nTo make the calculating process more beautiful, we lump the parameters up including bias into:\n\\[ \\mathbf{x}=\\begin{bmatrix} _1\\mathbf{w}\\\\ b \\end{bmatrix}\\tag{4} \\]\nand input lump up respectively:\n\\[ \\mathbf{z}=\\begin{bmatrix} \\mathbf{p}\\\\ 1 \\end{bmatrix}\\tag{5} \\]\n\\(\\mathbf{x}\\) is used to refer to as a parameter to make it successive with the posts about performance optimization(‘Performance Surfaces and Optimum Points’) and equation(1) became:\n\\[ a=W\\mathbf{p}+\\mathbf{b}=\\mathbf{x}^T\\mathbf{z}\\tag{6} \\]\nExpression for the ADALINE network mean square error:\n\\[ F(x)=\\mathbb {E}[e^2]=\\mathbb {E}[(t-a)^2]=\\mathbb {E} [(t-\\mathbf{x}^T\\mathbf{z})^2]\\tag{7} \\]\nwhere the symbol \\(\\mathbb{E}\\) is the expectation(or average of a certain training set). Average is also called mean and the mean of the square of error so it is called MSE.\n\\[ \\mathbb{E}[(t-\\mathbf{x}^T\\mathbf{z})^2]=\\mathbb{E}[t^2]-2\\mathbf{x}^T\\mathbb{E}[t\\mathbf{z}]+\\mathbf{x}^T\\mathbb{E}[\\mathbf{z}^T\\mathbf{z}]\\mathbf{x}\\tag{8} \\]\nwhere \\(\\mathbf{x}\\) and \\(t\\) is not a random variable so its expectation is itself. And equation(8) can be simplified as\n\\[ F(\\mathbf{x})=C-2\\mathbf{x}^T\\mathbf{h}+\\mathbf{x}^TR\\mathbf{x}\\tag{9} \\]\nwhere:\n \\(\\mathbf{h}=t\\mathbf{z}\\) in statistical view, this is a cross-correlation between input and output. \\(R=\\mathbb{E}[\\mathbf{z}\\mathbf{z}^T]\\) is input correlation matrix whose diagonal iters are mean square value of input. \\(C=\\mathbb{E}[t^2]\\) is a constant.  equation(9) is a ‘quadratic function’. and we can rewrite it in the form:\n\\[ F(\\mathbf{x})=C+ \\mathbf{d}^T\\mathbf{x}+\\frac{1}{2}\\mathbf{x}^TA\\mathbf{x}\\tag{10} \\]\nwhere:\n \\(C=C\\) \\(\\mathbf{d}^T=2\\mathbf{h}^T\\) \\(A=2R=2\\mathbb{E}[\\mathbf{z}\\mathbf{z}^T]\\) is positive definite or positive semidefinite matrix. So this means the square error function could have: a strong minimum if \\(R\\) is positive definite and could also have a weak minimum or no minimum if \\(R\\) is positive semidefinite.   Then the stationary point could be at the point: \\[ \\nabla F(\\mathbf{x})=-2\\mathbf{h}+2R\\mathbf{x}=0\\tag{11} \\]\nAnd finally, we get the minimum point of MSE:\n\\[ \\mathbf{x}^{\\star}=R^{-1}\\mathbf{h}\\tag{12} \\]\nbecause \\(R\\) is comprised of inputs, so inputs decides the matrix \\(R\\) and \\(R\\) decides the minimum of MSE \\(\\mathbf{x}^{\\star}\\). This means when we have no information on the Hessian matrix of the performance index, our inputs are used to comprise the approximation of the Hessian matrix.\nLMS Algorithm LMS is the short for least mean square. And it is the algorithm for searching the minimum of the performance index.\nWhen \\(\\mathbf{h}\\) and \\(R\\) are known, stationary points can be found directly. If \\(R^{-1}\\) is impossible to calculate we can use ‘steepest descent algorithm’. However, in common both \\(\\mathbf{h}\\) and \\(R\\) are unknown or are not convenient to be calculated. We would use the approximate steepest descent in which we use an estimated gradient.\nAnd we also instead expectation of square error with a squared error:\n\\[ \\hat{F}(\\mathbf{x})=(t(k)-a(k))^2=e^2(k)\\tag{13} \\]\nand the approximation of gradient is:\n\\[ \\hat{\\nabla}F=\\nabla e^2(k)\\tag{14} \\]\nThis is the key point of the algorithm and it is also known as ‘stochastic gradient’. And when this approximation is used in a gradient descent algorithm it is referred to as\n on-line learning incremental learning  which means parameters are updated as each input reaches.\nIn the \\(\\nabla e^2(k)\\), the first \\(R\\) components are the parts decided by the weights. So we have:\n\\[ [\\nabla e^2(k)]_j=\\frac{\\partial e^2(k)}{\\partial w_{1,j}}=2 e(k)\\frac{\\partial e(k)}{\\partial w_{1,j}}\\tag{15} \\]\nfor \\(j=1,2,\\cdots, R\\) and similarly\n\\[ [\\nabla e^2(k)]_{j+1}=\\frac{\\partial e^2(k)}{\\partial b}=2 e(k)\\frac{\\partial e(k)}{b}\\tag{16} \\]\nnow we consider \\(\\frac{\\partial e(k)}{\\partial w_{1,j}}\\) whose error is the difference between the output of ADALINE neuron with \\(R\\) weights and target:\n\\[ \\begin{aligned} \\frac{\\partial e(k)}{\\partial w_{1,j}}\u0026amp;=\\frac{\\partial[t(k)-a(k)]}{\\partial w_{1,j}}\\\\ \u0026amp;=\\frac{\\partial}{\\partial w_{1,j}}[t(k)-(_1W^T\\mathbf{p}(k)+b)]\\\\ \u0026amp;=\\frac{\\partial}{\\partial w_{1,j}}[t(k)-(\\sum_{i=1}^Rw_{1,i}p_i(k)+b)] \u0026amp;=-p_j(k) \\end{aligned}\\tag{17} \\]\nand similarly: \\[ \\frac{\\partial e(k)}{b} = -1\\tag{18} \\]\nand \\(\\mathbf{z}\\) is conprised of \\(p_j{k}\\) and \\(1\\) as we mentioned above, so the equation(14) can be rewritten as:\n\\[ \\hat{\\nabla}F=\\nabla e^2(k)=-2e(k)\\mathbf{z}(k)\\tag{19} \\]\nequation(19) gives us a beautiful form of the approximation of the gradient of squared error who is also an approximation of mean squared error.\nThen we take the approximation of \\(\\nabla F\\) into the steepest descent algorithm:\n\\[ \\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\alpha \\hat{\\nabla} F(\\mathbf{x})\\bigg |_{\\mathbf{x}=\\mathbf{x}_k^{\\star}}\\tag{20} \\]\nand take \\(\\hat{\\nabla} F = -2e(k)\\mathbf{z}(k)\\) into equation(20):\n\\[ \\mathbf{x}_{k+1}=\\mathbf{x}_{k}+2\\alpha e(k)\\mathbf{z}(k)\\tag{21} \\]\nThis is an LMS algorithm which is also known as the delta rule and Widrow-Hoff Learning algorithm. For multiple neuron neural networks, we have a matrix form:\n\\[ W(k+1)=W(k)+2\\alpha \\mathbf{e}(k) \\mathbf{p}^T(k)\\tag{22} \\]\nAnalysis of Convergence We have analyzed the convergence of the steepest descent algorithm, when\n\\[ \\alpha\u0026lt;\\frac{2}{\\lambda_{\\text{max}}}\\tag{23} \\]\nand to LMS, \\(\\mathbf{x}(k)\\) is a function of \\(\\mathbf{z}(k-1),\\cdots \\mathbf{z}(0)\\) and we assume \\(\\mathbf{z}(k-1),\\cdots \\mathbf{z}(0)\\) are statistical independent and \\(\\mathbf{x}(k)\\) is independent to \\(\\mathbf{z}(k)\\) statistically.\nBecause what we have used is an approximation of the steepest descent algorithm which guarantees to converge to \\(\\mathbf{x}^{\\star}=R^{-1}\\mathbf{h}\\) for equation(9) and what we should do is to proof LMS converge to \\(\\mathbf{x}^{\\star}=R^{-1}\\mathbf{h}\\) as well.\nThe update procedure of LMS is:\n\\[ \\mathbf{x}_{k+1}=\\mathbf{x}_{k}+2\\alpha e(k)\\mathbf{z}(k)\\tag{24} \\]\nThe expectation of both sides of equation(24):\n\\[ \\mathbb{E}[\\mathbf{x}_{k+1}]=\\mathbb{E}[\\mathbf{x}_{k}]+2\\alpha \\mathbb{E}[e(k)\\mathbf{z}(k)]\\tag{25} \\]\nsubstitude \\(t(k)-\\mathbf{x}_k^T\\mathbf{z}(k)\\) for the error \\(e(k)\\) and get:\n\\[ \\mathbb{E}[\\mathbf{x}_{k+1}]=\\mathbb{E}[\\mathbf{x}_{k}]+2\\alpha \\mathbb{E}[(t(k)-\\mathbf{x}_k^T\\mathbf{z}(k))\\mathbf{z}(k)] \\tag{26} \\]\nsubstitude \\(\\mathbf{z}^T(k)\\mathbf{x}_k\\) for \\(\\mathbf{x}^T_k\\mathbf{z}(k)\\) and rearrange terms to:\n\\[ \\mathbb{E}[\\mathbf{x}_{k+1}]=\\mathbb{E}[\\mathbf{x}_{k}]+2\\alpha \\{\\mathbb{E}[t(k)\\mathbf{z}(k)]-\\mathbb{E}[\\mathbf{z}(k)\\mathbf{z}^T(k)\\mathbf{x}_k]\\} \\tag{27} \\]\nbecause \\(\\mathbf{x}_k\\) is independent of \\(\\mathbf{z}(k)\\):\n\\[ \\mathbb{E}[\\mathbf{x}_{k+1}]=\\mathbb{E}[\\mathbf{x}_{k}]+2\\alpha \\{\\mathbf{h}-R\\mathbb{E}[\\mathbf{x}_k]\\} \\tag{28} \\]\nand this can also be written as:\n\\[ \\mathbb{E}[\\mathbf{x}_{k+1}]=[I-2\\alpha R]\\mathbb{E}[\\mathbf{x}_{k}]+2\\alpha \\mathbf{h} \\tag{29} \\]\nthis is also a dynamic system and to make it stable, eigenvalues of \\(I-2\\alpha R\\) should be greater than \\(-1\\) and less than \\(1\\). And this form of the matrix has the same eigenvectors of \\(R\\) and its eigenvalues are \\(1-2\\alpha \\lambda_i\\) so:\n\\[ \\begin{aligned} 1\u0026gt;1-2\\alpha\\lambda_i\u0026gt;\u0026amp;-1\\\\ 0\u0026lt;\\alpha\u0026lt;\u0026amp;\\frac{1}{\\lambda_i} \\end{aligned}\\tag{30} \\]\nequation(30) is equivalent to:\n\\[ \\alpha\u0026lt;\\frac{1}{\\lambda_\\text{max}}\\tag{31} \\]\nBecause of \\(A=2R\\) so equation (31) gives the same condition as ‘steepest descent algorithm’\nUnder this condition, when the system becomes stable, we would have:\n\\[ \\mathbb{E}[\\mathbf{x}_{\\text{ss}}]=[I-2\\alpha R]\\mathbb{E}[\\mathbf{x}_{\\text{ss}}] +2 \\alpha \\mathbf{h} \\tag{32} \\]\nor\n\\[ \\mathbb{E}[\\mathbf{x}_{\\text{ss}}]=R^{-1}\\mathbf{h}=\\mathbf{x}^{\\star} \\tag{33} \\]\nEquation(33) shows that the LMS solution which is generated by each incoming input from time to time would finally give a convergence solution.\nReferences  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","permalink":"https://anthony-tan.com/Widrow-Hoff-Learning/","summary":"Preliminaries ‘Performance Surfaces and Optimum Points’ Linear algebra stochastic approximation Probability Theory  ADALINE, LMS, and Widrow-Hoff learning1 Performance learning had been discussed. But we have not used it in any neural network. In this post, we talk about an important application of performance learning. And this new neural network was invented by Frank Widrow and his graduate student Marcian Hoff in 1960. It was almost the same time as Perceptron was developed which had been discussed in ‘Perceptron Learning Rule’.","title":"Widrow-Hoff Learning"},{"content":"Preliminaries ‘steepest descent method’ “Newton’s method”  Conjugate Gradient1 We have learned ‘steepest descent method’ and “Newton’s method”. The main advantage of Newton’s method is the speed, it converges quickly. And the main advantage of the steepest descent method guarantees to converge to a local minimum. But the limit of Newton’s method is that it needs too many resources for both computation and storage when the number of parameters is large. And the speed of the steepest descent method is too slow. What we are going to research is to mix them up into a new algorithm that needs fewer resources but convergents quickly. In other words, we use first-order derivatives but still have quadratic efficiency.\nIn the last section of ‘steepest descent method’, we found that when we minimize along the line at every iteration of the steepest descent method the directions of descent steps are orthogonal for a quadratic function.\nOur target here was simplified to a quadratic function to have an insight into the process of the new method:\n\\[ F(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^TA\\mathbf{x}+\\mathbf{d}\\mathbf{x}+c\\tag{1} \\]\nBefore we go to the method, we need to recall the linear algebra concept “conjugate” firstly:\n A set of vectors \\(\\{\\mathbf{p}_k\\}\\) for \\(k=1,2,\\cdots,n\\) are mutually conjugate with respect to a positive definite matrix \\(A\\) if and only if: \\[\\mathbf{p}^T_kA\\mathbf{p}_j=0\\tag{2}\\] where \\(j\\neq k\\)\n the vectors in a conjugate, set are linear independent, so if there are \\(n\\) vectors in a conjugate set, they can span a \\(n\\)-dimansion space. And to a certain space, there are infinite numbers of conjugate sets. For instance, eigenvectors of \\(A\\) in equation(1) are \\(\\{\\mathbf{z}_1,\\cdots,\\mathbf{z}_n\\}\\) and eigenvalues are \\(\\{\\lambda_1,\\cdots,\\lambda_n\\}\\) and we have:\n\\[ \\mathbf{z}_k^TA\\mathbf{z}_j=\\lambda_j\\mathbf{z}_k^T\\mathbf{z}_j\\tag{3} \\]\nwhen the matrix \\(A\\) is positive definite, eigenvectors are mutually orthogonal. And then equation(3) equal to \\(0\\) when \\(k\\neq j\\)\n‘Quadratic function’ told us the eigenvectors are principal axes, and searching along these eigenvectors can finally minimize the quadratic function.\nThe calculation of a Hessian matrix should be avoided in our new method because of the computational resources. So searching along the directions of eigenvectors would not be used.\nThen an idea comes to us whether other conjugate sets could give a trajectory to the minimum. And mathematicians did prove this guess is correct. Searching along a conjugate set, \\(\\{\\mathbf{p_1},\\dots,\\mathbf{p}_n\\}\\), of a matrix \\(A\\) can converge to the minimum in at most \\(n\\) searches.\nBased on this theory, what we should do next is find a conjugate set of a matrix without the use of the Hessian matrix. We restate that:\n\\[ \\nabla F(\\mathbf{x})=\\mathbf{g}=A\\mathbf{x}+\\mathbf{d}\\tag{4} \\]\nand\n\\[ \\nabla^2 F(\\mathbf{x})=A\\tag{5} \\]\nin the steepest descent method, we search along the negative direction of gradient \\(\\mathbf{g}\\).\nThen the change between the consecutive steps is: \\[ \\Delta \\mathbf{g}_k = \\mathbf{g}_{k+1}-\\mathbf{g}_k\\tag{6} \\]\nInsert equation(4) into equation(6):\n\\[ \\begin{aligned} \\Delta \\mathbf{g}_k \u0026amp;= \\mathbf{g}_{k+1}-\\mathbf{g}_k\\\\ \u0026amp;=A\\mathbf{x}_{k+1}+\\mathbf{d} -A\\mathbf{x}_{k}+\\mathbf{d}\\\\ \u0026amp;=A\\Delta\\mathbf{x} \\end{aligned} \\tag{7} \\]\nequation(7) is the key point in this new algorithm because this equation replace \\(A\\Delta\\mathbf{x}\\) with \\(\\Delta \\mathbf{g}_k\\) to avoid the calculation about matrix \\(A\\). And for in the post we update the variables in each step by \\(\\mathbf{x}_{k+1} =\\mathbf{x}_k +\\alpha\\mathbf{p}_k\\) then we have:\n\\[ \\Delta\\mathbf{x} =\\mathbf{x}_{k+1} -\\mathbf{x}_k =\\alpha\\mathbf{p}_k\\tag{8} \\]\nthe assumption in equation(2) makes sure that:\n\\[ \\alpha\\mathbf{p}^T_kA\\mathbf{p}_j=0\\tag{9} \\]\nand take equation(8) into equation(9):\n\\[ \\alpha\\mathbf{p}^T_kA\\mathbf{p}_j=\\Delta\\mathbf{x}^TA\\mathbf{p}_j=0\\tag{10} \\]\nThen the key point equation(7) would be used to replace the \\(\\Delta\\mathbf{x}^TA\\) in equation(10):\n\\[ \\Delta\\mathbf{x}^TA\\mathbf{p}_j=\\mathbf{g}^T\\mathbf{p}_j=0\\tag{11} \\]\nwhere \\(j\\neq k\\)\nTill now, we have all of what we need to build a method acting like a second-order method with just the amount of calculation of the first-order method. And the procedure of method is:\ninitialize \\(\\mathbf{p}_0=-\\mathbf{g}_0\\) construct a vector, the next direction, \\(\\mathbf{p}_k\\) orthogonal to \\(\\{\\Delta\\mathbf{g}_0,\\Delta\\mathbf{g}_1,\\cdots,\\Delta\\mathbf{g}_{k-1}\\}\\)(somehow it looks like Gram-Schimidt method) \\(\\mathbf{p}_k=-\\mathbf{g}_k+\\beta_k\\mathbf{p}_{k-1}\\) \\(\\beta_k\\) is a scalar and can be calculated in three kinds of ways: \\(\\beta_k=\\frac{\\Delta\\mathbf{g}_{k-1}^T\\mathbf{g}_k}{\\Delta\\mathbf{g}_{k-1}^T\\mathbf{p}_{k-1}}\\) \\(\\beta_k=\\frac{\\mathbf{g}_{k}^T\\mathbf{g}_k}{\\mathbf{g}_{k-1}^T\\mathbf{p}_{k-1}}\\) \\(\\beta_k=\\frac{\\Delta\\mathbf{g}_{k-1}^T\\mathbf{g}_k}{\\mathbf{g}_{k-1}^T\\mathbf{g}_{k-1}}\\)   References  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","permalink":"https://anthony-tan.com/Conjugate-Gradient/","summary":"Preliminaries ‘steepest descent method’ “Newton’s method”  Conjugate Gradient1 We have learned ‘steepest descent method’ and “Newton’s method”. The main advantage of Newton’s method is the speed, it converges quickly. And the main advantage of the steepest descent method guarantees to converge to a local minimum. But the limit of Newton’s method is that it needs too many resources for both computation and storage when the number of parameters is large.","title":"Conjugate Gradient"},{"content":"Preliminaries ‘steepest descent algorithm’ Linear Algebra Calculus 1,2  Newton’s Method1 Taylor series gives us the conditions for minimum points based on both first-order items and the second-order item. And first-order item approximation of a performance index function produced a powerful algorithm for locating the minimum points which we call ‘steepest descent algorithm’.\nNow we want to have an insight into the second-order approximation of a function to find out whether there is an algorithm that can also work as a guide to the minimum points. The approximation of \\(F(\\mathbf{x}_{k+1})\\) is:\n\\[ \\begin{aligned} F(\\mathbf{x}_{k+1})\u0026amp;=F(\\mathbf{x}_k+\\Delta \\mathbf{x}_k)\\\\ \u0026amp;\\approx F(\\mathbf{x}_k)+\\mathbf{g}^T_k\\Delta \\mathbf{x}_k+\\frac{1}{2}\\Delta \\mathbf{x}^T_kA_k\\Delta \\mathbf{x}_k \\end{aligned} \\tag{1} \\]\nNow we replace the performance index with its Taylor approximation in equation 1. The gradient of equation 1:\n\\[ \\nabla F(\\mathbf{x}_{k+1})=A_k\\mathbf{x}_k+\\mathbf{g}_k\\tag{2} \\]\nTo get the stationary points, we set equation(2) equal to \\(0\\):\n\\[ \\begin{aligned} A_k\\Delta \\mathbf{x}_k+\\mathbf{g}_k\u0026amp;=0\\\\ \\Delta \\mathbf{x}_k\u0026amp;=-A_k^{-1}\\mathbf{g}_k \\end{aligned} \\tag{3} \\]\nthen as the iterative algorithm framework said, we update \\(\\mathbf{x}_k\\) which is also known as Newton’s Method:\n\\[ \\mathbf{x}_{k+1}=\\mathbf{x}_k+\\Delta \\mathbf{x}_k=\\mathbf{x}_k-A_k^{-1}\\mathbf{g}_k\\tag{4} \\]\nNow is the time to look at an example:\n\\[ F(\\mathbf{x})=x^2_1+25x_2^2\\tag{5} \\]\nthe gradient of equation(5) is:\n\\[ \\nabla F(\\mathbf{x})= \\begin{bmatrix} \\frac{\\partial}{\\partial x_1}F(\\mathbf{x})\\\\ \\frac{\\partial}{\\partial x_2}F(\\mathbf{x}) \\end{bmatrix}= \\begin{bmatrix} 2x_1\\\\ 50x_2 \\end{bmatrix}\\tag{6} \\]\nthe Hessian matrix is:\n\\[ \\nabla F(\\mathbf{x})= \\begin{bmatrix} \\frac{\\partial^2}{\\partial^2 x_1}F(\\mathbf{x})\u0026amp; \\frac{\\partial^2}{\\partial x_1 \\partial x_2}F(\\mathbf{x})\\\\ \\frac{\\partial^2}{\\partial x_2\\partial x_1}F(\\mathbf{x})\u0026amp;\\frac{\\partial^2}{\\partial^2 x_2}F(\\mathbf{x}) \\end{bmatrix}= \\begin{bmatrix} 2\u0026amp;0\\\\ 0\u0026amp;50 \\end{bmatrix}\\tag{7} \\]\nthen we update \\(\\mathbf{x}\\) as equation(4) and with the intial point \\(\\mathbf{x}_0=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\)\n\\[ \\begin{aligned} \\mathbf{x}_1\u0026amp;=\\mathbf{x}_0-A_k^{-1}\\mathbf{g}_k\\\\ \u0026amp;=\\begin{bmatrix}1\\\\1\\end{bmatrix}-\\begin{bmatrix}0.5\u0026amp;0\\\\0\u0026amp;0.02\\end{bmatrix}\\begin{bmatrix}2\\\\50\\end{bmatrix}\\\\ \u0026amp;=\\begin{bmatrix}0\\\\0\\end{bmatrix} \\end{aligned} \\tag{8} \\]\nThen we test \\(\\mathbf{x}_{1}=\\begin{bmatrix}0\\\\0\\end{bmatrix}\\) with terminational condition. And then we stop this algorithm.\nThis one-step algorithm is not a coincidence. This method will always find the minimum of a quadratic function in 1 step. This is because the second-order approximation of the quadratic function is just itself in another form. They have the same minimum and stationary points. However, when \\(F(x)\\) is not quadratic the method may:\nnot generally converge in 1 step and be not sure whether the algorithm could converge or not which is dependent on both the performance index and the initial guess.  Let’s consider a function that is not quadratic:\n\\[ F(\\mathbf{x})=(x_2-x_1)^4 + 8x_1x_2-x_1+x_2+3\\tag{9} \\] There are a local minimum, a global minimum, and a saddle point.\nWith the initial point \\(\\mathbf{x}_0=\\begin{bmatrix}1.5\\\\0\\end{bmatrix}\\)\nThen we keep updating the position \\(\\mathbf{x}_k\\) with the equation(4) and we have:\nThe right side of the figure is the process of second-order Taylor approximation.\nif we initial the process at the point \\(\\mathbf{x}_0=\\begin{bmatrix}0.75\\\\0.75\\end{bmatrix}\\). It will be looked like:\nThe right side of the figure is the process of second-order Taylor approximation.\nand it converges to the saddle point.\nNewton’s method converges quickly in many applications. The analytic function can be accurately approximated by quadratic in a small neighborhood of a strong minimum. And Newton’s method can not distinguish between local minimum, global minimum, and saddle points.\nWhen we use a quadratic function approximation, we consider the quadratic function only and forget the other parts of the original function where it is not able to be approximated by the quadratic function. So what we can see through a quadratic approximate is only a small region where there is only one minimum. We can not distinguish which kind of stationary point it is. And Newton’s method can produce very unpredictable results, too.\nIf the initial point is \\(\\mathbf{x}_0=\\begin{bmatrix}1.15\\\\0.75\\end{bmatrix}\\) of the example above, it is far from the minimum but it finally converges to the minimum point :\nwhile the initial point \\(\\mathbf{x}_0=\\begin{bmatrix}0.75\\\\0.75\\end{bmatrix}\\) which is more close to the minimum converge to the saddle points:\nthe right side of the figure is the process of second-order Taylor approximation.\nConclusion Newton’s method is:\nfaster than the steepest descent quite complex possible oscillate or diverge (steepest descent is guaranteed to converge when the learning rate is suitable) and there is a variation of Newton’s method suited to neural networks. the computation and storage of the Hessian matrix and its inverse could easily exhaust our computational resource Newton’s method degenerates to the steepest descent method when \\(A_k=A_k^{-1}=I\\)  References  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","permalink":"https://anthony-tan.com/Newton_s-Method/","summary":"Preliminaries ‘steepest descent algorithm’ Linear Algebra Calculus 1,2  Newton’s Method1 Taylor series gives us the conditions for minimum points based on both first-order items and the second-order item. And first-order item approximation of a performance index function produced a powerful algorithm for locating the minimum points which we call ‘steepest descent algorithm’.\nNow we want to have an insight into the second-order approximation of a function to find out whether there is an algorithm that can also work as a guide to the minimum points.","title":"Newton's Method"},{"content":"Preliminaries ‘An Introduction to Performance Optimization’ Linear algebra Calculus 1,2  Direction Based Algorithm and a Variation1 This post describes a direction searching algorithm(\\(\\mathbf{x}_{k}\\)). And its variation gives a way to estimate step length (\\(\\alpha_k\\)).\nSteepest Descent To find the minimum points of a performance index by an iterative algorithm, we want to decrease the value of the performance index step by step which looks like going down from the top of the hill. And the crucial of this algorithm is that every iteration makes the performance index decrease:\n\\[ F(\\mathbf{x}_{k+1})\u0026lt;F(\\mathbf{x}_{k})\\tag{1} \\]\nOur mission is to find the direction \\(\\mathbf{p}_k\\) with a relatively short step length \\(\\alpha_k\\) which leads us downhill.\nThe first-order Taylor series of an iterative step is:\n\\[ F(\\mathbf{x}_{k+1})=F(\\mathbf{x}_{k}+\\Delta \\mathbf{x}_k)\\approx F(\\mathbf{x}_{k})+\\mathbf{g}^T\\Delta\\mathbf{x}_k\\tag{2} \\]\nwhere \\(\\mathbf{g}_k\\) is the gradient at position \\(\\mathbf{x}\\) of the performance index \\(F(\\mathbf{x})\\) which means:\n\\[ \\mathbf{g}_k = \\nabla F(\\mathbf{x})\\bigg |_{\\mathbf{x}=\\mathbf{x}_k}\\tag{3} \\]\nFrom equation(1) and equation(2) for the purpose \\(F(\\mathbf{x}_{k+1})\u0026lt;F(\\mathbf{x}_{k})\\), we need:\n\\[ \\mathbf{g}^T\\Delta\\mathbf{x}_k\u0026lt;0\\tag{4} \\]\n( \\(\\Delta\\mathbf{x}_k\\) , the change of \\(\\mathbf{x}_k\\), can also be represented by step length \\(\\alpha_k\\) and direction \\(\\mathbf{p}\\), then the equation(4) has a equivalent form:\n\\[ \\alpha_k\\mathbf{g}^T\\Delta\\mathbf{p}_k\u0026lt;0\\tag{5} \\]\nIn the previous posts, we have seen how to find the greatest value of \\(\\mathbf{g}^T\\Delta\\mathbf{p}_k\\). And now we can find the smallest value of \\(\\mathbf{g}^T\\Delta\\mathbf{p}_k\\) in the same way. Then we got the smallest value is:\n\\[ -\\mathbf{g}^T\\mathbf{g}\\tag{6} \\]\nwhich means that the deepest direction we would search is:\n\\[ \\Delta\\mathbf{p}_k=-\\mathbf{g}\\tag{7} \\]\nAccording to the iterative optimization algorithm framework in‘An Introduction to Performance Optimization’, the second step is :\n\\[ \\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\alpha_k\\mathbf{g}_k\\tag{8} \\]\nThe step length is also called the learning rate. And the choice of \\(\\alpha_k\\) can be:\nMinimizing \\(F(\\mathbf{x})\\) with \\(\\alpha_k\\) by minimizing along the line: \\(\\mathbf{x}_k-\\alpha_k\\mathbf{g}_k\\) Fixed \\(\\alpha_k\\), such as \\(\\alpha_k=0.002\\) Predetermined, like \\(\\alpha_k=\\frac{1}{k}\\)  An Example \\[ F(\\mathbf{x})=x_1^2+25x_2^2\\tag{9} \\]\nstart at point \\(x=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}\\) gradient of \\(F(\\mathbf{x})\\) is \\(\\nabla F(\\mathbf{x})=\\begin{bmatrix}\\frac{\\partial F}{\\partial x_1}\\\\\\frac{\\partial F}{\\partial x_2}\\end{bmatrix}=\\begin{bmatrix}2x_1\\\\50x_2\\end{bmatrix}\\) \\(\\mathbf{g}_0=\\nabla F(\\mathbf{x})\\bigg|_{\\mathbf{x}=\\mathbf{x}_0}=\\begin{bmatrix}1\\\\25\\end{bmatrix}\\) set \\(\\alpha = 0.01\\) update: \\(\\mathbf{x}_1=\\mathbf{x}_0-0.01\\mathbf{g}_0=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}-0.01\\begin{bmatrix}1\\\\25\\end{bmatrix}=\\begin{bmatrix}0.49\\\\0.25\\end{bmatrix}\\) update: \\(\\mathbf{x}_2=\\mathbf{x}_1-0.01\\mathbf{g}_1=\\begin{bmatrix}0.49\\\\0.25\\end{bmatrix}-0.01\\begin{bmatrix}0.98\\\\12.5\\end{bmatrix}=\\begin{bmatrix}0.4802\\\\0.125\\end{bmatrix}\\) go on updating until the smallest point is achieved.  The whole trajectory looks like this:\nand the learning rate, step length is a constant in this algorithm, however, we can test 2 different values and watch their behavior. When we set \\(\\alpha=0.01\\), we have:\nand when we set \\(\\alpha=0.02\\), we have: These results illustrated:\nIn first several steps, the descent speed is faster than in later steps A greater learning rate seems to have a higher speed This algorithm can converge to the minimum point.  The second point gives us a new idea, of what will the algorithms do when we have a relatively bigger learning rate. To be on the safe side we select a not so big learning rate \\(\\alpha=0.05\\), we have:\nthis algorithm diverges, which means it would never stop at the minimum and get farther and farther as steps go on. So we have to take care of the value of the learning rate, a small learning rate can slow down the algorithm but a big one can break up the algorithm.\nStable Learning Rates To have a fast speed and converge to the minimum, we need to study the learning rate \\(\\alpha\\). To simplify the problem, we start with supposing the performance index is a quadratic function:\n\\[ F(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^TA\\mathbf{x}+\\mathbf{d}^T\\mathbf{x}+c\\tag{10} \\]\nand we have already known its gradient is:\n\\[ \\nabla F(\\mathbf{x})=A \\mathbf{x}+\\mathbf{d}\\tag{11} \\]\nwe take equation(11) into update step equation(8), we have:\n\\[ \\mathbf{x}_{k+1}=\\mathbf{x}_k-\\alpha\\mathbf{g}_k=\\mathbf{x}_k-\\alpha(A \\mathbf{x}_k+\\mathbf{d})\\tag{12} \\]\nor an equivalent form:\n\\[ \\mathbf{x}_{k+1}=(I-\\alpha A)\\mathbf{x}_k - \\alpha\\mathbf{d}\\tag{13} \\]\nIn the linear algebra course or other courses, this equation is called a ‘linear dynamic system’. To make the system stable, the eigenvalues of \\(I-\\alpha A\\) are less than one in magnitude. And \\(I-\\alpha A\\) has the same eigenvectors with \\(A\\). Let \\([\\lambda_1,\\lambda_2,\\cdots,\\lambda_n]\\) be the eigenvalues of \\(A\\) and let \\([\\mathbf{z}_1,\\mathbf{z}_2,\\cdots,\\mathbf{z}_n]\\) be the eigenvectors of \\(A\\)\n\\[ [I-\\alpha A]\\mathbf{z}_i=\\mathbf{z}_i-\\alpha A\\mathbf{z}_i=\\mathbf{z}_i-\\alpha \\lambda_i\\mathbf{z}_i=(1-\\alpha\\lambda_i)\\mathbf{z}_i\\tag{14} \\]\n\\(I-\\alpha A\\) has the same eigenvectors with \\(A\\) and has the eigenvalues: \\([1-\\alpha\\lambda_1,1-\\alpha\\lambda_2,\\cdots,1-\\alpha\\lambda_n]\\)\nConcerning the equation(13) and eigenvalues of \\(I-\\alpha A\\) to stabilize the system which here is the steepest descent algorithm, we need:\n\\[ \\begin{aligned} \u0026amp;|1-\\alpha \\lambda_i|\u0026amp;\u0026lt;1\\\\ -1\u0026amp;\u0026lt;1-\\alpha \\lambda_i\u0026amp;\u0026lt;1\\\\ -2\u0026amp;\u0026lt;-\\alpha \\lambda_i\u0026amp;\u0026lt;0\\\\ \\end{aligned}\\tag{15} \\]\nfor \\(\\alpha\u0026gt;0\\)\n\\[ \\begin{aligned} \\frac{2}{\\alpha}\u0026amp;\u0026gt; \\lambda_i\u0026amp;\u0026gt;0\\\\ \\end{aligned}\\tag{16} \\]\nfrom equation(16), we finally have \\[ \\frac{2}{\\lambda_i}\u0026gt;\\alpha\\tag{17} \\]\nwhich implies:\n\\[ \\frac{2}{\\lambda_{\\text{max}}}\u0026gt;\\alpha\\tag{18} \\]\nThis gives us the maximum stable learning rate is inversely proportional to the maximum curvature(direction along with eigenvector according to the maximum eigenvalue \\(\\lambda_\\text{max}\\)) of the quadratic function.\nLet’s go back to the example, the Hessian matrix of the equation(9) is: \\[ A=\\begin{bmatrix} 2\u0026amp;0\\\\0\u0026amp;50 \\end{bmatrix}\\tag{19} \\]\nits eigenvectors and eigenvalues are: \\[ \\{\\lambda_1=2,\\mathbf{z}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}\\},\\{\\lambda_2=50,\\mathbf{z}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}\\}\\tag{20} \\]\ntaking \\(\\lambda_\\text{max}=\\lambda_2=50\\) into equation(18), we get: \\[ \\alpha_\\text{max}\u0026lt;\\frac{2}{50}=0.04\\tag{21} \\]\nso, let’s check the behavior of the algorithm when \\(\\alpha=0.039\\) and \\(\\alpha=0.041\\):\nset \\(\\alpha=0.039\\) we have: set \\(\\alpha=0.041\\) we have:  Up to now, both concepts and implements have been built to prove the correctness of the algorithm. And we also have the following tips:\nThe algorithm tends to converge most quickly in the direction of the eigenvector corresponding to the largest eigenvalue The algorithm tends to converge most slowly in the direction of the eigenvector corresponding to the smallest eigenvalue Do not overshoot the minimum point for the too-long step(learning rate \\(\\alpha\\))  Minimizing along a Line The section above gives us the upper bound of \\(\\alpha\\), but we have three kinds of strategies for selecting a \\(\\alpha\\).\nThe first one is “Minimizing \\(F(\\mathbf{x})\\) with \\(\\alpha_k\\) by minimizing along the line: \\(\\mathbf{x}_k-\\alpha_k\\mathbf{g}_k\\)”. What shall we do with this kind of \\(\\alpha\\)?\nTo arbitrary functions, the stationary point along a direction can be calculated by:\n\\[ \\begin{aligned} \u0026amp;\\frac{dF}{d\\alpha_k}(\\mathbf{x}_k+\\alpha_k\\mathbf{p}_k)\\\\ \u0026amp;=\\nabla F(\\mathbf{x})^T\\bigg|_{\\mathbf{x}=\\mathbf{x}_k}\\mathbf{p}_k+\\alpha_k\\mathbf{p}_k^T\\nabla F^2(\\mathbf{x})^T\\bigg|_{\\mathbf{x}=\\mathbf{x}_k}\\mathbf{p}_k\\\\ \u0026amp;=0 \\end{aligned}\\tag{22} \\]\nand then\n\\[ \\alpha=-\\frac{\\nabla F(\\mathbf{x})^T\\bigg|_{\\mathbf{x}=\\mathbf{x}_k}\\mathbf{p}_k}{\\mathbf{p}_k^T\\nabla F^2(\\mathbf{x})\\bigg|_{\\mathbf{x}=\\mathbf{x}_k}\\mathbf{p}_k}=-\\frac{\\mathbf{g}^T_k\\mathbf{p}_k}{\\mathbf{p}_k^TA_k\\mathbf{p}_k}\\tag{23} \\]\nwhere: \\(A_k\\) is the Hessian matrix of an old guess \\(\\mathbf{x}_k\\):\n\\[ A_k=\\nabla^2F(\\mathbf{x})\\bigg|_{\\mathbf{x}=\\mathbf{x}_k}\\tag{24} \\]\nHere we look at an example:\n\\[ F(x)=\\frac{1}{2}\\mathbf{x}^T\\begin{bmatrix} 2\u0026amp;1\\\\1\u0026amp;2 \\end{bmatrix}\\mathbf{x}\\tag{25} \\]\nwith the initial guess \\[ \\mathbf{x}_0=\\begin{bmatrix}0.8\\\\-0.25\\end{bmatrix}\\tag{26} \\]\nthe gradient of the function is\n\\[ \\nabla F(\\mathbf{x})=\\begin{bmatrix}2x_1+x_2\\\\x_1+2x_2\\end{bmatrix}\\tag{27} \\]\nthe initial direction of the algorithm is\n\\[ \\mathbf{p}_0=-\\mathbf{g}_0=-\\nabla F(\\mathbf{x})\\bigg|_{\\mathbf{x}=\\mathbf{x}_k}=\\begin{bmatrix}-1.35\\\\-0.3\\end{bmatrix}\\tag{28} \\]\nand take equation(28) and \\(A=\\begin{bmatrix}2\u0026amp;1\\\\1\u0026amp;2\\end{bmatrix}\\) into equation(23): \\[ \\alpha_0=\\frac{\\begin{bmatrix}1.35\u0026amp;0.3\\end{bmatrix}\\begin{bmatrix}2\u0026amp;1\\\\1\u0026amp;2\\end{bmatrix}}{\\begin{bmatrix}1.35\u0026amp;0.3\\end{bmatrix}\\begin{bmatrix}2\u0026amp;1\\\\1\u0026amp;2\\end{bmatrix}\\begin{bmatrix}1.35\\\\0.3\\end{bmatrix}}=0.413\\tag{29} \\]\nand take all these data into equation(8):\n\\[ \\mathbf{x}_1=\\mathbf{x}_0-\\alpha_0\\mathbf{g}_0=\\begin{bmatrix}0.8\\\\-0.25\\end{bmatrix}-0.413\\begin{bmatrix}1.35\\\\0.3\\end{bmatrix}=\\begin{bmatrix}0.24\\\\-0.37\\end{bmatrix}\\tag{30} \\]\nWhat is going on is repeating the steps: equation(26) to equation(30) until some terminative conditions are achieved. It works like this:\nAll the new directions in the steps are orthogonal to their last direction:\n\\[ \\mathbf{p}_{k+1}^T\\mathbf{p}_k=\\mathbf{g}_{k+1}^T\\mathbf{p}_k=0\\tag{31} \\]\nwhat we need now is to proof is \\(\\mathbf{g}_{k+1}^T\\mathbf{p}_k=0\\), with the chain rule of equation(22):\n\\[ \\begin{aligned} \\frac{d}{d\\alpha_k}F(\\mathbf{x}_{k+1})\u0026amp;=\\frac{d}{d\\alpha_k}F(\\mathbf{x}_k+\\alpha_k\\mathbf{p}_k)\\\\ \u0026amp;=\\nabla F(\\mathbf{x})^T\\bigg|_{\\mathbf{x}=\\mathbf{x}_{k+1}}\\frac{d}{d\\alpha_k}(\\mathbf{x}_k+\\alpha_k\\mathbf{p}_k)\\\\ \u0026amp;=\\nabla F(\\mathbf{x})^T\\bigg|_{\\mathbf{x}=\\mathbf{x}_{k+1}}\\mathbf{p}_k\\\\ \u0026amp;=0 \\end{aligned}\\tag{32} \\]\nequation(32) gives us: a new direction is always orthogonal to the last step direction at the minimum point of the function along the last step direction. This is also an inspiration for another algorithm called conjugate direction.\nReferences  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","permalink":"https://anthony-tan.com/Steepest-Descent-Method/","summary":"Preliminaries ‘An Introduction to Performance Optimization’ Linear algebra Calculus 1,2  Direction Based Algorithm and a Variation1 This post describes a direction searching algorithm(\\(\\mathbf{x}_{k}\\)). And its variation gives a way to estimate step length (\\(\\alpha_k\\)).\nSteepest Descent To find the minimum points of a performance index by an iterative algorithm, we want to decrease the value of the performance index step by step which looks like going down from the top of the hill.","title":"Steepest Descent Method"},{"content":"Preliminaries Nothing  Performance Optimization1 Taylor series had been used for analyzing the performance surface and locating the optimum points of a certain performance index. This short post is a brief introduction to performance optimization and the following posts are the samples of three optimization algorithms categories:\n‘Steepest Descent’ “Newton’s Method” ‘Conjugate Gradient’  Recall the analysis of the performance index, which is a function of the parameters of the model. Most of the optimization problems could not be solved analytically. So, searching for the whole solution space is a straightforward strategy. However, no algorithms or computers can search a whole parameter space even which has only 1 dimension to locate the optimal points of the surface. So we need a map.\nThese algorithms we discussed here are developed hundreds of years ago. And the basic principles of optimization were discovered during the \\(17^{\\text{th}}\\) century. And some of them were brought up by Kepler, Fermat, Newton, and Leibniz. However, computers are more powerful than paper and pencils. So, these optimization algorithms had been rediscovered and became a major branch of mathematics. Thanks to the brilliant scientists for their contribution to our human beings:\nAll the algorithms we are going to talk about are iterative. Their general framework is:\nstart from some initial guess \\(\\mathbf{x}_0\\) update our guess in stages:   \\(\\mathbf{x}_{k+1}=\\mathbf{x}_k+\\alpha_k \\mathbf{p}_k\\) or \\(\\Delta \\mathbf{x}_k=\\mathbf{x}_{k+1}-\\mathbf{x}_k=\\alpha_k \\mathbf{p}_k\\)  check the terminational condition, decide to go back to step 2 or to terminate the algorithm.  In the algorithms, \\(\\mathbf{p}_k\\) is the search direction that works like the compass of Captain Jack Sparrow. It can lead you to what you want. \\(\\alpha_k\\) is the step length which means how far we should go along the current direction of \\(\\mathbf{p}_k\\). \\(\\mathbf{x}_0\\) is an initial position of the algorithm.\nThese three elements are the basis of three categories of optimization algorithms. How to decide the direction we are going to search, how far we should go in a certain direction, and how to initiate the first point gave us research aspects.\nReferences  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Performance-Optimization/","summary":"Preliminaries Nothing  Performance Optimization1 Taylor series had been used for analyzing the performance surface and locating the optimum points of a certain performance index. This short post is a brief introduction to performance optimization and the following posts are the samples of three optimization algorithms categories:\n‘Steepest Descent’ “Newton’s Method” ‘Conjugate Gradient’  Recall the analysis of the performance index, which is a function of the parameters of the model.","title":"An Introduction to Performance Optimization"},{"content":"Preliminaries Linear algebra Calculus 1,2 Taylor series  Quadratic Functions1 Quadratic function, a type of performance index, is universal. One of its key properties is that it can be represented in a second-order Taylor series precisely.\n\\[ F(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^TA\\mathbf{x}+\\mathbf{d}\\mathbf{x}+c\\tag{1} \\]\nwhere \\(A\\) is a symmetric matrix(if it is not symmetric, it can be easily converted into symmetric). And recall the property of gradient:\n\\[ \\nabla (\\mathbf{h}^T\\mathbf{x})=\\nabla (\\mathbf{x}^T\\mathbf{h})=\\mathbf{h}\\tag{2} \\]\nand\n\\[ \\nabla (\\mathbf{x}^TQ\\mathbf{x})=Q\\mathbf{x}+Q^T\\mathbf{x}=2Q\\mathbf{x}\\tag{3} \\]\nthen the first order of quadratic functions are:\n\\[ \\nabla F(\\mathbf{x})=A\\mathbf{x}+\\mathbf{d}\\tag{4} \\]\nand second-order of quadratic functions are: \\[ \\nabla^2 F(\\mathbf{x})=A\\tag{5} \\]\nThe shape of quadratic functions can be described by eigenvalues and eigenvectors of its Hessian matrix. Hessian matrix is a symmetric matrix and its eigenvectors are mutually orthogonal, let:\n\\[ \\mathbf{z}_1,\\mathbf{z}_2,\\cdots ,\\mathbf{z}_n\\tag{6} \\]\ndenote the whole set of the eigenvectors of the Hessian matrix \\(A\\) and we build a matrix:\n\\[ B=\\begin{bmatrix} \\mathbf{z}_1\u0026amp;\\mathbf{z}_2\u0026amp;\\cdots \u0026amp;\\mathbf{z}_n \\end{bmatrix}\\tag{7} \\]\nand because of \\(BB^T=I\\), we have \\(B^{-1}=B^T\\). When the Hessian matrix is positive definite, it would have \\(n\\) eigenvectors(Hessian has a full rank \\(n\\)) and we can change the Hessian matrix into a new matrix whose basis are the set of eigenvectors:\n\\[ A\u0026#39;=B^TAB=\\begin{bmatrix} \\lambda_1\u0026amp;\u0026amp;\u0026amp;\\\\ \u0026amp;\\lambda_2\u0026amp;\u0026amp;\\\\ \u0026amp;\u0026amp;\\ddots\u0026amp;\\\\ \u0026amp;\u0026amp;\u0026amp;\\lambda_n \\end{bmatrix}=\\Lambda\\tag{8} \\]\nafter changing the basis, the new Hessian matrix is a diagonal matrix whose elements on the diagonal are the eigenvalues. This process can be inversed because of \\(BB^T=I\\):\n\\[ A=BA\u0026#39;B^T=B\\Lambda B^T\\tag{9} \\]\nWe can calculate the derivative in any directions:\n\\[ \\frac{\\mathbf{p}^T\\nabla^2 F(\\mathbf{x})\\mathbf{p}}{||\\mathbf{p}||^2}=\\frac{\\mathbf{p}^TA\\mathbf{p}}{||\\mathbf{p}||^2}\\tag{10} \\]\nFor columns of \\(B\\) can span the whole space, So we can find a vector \\(\\mathbf{c}\\) satisfy:\n\\[ \\mathbf{p}=B\\mathbf{c}\\tag{11} \\]\nthen take equation(8), equation(11) into equation(10) we get:\n\\[ \\frac{\\mathbf{c}^TB^TAB\\mathbf{c}}{\\mathbf{c}^TB^TB\\mathbf{c}}=\\frac{\\mathbf{c}^T\\Lambda\\mathbf{c}}{\\mathbf{c}^TI\\mathbf{c}}=\\frac{\\sum^n_{i=1}\\lambda_i c_i^2}{\\sum_{i=1}^n c^2_i}\\tag{12} \\]\nFrom equation(12), we could conclude:\n\\[ \\lambda_{\\text{min}}\\leq \\frac{\\mathbf{p}^TA\\mathbf{p}}{||\\mathbf{p}||^2}\\leq \\lambda_{\\text{max}}\\tag{13} \\]\nmaximum \\(2^{\\text{nd}}\\) derivative occure along with \\(\\mathbf{z}_{\\text{max}}\\)(according to \\(\\lambda_{\\text{max}}\\)) eigenvalues is the \\(2^{\\text{nd}}\\) derivative along its eigenvector. eigenvectors could define a new coordinate system eigenvectors are principal axes of the function contour. going along with the \\(\\mathbf{z}_{\\text{max}}\\) direction could have the largest change in function value \\(|\\Delta F(x)|\\) eigenvalues here are all positive because of positive definite.  An example:\n\\[ F(\\mathbf{x})=x_1^2+x_1x_2+x_2^2 =\\frac{1}{2}\\mathbf{x}^T \\begin{bmatrix} 2\u0026amp;1\\\\1\u0026amp;2 \\end{bmatrix}\\mathbf{x}\\tag{14} \\]\nwe can calculate the eigenvectors and eigenvalues:\n\\[ \\begin{aligned} \u0026amp;\\lambda_1=1\u0026amp;\u0026amp;\\mathbf{z}_1=\\begin{bmatrix} 1\\\\-1 \\end{bmatrix}\\\\ \u0026amp;\\lambda_2=3\u0026amp;\u0026amp;\\mathbf{z}_2=\\begin{bmatrix} 1\\\\1 \\end{bmatrix} \\end{aligned}\\tag{15} \\]\nThe contour plot and 3-D plots are:\nAnother example:\n\\[ F(\\mathbf{x})=-\\frac{1}{4}x_1^2-\\frac{3}{2}x_1x_2-\\frac{1}{4}x_2^2 =\\frac{1}{2}\\mathbf{x}^T \\begin{bmatrix} -0.5\u0026amp;-1.5\\\\-1.5\u0026amp;-0.5 \\end{bmatrix}\\mathbf{x}\\tag{16} \\]\nwe can calculate the eigenvectors and eigenvalues:\n\\[ \\begin{aligned} \u0026amp;\\lambda_1=1\u0026amp;\u0026amp;\\mathbf{z}_1=\\begin{bmatrix} -1\\\\1 \\end{bmatrix}\\\\ \u0026amp;\\lambda_2=-2\u0026amp;\u0026amp;\\mathbf{z}_2=\\begin{bmatrix} -1\\\\-1 \\end{bmatrix} \\end{aligned}\\tag{17} \\]\nThe contour plot and 3-D plots is:\nConclusion \\(\\lambda_i\u0026gt;0\\) or \\(i=1,2,\\cdots\\), \\(F(x)\\) have a single strong minimum \\(\\lambda_i\u0026lt;0\\) or \\(i=1,2,\\cdots\\), \\(F(x)\\) have a single strong maximum \\(\\lambda_i\\) have both negative and positive together. \\(F(x)\\) has a saddle point \\(\\lambda_i\\geq 0\\) and have a \\(\\lambda_j=0\\), \\(F(x)\\) has a weak minimum or has no stationary point. \\(\\lambda_i\\leq 0\\) and have a \\(\\lambda_j=0\\), \\(F(x)\\) has a weak maximum or has no stationary point.  References  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","permalink":"https://anthony-tan.com/Quadratic-Functions/","summary":"Preliminaries Linear algebra Calculus 1,2 Taylor series  Quadratic Functions1 Quadratic function, a type of performance index, is universal. One of its key properties is that it can be represented in a second-order Taylor series precisely.\n\\[ F(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^TA\\mathbf{x}+\\mathbf{d}\\mathbf{x}+c\\tag{1} \\]\nwhere \\(A\\) is a symmetric matrix(if it is not symmetric, it can be easily converted into symmetric). And recall the property of gradient:\n\\[ \\nabla (\\mathbf{h}^T\\mathbf{x})=\\nabla (\\mathbf{x}^T\\mathbf{h})=\\mathbf{h}\\tag{2} \\]\nand\n\\[ \\nabla (\\mathbf{x}^TQ\\mathbf{x})=Q\\mathbf{x}+Q^T\\mathbf{x}=2Q\\mathbf{x}\\tag{3} \\]","title":"Quadratic Functions"},{"content":"Preliminaries Perceptron learning algorithm Hebbian learning algorithm Linear algebra  Neural Network Training Technique1 Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:\nPerformance Learning Associative Learning Competitive Learning  The linear associator we discussed in ‘Hebbian Learning’ is a kind of associative learning, which is used to build a connection between two events. Competitive learning is a kind of unsupervised learning in which nodes(neurons) of the neural network compete for the right to respond to a subset of the input data.2\nThe main topic we are going to discuss today is Performance Learning which is a widely used training method in neural network projects. By the way, the categories of learning can be classified in different ways. This is not the only kind of classification.\nPerformance Learning and Performance Index Training a network is to find suitable parameters for the model to meet our requirements for different tasks. If we can measure how suitable the neural network is for the task, we can then decide what to do next to modify the parameters. Performance learning is the procedure that modifies the parameters of neural networks by their performance based on certain measurements of the neural network performance.\nThe measurement of the performance we investigate here is called performance index and its appearance is called performance surface. What we should also be concerned about is the conditions for the existence of the minima or maxima. Because the minima or maxima decide the final result of the performance of the neural network for the task. In a word, what we need to do is to optimize the performance index by adjusting the parameters of the neural network with the information given by a training set(for different tasks).\nPerformance learning contains several different laws. And there are two common steps involved in the optimization process: 1. Define the ‘performance’ - a quantitative measure of network performance is called the performance index which has the properties when the neural network works well it has a lower value but when the neural network works poorly it has a larger value 2. Search parameters space to reduce the performance index\nThis is the heart of performance learning. We cloud also analyze the characteristics of the performance index before searching for the minima in the parameter space. Because, if the performance index we had selected did not meet the conditions of the existence of minima, searching for a minimum is just a waste of time. Or we can also set the additional condition to guarantee the existence of a minimum point.\nTaylor Series When we have a suitable performance index, what we need is a certain algorithm or a framework to deal with the optimization task. To design such a framework, we should study the performance index function first. Taylor series is a powerful tool to analyze the variation around a certain point of a function.\nScalar form function \\(F(x)\\) is an analytic function which means derivatives of \\(F(x)\\) exist everywhere. Then the Taylor series expansion of point \\(x^{\\star}\\) is:\n\\[ \\begin{aligned} F(x)=F(x^{\\star})\u0026amp;+\\frac{d}{dx}F(x)|_{x=x^{\\star}}(x-x^{\\star})\\\\ \u0026amp;+\\frac{1}{2}\\frac{d^2}{d^2x}F(x)|_{x=x^{\\star}}(x-x^{\\star})^2\\\\ \u0026amp;\\vdots \\\\ \u0026amp;+\\frac{1}{n!}\\frac{d^n}{d^nx}F(x)|_{x=x^{\\star}}(x-x^{\\star})^n \\end{aligned}\\tag{1} \\]\nThis series with infinity items can exactly equal the origin analytic function.\nIf we only want to approximate the function in a small region near \\(x^{\\star}\\), finite items in equation(1) are usually enough.\nFor instance, we have a function \\(F(x)=\\cos(x)\\) when \\(x^{\\star}=0\\), then:\n\\[ \\begin{aligned} F(x)=\\cos(x)\u0026amp;=\\cos(0)-\\sin(0)(x-0)-\\frac{1}{2}\\cos(0)(x-0)^2+\\cdots\\\\ \u0026amp;=1-\\frac{1}{2}x^2+\\frac{1}{24}x^4+\\cdots \\end{aligned}\\tag{2} \\]\n \\(0^{\\text{th}}\\) order approximation of \\(F(x)\\) near \\(0\\) is \\(F(x)\\approx F_0(x)=1\\) \\(1^{\\text{st}}\\) order approximation of \\(F(x)\\) near \\(0\\) is \\(F(x)\\approx F_1(x)=1+0\\) \\(2^{\\text{nd}}\\) order approximation of \\(F(x)\\) near \\(0\\) is \\(F(x)\\approx F_2(x)=1+0-\\frac{1}{2}x^2\\) \\(3^{\\text{rd}}\\) order approximation of \\(F(x)\\) near \\(0\\) is \\(F(x)\\approx F_3(x)=1+0-\\frac{1}{2}x^2+0\\) \\(4^{\\text{th}}\\) order approximation of \\(F(x)\\) near \\(0\\) is \\(F(x)\\approx F_4(x)=1+0-\\frac{1}{2}x^2+0+\\frac{1}{24}x^4\\)  Odd number\\(^{\\text{th}}\\) iterm is \\(0\\) because of the value of \\(\\sin(x)\\) at \\(0\\) is \\(0\\)\nAnd the \\(0^{\\text{th}},1^{\\text{st}},2^{\\text{nd}},3^{\\text{rd}}\\) and \\(4^{\\text{th}}\\) approximation of \\(F(x)\\) looks like:\nand, from the figure above we can observe that \\(F_0(x)\\) can only approximate \\(F(x)\\) at \\(x^{\\star}=0\\) point. While in the interval between \\(-1\\) and \\(+1\\), \\(F(x)\\) can be precisely approximated by \\(F_2(x)\\). In a more wider interval like \\([-1.6,+1.6]\\) to approximate \\(F(x)\\), \\(4^{\\text{th}}\\) order Taylor series are needed.\nThen we can get the conclusion that if we want a certain precise extent in a relatively larger interval we need more items in the Taylor series. And we should pay attention to the interval out of the precise region, such as \\(F_2(x)\\) in the interval \\((-\\infty,-1]\\cup [1,+\\infty)\\) are going away from \\(F(x)\\) as \\(x\\) moving away from \\(x^{\\star}=0\\)\nVector form function When the performance index is a function of a vector \\(\\mathbf{x}\\), a vector from the Taylor series should be presented. And in the neural network, parameters are variables of the performance index so the vector from the Taylor series is the basic tool in performance learning. Function \\(F(\\mathbf{x})\\) can be decomposed in any precise:\n\\[ \\begin{aligned} F(\\mathbf{x})=F(\\mathbf{x}^{\\star})\u0026amp; +\\frac{\\partial}{\\partial x_1}F(\\mathbf{x})|_{\\mathbf{x}=\\mathbf{x}^{\\star}}(x_1-x_1^{\\star})\\\\ \u0026amp;+\\frac{\\partial}{\\partial x_2}F(\\mathbf{x})|_{\\mathbf{x}=\\mathbf{x}^{\\star}}(x_2-x_2^{\\star})+\\cdots\\\\ \u0026amp;+\\frac{1}{2}\\frac{\\partial^2}{\\partial^2 x_1}F(\\mathbf{x})|_{\\mathbf{x}=\\mathbf{x}^{\\star}}(x_1-x_1^{\\star})^2\\\\ \u0026amp;+\\frac{1}{2}\\frac{\\partial^2}{\\partial x_1 \\partial x_2}F(\\mathbf{x})|_{\\mathbf{x}=\\mathbf{x}^{\\star}}(x_1-x_1^{\\star})(x_2-x_2^{\\star}) +\\cdots\\\\ \u0026amp;\\vdots \\end{aligned}\\tag{3} \\]\nif we notate the gradient as : \\[ \\nabla F(x)=\\begin{bmatrix} \\frac{\\partial F}{\\partial x_1}\\\\ \\frac{\\partial F}{\\partial x_2}\\\\ \\vdots\\\\ \\frac{\\partial F}{\\partial x_n} \\end{bmatrix}\\tag{4} \\]\nthen the Taylor series can be written as: \\[ \\begin{aligned} F(\\mathbf{x})=F(\\mathbf{x}^{\\star})\u0026amp;+(\\mathbf{x}-\\mathbf{x}^{\\star})^T\\nabla F(x)|_{\\mathbf{x}=\\mathbf{x}^{\\star}}\\\\ \u0026amp;+\\frac{1}{2}(\\mathbf{x}-\\mathbf{x}^{\\star})^T\\nabla^2 F(x)|_{\\mathbf{x}=\\mathbf{x}^{\\star}}(x_1-x_1^{\\star})\\\\ \u0026amp; \\vdots \\end{aligned}\\tag{5} \\]\nThe coefficients of the second-order item can be written in a matrix form, and it is also called the Hessian matrix:\n\\[ \\nabla^2 F(x) = \\begin{bmatrix} \\frac{\\partial^2}{\\partial^2 x_1}\u0026amp;\\cdots\u0026amp;\\frac{\\partial^2}{\\partial x_1 \\partial x_n}\\\\ \\vdots\u0026amp;\\ddots\u0026amp;\\vdots \\\\ \\frac{\\partial^2}{\\partial x_n \\partial x_1}\u0026amp;\\cdots\u0026amp;\\frac{\\partial^2}{\\partial^2 x_n} \\end{bmatrix}\\tag{6} \\]\nHessian matrix has many beautiful properties, such as it is always - Square matrix - Symmetric matrix\nIn the matrix, the elements on the diagonal are \\(\\frac{\\partial^2 F}{\\partial^2 x_i}\\) is the \\(2^{\\text{nd}}\\) derivative along the \\(x_i\\)-axis and in the gradient vector, \\(\\frac{d F}{d x_i}\\) is the first derivative along the \\(x_i\\)-axis.\nTo calculate the \\(1^{\\text{st}}\\) or \\(2^{\\text{nd}}\\) derivative along arbitrary deriction \\(\\mathbf{p}\\), we have: 1. \\(1^{\\text{st}}\\)-order derivative along \\(\\mathbf{p}\\) : \\(\\frac{\\mathbf{p}^T\\nabla F(\\mathbf{x})}{||\\mathbf{p}||}\\) 2. \\(2^{\\text{nd}}\\)-order derivative along \\(\\mathbf{p}\\) : \\(\\frac{\\mathbf{p}^T\\nabla^2 F(\\mathbf{x})\\mathbf{p}}{||\\mathbf{p}||^2}\\)\nFor instance, we have a function \\[ F(\\mathbf{x})=x_1^2+2x_2^2\\tag{7} \\]\nto find the derivative at \\(\\mathbf{x}^{\\star}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}\\) in the direction \\(\\mathbf{p}=\\begin{bmatrix}2\\\\-1\\end{bmatrix}\\), we get the derivative at \\(\\mathbf{x}^{\\star}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}\\):\n\\[ \\nabla F|_{\\mathbf{x}=\\mathbf{x}^{\\star}} = \\begin{bmatrix} \\frac{\\partial F}{\\partial x_1}\\\\ \\frac{\\partial F}{\\partial x_2} \\end{bmatrix}\\bigg|_{\\mathbf{x}=\\mathbf{x}^{\\star}}=\\begin{bmatrix} 2x_1\\\\ 4x_2 \\end{bmatrix}\\bigg|_{\\mathbf{x}=\\mathbf{x}^{\\star}}=\\begin{bmatrix} 1\\\\ 2 \\end{bmatrix}\\tag{8} \\]\nand the direction \\(\\mathbf{p}=\\begin{bmatrix}2\\\\-1\\end{bmatrix}\\), the derivative is\n\\[ \\frac{\\mathbf{p}^T\\nabla F(\\mathbf{x}^{\\star})}{||\\mathbf{p}||}= \\frac{\\begin{bmatrix}2\u0026amp;-1\\end{bmatrix} \\begin{bmatrix}1\\\\2\\end{bmatrix}}{\\sqrt{2^2+(-1)^2}}=\\frac{0}{\\sqrt{5}}=0\\tag{9} \\]\n\\(0\\) is a special number in the whole real numbers set. And the derivative is zero also meaningful in the optimization procedure. To find a zero slop direction we should solve the equation:\n\\[ \\frac{\\mathbf{p}^T\\nabla F(\\mathbf{x}^{\\star})}{||\\mathbf{p}||}=0\\tag{10} \\]\nthis means \\(\\mathbf{p}\\) can not be \\(\\mathbf{0}\\) because \\(0\\) length is illegle. And \\(\\mathbf{p}_{\\text{unit}}=\\frac{\\mathbf{p}}{||\\mathbf{p}||}\\) is a unit vector along deriction \\(\\mathbf{p}\\) so this can be written as:\n\\[ \\mathbf{p}_{\\text{unit}}^T\\nabla F(\\mathbf{x}^{\\star})=0\\tag{11} \\]\nwhich means \\(\\mathbf{p}_{\\text{unit}}\\) is orthogonal to gradient \\(\\nabla F(\\mathbf{x}^{\\star})\\)\nAnother special deriction is which one has the greatest slop. Assuming the deriction \\(\\mathbf{p}_{\\text{unit}}=\\frac{\\mathbf{p}}{||\\mathbf{p}||}\\) is the greatest slope, so:\n\\[ \\mathbf{p}_{\\text{unit}}^T\\nabla F(\\mathbf{x}^{\\star})=||\\mathbf{p}_{\\text{unit}}^T|| \\cdot ||\\nabla F(\\mathbf{x}^{\\star})||\\cos(\\theta)\\tag{12} \\]\nhas the greatest value. We know this can only happen when \\(\\cos(\\theta)=1\\) which means the direction of the gradient has the greatest slop.\nNecessary Conditions for Optimality The main objective of performance learning is to minimize the performance index. So the condition of existence of a minimum performance index should be investigated:\nA strong minimum \\(\\mathbf{x}\\) which mean in any deriction \\(\\mathbf{p}_{\\text{unit}}\\) around this point with a short distance \\(\\delta\\) always has \\(F(\\mathbf{x})\u0026lt;F(\\mathbf{x}+\\delta \\mathbf{p}_{\\text{unit}})\\), where \\(\\delta \\to 0\\) A weak minimum \\(\\mathbf{x}\\) which mean in any deriction \\(\\mathbf{p}_{\\text{unit}}\\) around this point with a short distance \\(\\delta\\) always has \\(F(\\mathbf{x})\\leq F(\\mathbf{x}+\\delta \\mathbf{p}_{\\text{unit}})\\), where \\(\\delta \\to 0\\) Global minimum is the minimum one of all weak or strong minimum sets.  For instance,\n\\[ F(x)=3x^4-7x^2-\\frac{1}{2}x+6\\tag{13} \\]\n 2 local minimums at near \\(x_1=-1.1\\) and near \\(x_2=1.1\\) near \\(x_2=1.1\\) gives the global minimum  If the variable of function is a vector:\n\\[ F(\\mathbf{x})=(x_2-x_1)^4+8x_1x_2-x_1+x_2+3\\tag{14} \\]\nand it looks like this:\nin 3-D space. And the contour plot of this function is:\nThere are three points in this contour figure:\nand the pink points are the two local minimums, and the black points are called a saddle points.\nThese two examples illustrate the minimum points and saddle points. But what conditions are needed to confirm the existence?\nFirst-order Conditions Go back to the Taylor series with \\(\\Delta x=x-x^{\\star}\\neq 0\\), the first item of the series is taken into account:\n\\[ \\begin{aligned} F(x)\u0026amp;=F(x^{\\star})+\\frac{d}{dx}F(x)|_{x=x^{\\star}}(x-x^{\\star})\\\\ F(x^{\\star}+\\Delta x)\u0026amp;=F(x^{\\star})+\\nabla F(x)\\Delta x \\end{aligned}\\tag{15} \\]\nwhen \\(x^{\\star}\\) is a candidate of minimum point, we want:\n\\[ F(x^{\\star}+\\Delta x)\\geq F(x^{\\star})\\tag{16} \\]\nso, in equation(15), \\[ \\nabla F(x)\\Delta x\\geq 0 \\tag{17} \\]\nare needed. Considering another direction, if \\(x^{\\star}\\) is the minimum point, it also has:\n\\[ F(x^{\\star}-\\Delta x)=F(x^{\\star})-\\nabla F(x)\\Delta x\\geq F(x^{\\star})\\tag{18} \\]\nthen\n\\[ \\nabla F(x)\\Delta x\\leq 0 \\tag{19} \\]\nTo satisfy both equation(17) and equation(19) if and only if\n\\[ \\nabla F(x)\\Delta x=0\\tag{20} \\]\nBecause \\(\\Delta x\\neq 0\\), so we must have \\(\\nabla F(x)\\) when \\(x\\) is the minimum point. \\(\\nabla F(x)=0\\) is a necessary but not sufficient condition. The first-order condition has been concluded above.\nSecond-order Condition The first-order condition does not give a sufficient condition. Now let’s consider the second-order condition. With \\(\\Delta\\mathbf{x}=\\mathbf{x}-\\mathbf{x}^{\\star}\\neq \\mathbf{0}\\) and gradient equal to \\(\\mathbf{0}\\), and take them into equation(5) . Then the second-order Taylor series is:\n\\[ F(\\mathbf{\\Delta\\mathbf{x}-\\mathbf{x}^{\\star}})=F(\\mathbf{x}^{\\star})+\\frac{1}{2}\\Delta\\mathbf{x}^T\\nabla^2 F(x)|_{\\mathbf{x}=\\mathbf{x}^{\\star}}\\Delta\\mathbf{x}\\tag{21} \\]\nWhen \\(||\\Delta\\mathbf{x}||\\) is small, zero-order, first-order, and second-order terms of the Taylor series are precise enough to approximate the original function. When \\(F(\\mathbf{x}^{\\star})\\) is a strong minimum, the second-order item should be:\n\\[ \\frac{1}{2}\\Delta\\mathbf{x}^T\\nabla^2 F(x)|_{\\mathbf{x}=\\mathbf{x}^{\\star}}\\Delta\\mathbf{x}\u0026gt;0 \\]\nRecalling linear algebra knowledge, positive definite matrix \\(A\\) is:\n\\[ \\mathbf{z}^T A \\mathbf{z}\u0026gt;0 \\text{ for any } \\mathbf{z} \\]\nand positive semidefinite matrix \\(A\\) is:\n\\[ \\mathbf{z}^T A \\mathbf{z}\\geq0 \\text{ for any } \\mathbf{z} \\]\nSo when the gradient is \\(\\mathbf{0}\\) and the second-order derivative is positive definite(semidefinite), this point is a strong(weak) minimum. And positive definite Hessian matrix is a sufficient condition to a strong minimum point, but it’s not a necessary condition. Because when the Hessian matrix is \\(0\\) the third-order item has to be calculated.\nReferences  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n https://en.wikipedia.org/wiki/Competitive_learning↩︎\n   ","permalink":"https://anthony-tan.com/Performance-Surfaces-and-Optimum-Points/","summary":"Preliminaries Perceptron learning algorithm Hebbian learning algorithm Linear algebra  Neural Network Training Technique1 Several architectures of the neural networks had been introduced. And each neural network had its own learning rule, like, the perceptron learning algorithm, and the Hebbian learning algorithm. When more and more neural network architectures were designed, some general training methods were necessary. Up to now, we can classify all training rules in three categories in a general way:","title":"Performance Surfaces and Optimum Points"},{"content":"Preliminaries Linear algebra  Hebb Rule1 Hebb rule is one of the earliest neural network learning laws. It was published in 1949 by Donald O. Hebb, a Canadian psychologist, in his work ’ The Organization of Behavior’. In this great book, he proposed a possible mechanism for synaptic modification in the brain. And this rule then was used in training the artificial neural networks for pattern recognition.\n’ The Organization of Behavior’ The main premise of the book is that behavior could be explained by the action of a neuron. This was a relatively different idea at that time when the dominant concept is the correlation between stimulus and response by psychologists. This could also be considered a philosophy battle between ‘top-down’ and ‘down-top’.\n “When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased”\n This is a physical mechanism for learning at the cellular level. Dr. Hebb thought if two nerve cells were closed enough and they seemed related that was both of them fired simultaneously in high frequency. Then the connection between them would be strengthened. However, at that time, Dr. Hebb did not give firm evidence of his theory. The subsequent research in the next few years did prove the existence of this strengthening.\nHebb’s postulate is not completely new, because some similar ideas had been proposed before. However, Dr. Hebb gave a more systematic one.\nLinear Associator The first use of Hebb’s learning rule in artificial neural networks is a linear associator. Here is the simplest example of this neural network to illustrate the concept of Hebb’s postulate. A more complex architecture may drag us into the mire and miss the key points of the learning rule itself.\nLinear associator was proposed by James Anderson and Teuwo Kohonen in 1972 independently. And its abbreviated notation is:\nThis architecture consists of a layer of \\(S\\) neurons each of which has \\(R\\) inputs and their transfer functions of them are all linear functions. So the output of this architecture can be simply calculated:\n\\[ a_i=\\sum_{j=1}^{R}w_{ij}p_j\\tag{1} \\]\nwhere the \\(i\\) th component of the output vector is given by the summation of all inputs weighted by the weights of the connections between the input and the \\(i\\) th neuron. Or, it also can be written in a matrix form:\n\\[ \\mathbf{a}=\\mathbf{W}\\mathbf{p}\\tag{2} \\]\nAssociative memory is to learn \\(Q\\) pairs of prototype input/output vectors:\n\\[ \\{\\mathbf{p}_1,\\mathbf{t}_1\\},\\{\\mathbf{p}_2,\\mathbf{t}_2\\},\\cdots,\\{\\mathbf{p}_Q,\\mathbf{t}_Q\\}\\tag{3} \\]\nthen the associator will output \\(\\mathbf{a}=\\mathbf{t}_i\\) crospending to input \\(\\mathbf{p}=\\mathbf{p}_i\\) for \\(i=1,2,\\cdots Q\\). And when slight change of input occured(i.e. \\(\\mathbf{p}=\\mathbf{p}_i+\\delta\\)), output should also change slightly( i.e. \\(\\mathbf{a}=\\mathbf{t}_i+\\varepsilon\\)).\nReview Hebb rule: “if neurons on both sides of the synapse are activated simultaneously, the strength of the synapse will increase”. And considering \\(\\mathbf{a}_i=\\sum_{j=1}^{R}\\mathbf{w}_{ij}\\mathbf{p}_j\\) where \\(\\mathbf{w}_{ij}\\) is the weight between input \\(\\mathbf{p}_j\\) and output \\(\\mathbf{a}_i\\) so the mathematical Hebb rule is: \\[ w_{ij}^{\\text{new}} = w_{ij}^{\\text{old}} + \\alpha f_i(a_{iq}) g_j(p_{jq})\\tag{4} \\] where: - \\(q\\): the identification of training data - \\(\\alpha\\): learning rate\nThis mathematical model uses two functions \\(f\\) and \\(g\\) to map raw input and output into suitable values and then multiply them as an increment to the weight of the connection. These two actual functions are not known for sure, so writing the functions as linear functions is also reasonable. Then we have the simplified form of Hebb’s rule:\n\\[ w_{ij}^{\\text{new}} = w_{ij}^{\\text{old}} + \\alpha a_{iq} p_{jq} \\tag{5} \\]\nA learning rate of \\(\\alpha\\) is necessary. Because it can be used to control the process of update of weights.\nThe equation(5) does not only represent Hebb’s rule that the connection would increase when both sides of the synapses are active but also give other increments of connection when both sides of the synapses are negative. This is an extension of Hebb’s rule which may have no biological fact to support it.\nIn this post, we talk about the only supervised learning of Hebb’s rule. However, there is also an unsupervised version of Hebb’s rule which will be investigated in another post.\nRecall that we have a training set:\n\\[ \\{(\\mathbf{p}_1,\\mathbf{t}_1),(\\mathbf{p}_2,\\mathbf{t}_2),\\cdots,(\\mathbf{p}_Q,\\mathbf{t}_Q)\\}\\tag{6} \\]\nHebb’s postulate states the relationship between the outputs and the inputs. However, the outputs sometimes are not the correct response to inputs in some tasks. And as we know, in a supervised learning task correct outputs which are also called targets are given. So we replace the output of the model \\(a_{iq}\\) in equation(5) with the known correct output(target) \\(t_{iq}\\), so the supervised learning form of Hebb’s rule is: \\[ w_{ij}^{\\text{new}} = w_{ij}^{\\text{old}} + \\alpha t_{iq} p_{jq} \\tag{7} \\]\nwhere: - \\(t_{iq}\\) is the \\(i\\) th element of \\(q\\)th target \\(\\mathbf{t}_q\\) - \\(p_{jq}\\) is the \\(j\\) th element of \\(q\\)th input \\(\\mathbf{p}_q\\)\nof course, it also has a matrix form:\n\\[ \\mathbf{W}^{\\text{new}}=\\mathbf{W}^{\\text{old}}+\\alpha\\mathbf{t}_q\\mathbf{p}_q^T\\tag{8} \\]\nIf we initial \\(\\mathbf{W}=\\mathbf{0}\\) , we would get the final weight matrix for the training set: \\[ \\mathbf{W}=\\mathbf{t}_1\\mathbf{p}_1^T+\\mathbf{t}_2\\mathbf{p}_2^T+\\cdots+\\mathbf{t}_Q\\mathbf{p}_Q^T=\\sum_{i=1}^{Q}\\mathbf{t}_i\\mathbf{p}_i^T\\tag{9} \\] or in a matrix form: \\[ \\mathbf{W}=\\begin{bmatrix} \\mathbf{t}_1,\\mathbf{t}_2,\\cdots,\\mathbf{t}_Q \\end{bmatrix}\\begin{bmatrix} \\mathbf{p}_1^T\\\\ \\mathbf{p}_2^T\\\\ \\vdots\\\\ \\mathbf{p}_Q^T \\end{bmatrix}=\\mathbf{T}\\mathbf{P}^T\\tag{10} \\]\nPerformance Analysis Now let’s go into the inside of the linear associator mathematically. Mathematical analysis or mathematical proof can bring us strong confidence in the following implementation of Hebb’s rule.\n\\(\\mathbf{p}_q\\) are Orthonormal Firstly, Considering the most special but simple case, when all inputs \\(\\mathbf{p}_q\\) are orthonormal which means orthogonal mutually and having a unit length. Then with equation(10) the output corresponding to the input \\(\\mathbf{P}_q\\) can be computed:\n\\[ \\mathbf{a}=\\mathbf{W}\\mathbf{p}_k=(\\sum^{Q}_{q=1}\\mathbf{t}_q\\mathbf{p}_q^T)\\mathbf{p}_k=\\sum^{Q}_{q=1}\\mathbf{t}_q(\\mathbf{p}_q^T\\mathbf{p}_k)\\tag{11} \\]\nfor we have supposed that \\(\\mathbf{p}_q\\) are orthonormal which means:\n\\[ \\mathbf{p}_q^T\\mathbf{p}_k=\\begin{cases} 1\u0026amp;\\text{ if }q=k\\\\ 0\u0026amp;\\text{ if }q\\neq k \\end{cases}\\tag{12} \\]\nfrom equation(11) and equation(12), we confirm that weights matrix \\(\\mathbf{W}\\) built by Hebb’s postulate gives the right outputs when inputs are orthonormal.\nThe conclusion is that if input prototype vectors are orthonormal, Hebb’s rule is correct.\n\\(\\mathbf{p}_q\\) are Normal but not Orthogonal More generally case is \\(\\mathbf{p}_q\\) is not Orthogonal. And before putting them into an algorithm, we can convert every prototype vector into unit length without changing their directions. Then we have:\n\\[ \\mathbf{a}=\\mathbf{W}\\mathbf{p}_k=(\\sum^{Q}_{q=1}\\mathbf{t}_q\\mathbf{p}_q^T)\\mathbf{p}_k=\\sum^{Q}_{q=1}\\mathbf{t}_q(\\mathbf{p}_q^T\\mathbf{p}_k)=\\mathbf{t}_k+\\sum_{q\\neq k}\\mathbf{t}_q(\\mathbf{p}_q^T\\mathbf{p}_k)\\tag{13} \\]\nFor us the vectors are normal but not orthogonal:\n\\[ \\mathbf{t}_q\\mathbf{p}_q^T\\mathbf{p}_k=\\begin{cases} \\mathbf{t}_q \u0026amp; \\text{ when } q = k\\\\ \\mathbf{t}_q\\mathbf{p}_q^T\\mathbf{p}_k \u0026amp; \\text{ when } q \\nsupseteq k \\end{cases}\\tag{14} \\]\nthen equation(13) can be also written as:\n\\[ \\mathbf{a}=\\mathbf{t}_k+\\sum_{q\\neq k}\\mathbf{t}_q(\\mathbf{p}_q^T\\mathbf{p}_k)\\tag{15} \\]\nif we want to produce the outputs of the linear associator as close as the targets, \\(\\sum_{q\\neq k}\\mathbf{t}_q(\\mathbf{p}_q^T\\mathbf{p}_k)\\) should be as small as possible.\nAn example, when we have the training set:\n\\[ \\{\\mathbf{p}_1=\\begin{bmatrix}0.5\\\\-0.5\\\\0.5\\\\-0.5\\end{bmatrix},\\mathbf{t}_1=\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\}, \\{\\mathbf{p}_2=\\begin{bmatrix}0.5\\\\0.5\\\\-0.5\\\\-0.5\\end{bmatrix},\\mathbf{t}_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\} \\]\nthen the weight matrix can be calculated:\n\\[ \\begin{aligned} \\mathbf{W}=\\mathbf{T}\\mathbf{P}^T=\\begin{bmatrix} \\mathbf{t}_1\u0026amp;\\mathbf{t}_2 \\end{bmatrix}\\begin{bmatrix} \\mathbf{p}_1^T\\\\\\mathbf{p}_2^T \\end{bmatrix} \u0026amp;= \\begin{bmatrix} 1\u0026amp;1\\\\ -1\u0026amp;1 \\end{bmatrix}\\begin{bmatrix} 0.5\u0026amp;-0.5\u0026amp;0.5\u0026amp;-0.5\\\\ 0.5\u0026amp;0.5\u0026amp;-0.5\u0026amp;-0.5 \\end{bmatrix}\\\\\u0026amp;=\\begin{bmatrix} 1\u0026amp;0\u0026amp;0\u0026amp;-1\\\\ 0\u0026amp;1\u0026amp;-1\u0026amp;0 \\end{bmatrix}\\end{aligned} \\]\nwe can, now, test these two inputs:\n\\(\\mathbf{a}_1=\\mathbf{W}\\mathbf{p}_1=\\begin{bmatrix}1\u0026amp;0\u0026amp;0\u0026amp;-1\\\\0\u0026amp;1\u0026amp;-1\u0026amp;0\\end{bmatrix}\\begin{bmatrix}0.5\\\\-0.5\\\\0.5\\\\-0.5\\end{bmatrix}=\\begin{bmatrix}1\\\\-1\\end{bmatrix}=\\mathbf{t}_1\\) Correct! \\(\\mathbf{a}_2=\\mathbf{W}\\mathbf{p}_2=\\begin{bmatrix}1\u0026amp;0\u0026amp;0\u0026amp;-1\\\\0\u0026amp;1\u0026amp;-1\u0026amp;0\\end{bmatrix}\\begin{bmatrix}0.5\\\\0.5\\\\-0.5\\\\-0.5\\end{bmatrix}=\\begin{bmatrix}1\\\\1\\end{bmatrix}=\\mathbf{t}_2\\) Correct!  Another example is when we have the training set:\n\\[ \\{\\mathbf{p}_1=\\begin{bmatrix}1\\\\-1\\\\-1\\end{bmatrix},\\text{orange}\\}, \\{\\mathbf{p}_2=\\begin{bmatrix}1\\\\1\\\\-1\\end{bmatrix},\\text{apple}\\} \\]\nfirstly we convert target ‘apple’ and ‘orange’ into numbers: - orange \\(\\mathbf{t}_1=\\begin{bmatrix}-1\\end{bmatrix}\\) - apple \\(\\mathbf{t}_1=\\begin{bmatrix}1\\end{bmatrix}\\)\nsecondly, we normalize the input vector that would make them have a unit length:\n\\[ \\{\\mathbf{p}_1=\\begin{bmatrix}0.5774\\\\-0.5774\\\\0.5774\\end{bmatrix},\\mathbf{t}_1=\\begin{bmatrix}-1\\end{bmatrix}\\}, \\{\\mathbf{p}_2=\\begin{bmatrix}0.5774\\\\0.5774\\\\-0.5774\\end{bmatrix},\\mathbf{t}_1=\\begin{bmatrix}1\\end{bmatrix}\\} \\]\nthen the weight matrix can be calculated:\n\\[ \\begin{aligned} \\mathbf{W}=\\mathbf{T}\\mathbf{P}^T=\\begin{bmatrix} \\mathbf{t}_1\u0026amp;\\mathbf{t}_2 \\end{bmatrix}\\begin{bmatrix} \\mathbf{p}_1^T\\\\\\mathbf{p}_2^T \\end{bmatrix} \u0026amp;= \\begin{bmatrix} -1\u0026amp;1 \\end{bmatrix}\\begin{bmatrix} 0.5774\u0026amp;-0.5774\u0026amp;-0.5774\\\\ 0.5774\u0026amp;0.5774\u0026amp;-0.5774 \\end{bmatrix}\\\\\u0026amp;=\\begin{bmatrix} 0\u0026amp;1.1548\u0026amp;0 \\end{bmatrix} \\end{aligned} \\]\nwe can, now, test these two inputs:\n\\(\\mathbf{a}_1=\\mathbf{W}\\mathbf{p}_1=\\begin{bmatrix} 0\u0026amp;1.1548\u0026amp;0\\end{bmatrix}\\begin{bmatrix}0.5774\\\\-0.5774\\\\-0.5774\\end{bmatrix}=\\begin{bmatrix}-0.6668\\end{bmatrix}=\\mathbf{t}_1\\) Correct! \\(\\mathbf{a}_2=\\mathbf{W}\\mathbf{p}_2=\\begin{bmatrix} 0\u0026amp;1.1548\u0026amp;0\\end{bmatrix}\\begin{bmatrix}0.5774\\\\0.5774\\\\-0.5774\\end{bmatrix}=\\begin{bmatrix}0.6668\\end{bmatrix}\\)  \\(\\mathbf{a}_1\\) is closer to \\([-1]\\) than \\([1]\\) so it belongs to \\(\\mathbf{t}_1\\) And \\(\\mathbf{a}_2\\) is closer to \\([1]\\) than \\([-1]\\) so it belongs to \\(\\mathbf{t}_2\\). So, the algorithm gives an output close to the correct target.\nThere is another kind of algorithm that can deal with the task correctly rather than closely, for instance, the pseudoinverse rule can come up with another \\(\\mathbf{W}^{\\star}\\) which can give the correct answer to the above question.\nApplication of Hebb Learning An application is proposed here. We have 3 inputs and outputs:\nThey are \\(5\\times 6\\) pixels images that have only white and black pixels.\nWe then read the image and convert the white and black into \\(\\{1,-1\\}\\) so the ‘zero’ image change into the matrix: \\[ \\begin{aligned} \\{\u0026amp;\\\\ \u0026amp;-1,1,1,1,-1,\\\\ \u0026amp;1,-1,-1,-1,1,\\\\ \u0026amp;1,-1,-1,-1,1,\\\\ \u0026amp;1,-1,-1,-1,1,\\\\ \u0026amp;1,-1,-1,-1,1,\\\\ \u0026amp;-1,1,1,1,-1\\\\ \\}\u0026amp; \\end{aligned} \\]\nwe use the inputs as the target then the neuron network architecture become(transfer function is the hard limit):\nfollowing the algorithm we summarized above we got the code:\n# part of code class HebbLearning():   def __init__(self, training_data_path=\u0026#39;./data/train\u0026#39;, gamma=0, alpha=0.5):  \u0026quot;\u0026quot;\u0026quot; initial function :param training_data_path: the path of the training set and their labels. They should be organized as pictures in \u0026#39;.png\u0026#39; form :param gamma: the punishment coefficients :param alpha: learning rate \u0026quot;\u0026quot;\u0026quot;  self.gamma = gamma  self.alpha = alpha  x = self.load_data(training_data_path)  self.X = np.array(x)  self.label = np.array(x)  self.weights = np.zeros((np.shape(x)[1], np.shape(x)[1]))  self.test_data = []   def load_data(self, data_path):  \u0026quot;\u0026quot;\u0026quot; load image data and transfer it into matrix form :param data_path: the path of data :return: a training set and targets respectively \u0026quot;\u0026quot;\u0026quot;  name_list = os.listdir(data_path)  X = []  for file_name in name_list:  data = cv2.imread(os.path.join(data_path, file_name), 0)  if data is None:  continue  else:  data = data.reshape(1, -1)[0].astype(np.float64)  for i in range(len(data)):  if data[i] \u0026gt; 0:  data[i] = 1  else:  data[i] = -1  data=data/np.linalg.norm(data)  X.append(data)  return X   def process(self):  \u0026quot;\u0026quot;\u0026quot; Comput weights using the Hebb learning function :return: \u0026quot;\u0026quot;\u0026quot;  for x, label in zip(self.X, self.label):  self.weights = self.weights + self.alpha * np.dot(label.reshape(-1,1), x.reshape(1,-1)) - self.gamma*self.weights   def test(self, input_path=\u0026#39;./data/test\u0026#39;):  \u0026quot;\u0026quot;\u0026quot; test function used to test a given input use the linear associator :param input_path: test date should be organized as pictures whose names are their label :return: output label and \u0026quot;\u0026quot;\u0026quot;  self.test_data = self.load_data(input_path)   labels_test = []  for x in self.test_data:  output_origin = np.dot(self.weights,x.reshape(-1,1))  labels_test.append(symmetrical_hard_limit(output_origin))  return np.array(labels_test) The whole project can be found: https://github.com/Tony-Tan/NeuronNetworks/tree/master/supervised_Hebb_learning please, Star it!\nThe algorithm gives the following result(left: input; right: output):\nIt looks like you have associate memory.\nSome Variations of Hebb Learning Derivate rules of Hebb learning are developed. And they overcome the shortage of Hebb learning algorithms, like:\n Elements of \\(\\mathbf{W}\\) would grow bigger when more prototypes are provided.\n To overcome this problem, a lot of ideas came into mind:\nLearning rate \\(\\alpha\\) can be used to slow down this phenomina Adding a decay term, so the learning rule is changed into a smooth filter: \\(\\mathbf{W}^{\\text{new}}=\\mathbf{W}^{\\text{old}}+\\alpha\\mathbf{t}_q\\mathbf{p}_q^T-\\gamma\\mathbf{W}^{\\text{old}}\\) which can also be written as \\(\\mathbf{W}^{\\text{new}}=(1-\\gamma)\\mathbf{W}^{\\text{old}}+\\alpha\\mathbf{t}_q\\mathbf{p}_q^T\\) where \\(0\u0026lt;\\gamma\u0026lt;1\\) Using the residual between output and target to multipy input as the increasement of the weights: \\(\\mathbf{W}^{\\text{new}}=\\mathbf{W}^{\\text{old}}+\\alpha(\\mathbf{t}_q-\\mathbf{a}_q)\\mathbf{p}_q^T\\)  The second idea, when \\(\\gamma\\to 1\\) the algorithm quickly forgets the old weights. but when \\(\\gamma\\to 0\\) the algorithm goes back to the standard form. This idea of filter would be widely used in the following algorithms. The third method also known as the Widrow-Hoff algorithm, could minimize mean square error as well as minimize the sum of the square error. And this algorithm also has another advantage which is the update of the weights step by step whenever the prototype is provided. So it can quickly adapt to the changing environment while some other algorithms do not have this feature.\nReferences  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","permalink":"https://anthony-tan.com/Supervised-Hebbian-Learning/","summary":"Preliminaries Linear algebra  Hebb Rule1 Hebb rule is one of the earliest neural network learning laws. It was published in 1949 by Donald O. Hebb, a Canadian psychologist, in his work ’ The Organization of Behavior’. In this great book, he proposed a possible mechanism for synaptic modification in the brain. And this rule then was used in training the artificial neural networks for pattern recognition.\n’ The Organization of Behavior’ The main premise of the book is that behavior could be explained by the action of a neuron.","title":"Supervised Hebbian Learning"},{"content":"Preliminaries An Introduction to Neural Networks Neuron Model and Network Architecture Perceptron Learning Rule  Implement of Perceptron1 What we need to do next is to implement the algorithm described in ‘Perceptron Learning Rule’ and observe the effect of 1. different parameters, 2. different training sets, 3. and different transfer functions.\nA single neuron perceptron consists of a linear combination and a threshold operation simply. So we note its capacity is close to a linear classification. Then our experiments should be done with samples that are linearly separable.\nBefore our experiments, the first mission that comes to us is to find some or design some training set. That is the data like:\n\\[ \\{(\\begin{bmatrix}1\\\\2\\end{bmatrix},1),(\\begin{bmatrix}-1\\\\2\\end{bmatrix},0),(\\begin{bmatrix}0\\\\-1\\end{bmatrix},0)\\} \\]\ncontaining inputs(2-dimensional) and corresponding targets. This training set containing 3 points can be generated by hand, but when we need more than 100 points, we need a tool.\nGenerating of Sample The tool can be found at https://github.com/Tony-Tan/2DRandomSampleGenerater. And please star it!\nThis script used an image as input, and a different color in the image means different classes and the regions should be of the closed forms, like:\nAn image of two classes is fed into the script. And a ’*.csv’ file will be generated. Part of the file is like this:\nwhere ‘x_1’ and ‘x_2’ are inputs and ‘Label’ is the target. For the scripts that can generate multi-class training data, the label starts with \\(1\\) to \\(2,3,4,\\dots\\). So if you want to reassign the target, it should be done in your program.\nWe can also draw a figure of these points and use a different color to identify their classes:\nThe use of script:\nAlgorithm and Code Firstly, we assume we have had a training set that is 2-dimensional linear separable. And our algorithm is:\ninitial weights and bias randomly test inputs one by one,   if the output is different from the target, go to step 3 if the whole training set is correctly classified, stop the algorithm. else continue to step 2.  update \\(\\mathbf{w}=\\mathbf{w}+l\\cdot (t-a)\\cdot \\mathbf{p}\\)   \\(\\mathbf{w}\\) is the weights \\(\\mathbf{p}\\) is the input, \\(t\\) is target can be one of \\(\\{0,1\\}\\), \\(a\\) is the output, one of \\(\\{0,1\\}\\), \\(l\\) is the learning rate, should be \\(0\u0026lt; l \\leq 1\\)  update \\(b=b+l\\cdot (t-a)\\cdot 1\\)   \\(b\\) is the bias \\(t\\) is target can be one of \\(\\{0,1\\}\\), \\(a\\) is the output, can be one of \\(\\{0,1\\}\\), \\(l\\) is the learning rate, should be \\(0\u0026lt; l \\leq 1\\)  go back to 2.  The whole code can be found: https://github.com/Tony-Tan/NeuronNetworks/tree/master/simple_perceptron, and the main part of the code is:\ndef convergence_algorithm(self, lr=1.0, max_iteration_num = 1000000):  for epoch in range(max_iteration_num):   all_inputs_predict_correctly = True  for x, label in zip(self.X, self.labels):  predict_value = self.process(x)  if predict_value == label:  continue  else:  all_inputs_predict_correctly = False  self.weights += lr*(label - predict_value)* x  print(str(epoch) + str(\u0026#39; : \u0026#39;) + str(self.weights[0]))   if all_inputs_predict_correctly:  return True Training set 4 in the project is shown below.\nTraining set 4 Points in the training set\nResult(after 55449 epochs)\nAnalysis of the Algorithm What we are concerned about the algorithm are its precision and efficiency. Because the convergence of the algorithm had been proved, what we are interested in is how to speed up the convergence.\nEpoch is a period when every point in the dataset is used once and only once.\nRandomize the input sequence After 100 times test, we got the number of epochs of random order of the training set used in the algorithm, and the number of epochs of the original order of the training set where the first half part of the dataset belongs to one class, and the last half part belongs to another class\"\nThe following are the result of more training data than above.\nInitial Weights Weights can be initialed in multiple ways: 1. \\(\\mathbf{w}=\\mathbf{0}\\) 2. \\(\\mathbf{w}=\\mathbf{p}_i\\), \\(\\mathbf{p}_i\\) is randomly selected from the training set. 3. \\(\\mathbf{w}=\\frac{1}{N}\\sum_{i=0}^{N}\\mathbf{p}_i\\) 4. randomly in (-1000,1000) for every element.\nThese four different selections’ comparison of epochs they used: and total modification they used:\nDifferent Transfer Functions We test: 1. hard limit 2. symmetrical hard limit 3. positive linear 4. hyperbolic tangent sigmoid 5. log sigmoid 6. symmetric saturating linear 7. saturating linear 8. linear\nThese eight different Transfer Functions’ comparison of epochs they used: and total modification they used: Conclusion The random sequence is far better than the original order dataset which distributes different classes of data separately and randomly Initial methods can affect the convergence speed but not too much. Setting weights as one of the input \\(p_i\\) seems good. The tool to generate samples should be staredhttps://github.com/Tony-Tan/2DRandomSampleGenerater  References  Haykin, Simon. Neural Networks and Learning Machines, 3/E. Pearson Education India, 2010.↩︎\n   ","permalink":"https://anthony-tan.com/Implement-of-Perceptron/","summary":"Preliminaries An Introduction to Neural Networks Neuron Model and Network Architecture Perceptron Learning Rule  Implement of Perceptron1 What we need to do next is to implement the algorithm described in ‘Perceptron Learning Rule’ and observe the effect of 1. different parameters, 2. different training sets, 3. and different transfer functions.\nA single neuron perceptron consists of a linear combination and a threshold operation simply. So we note its capacity is close to a linear classification.","title":"Implement of Perceptron"},{"content":"Preliminaries supervised learning unsupervised learning reinforcement learning ‘An Introduction to Neural Networks’  Learning Rules1 We have built some neural network models in the post ‘An Introduction to Neural Networks’ and as we know architectures and learning rules are two main aspects of designing a useful network. The architectures we have introduced could not be used yet. What we are going to do is to investigate the learning rules for different architectures.\nThe learning rule is a procedure to modify weights, bias, and other parameters in the model to perform a certain task. There are generally 3 types of learning rules:\nSupervised Learning   There is a training set of the model that contains a number of input data as well as correct output values that are also known as targets, like the set \\(\\{\\mathbf{p}_1,t_1\\},\\{\\mathbf{p}_2,t_2\\},\\cdots,\\{\\mathbf{p}_Q,t_Q\\},\\) where \\(\\mathbf{p}_i\\) (for \\(i\u0026lt;Q\\)) is the input of the model and \\(t_i\\)(for \\(i\u0026lt;Q\\)) is the corresponding target(any kind of output, like labels, regression values) the whole process is:  where target and current output are used to modify the neuron network to produce an output as close to the target as possible according to the input.  Unsupervised Learning   Unlike supervised learning, unsupervised learning doesn’t know the correct output at all, in other words, what the neuron network knows is only the inputs, and how to modify the parameters in the model is depend only on the inputs. This kind of learning is always used to perform clustering operations or vector quantization.   Reinforcement learning   it is another learning rule which is more like supervised learning and works more like a biological brain. there is no correct target as well but there is a grade to measure the performance of the network over some sequence of input.   Perceptron Architecture A perceptron is an easy model of a neuron, and what we are interested in is how to design a learning rule or develop a certain algorithm to make a perceptron possible to classify input signals. Perceptron is the easiest neuron model, and it is a good basis for more complex networks.\nIn a perceptron, it has weights, biases, and transfer functions. These are basic components of a neuron model. However, the transfer function here is specialized as a ‘hard limit function’\n\\[ f(x) = \\begin{cases} 0\u0026amp; \\text{ if } x\u0026lt;0\\\\ 1\u0026amp; \\text{ if } x\\geq 0 \\end{cases}\\tag{1} \\]\nand abbreviated notation with a layer of \\(S\\) neuron network is:\nwhere \\(W\\) are the weight matrix of \\(S\\) perceptrons. And each row of the matrix contains all the weight of a perceptron.\nSingle-Neuron Perceptron In some simple models, we can visualize a line or a plane named ‘decision boundary’ which is determined by the input vectors who make the input of its transfer function \\(n=0\\). For instance, in a 2-dimensional linear classification task, we were asked to find a line that can separate the input points into two different classes. This line here acts as a decision boundary and all the points on the line would produce \\(0\\) output through the linear model.\nLet’s start to study with one perceptron.\nIts decision boundary is the line:\n\\[ n=w_{1,1}p_1+w_{1,2}p_2+b=0\\tag{2} \\]\nFor example, when \\(w_{1,1}=1\\), \\(w_{1,2}=1\\) and \\(b=-1\\), we get the decision boundary:\n\\[ p_1+p_2-1=0\\tag{3} \\]\nfrom the figure, we can conclude the weight vector has the following properties:\n\\(W\\) always points to the purple region where \\(n=w_{1,1}p_1+w_{1,2}p_2+b\u0026gt;0\\) The relation of \\(W\\) and the direction of the decision boundary is orthogonality.  Can this simple example perform some task? Of course. If we test the input \\(\\mathbf{p}_1=\\begin{bmatrix}0\u0026amp;0\\end{bmatrix}^T,\\mathbf{p}_1=\\begin{bmatrix}0\u0026amp;1\\end{bmatrix}^T,\\mathbf{p}_1=\\begin{bmatrix}1\u0026amp;0\\end{bmatrix}^T,\\mathbf{p}_1=\\begin{bmatrix}1\u0026amp;1\\end{bmatrix}^T\\) respectively, we could get \\(a_1=0,a_2=0,a_3=0,a_4=1\\). This is the ‘and’ operation in logical calculation.\n   input output     \\(\\begin{bmatrix}0\u0026amp;0\\end{bmatrix}^T\\) \\(0\\)   \\(\\begin{bmatrix}0\u0026amp;1\\end{bmatrix}^T\\) \\(0\\)   \\(\\begin{bmatrix}1\u0026amp;0\\end{bmatrix}^T\\) \\(0\\)   \\(\\begin{bmatrix}1\u0026amp;1\\end{bmatrix}^T\\) \\(1\\)    Multiple-Neuron Perceptron A multiple neuron perceptron is just a combination of some perceptrons, whose weight matrix \\(W\\) has multiple rows. And the transpose of row \\(i\\) of the \\(W\\) is notated as \\(_iW\\)(column form of the \\(i\\)th row in \\(W\\)) then the \\(i\\) th neuron has the decision boundary:\n\\[ _iW^T\\mathbf{p}+b_i=0\\tag{4} \\]\nFor the property of hard limit function that the output could just be one of \\(\\{0,1\\}\\). And for \\(S\\) neurons, there are at most \\(2^S\\) categories, that is to say to a \\(S\\) neuron perceptron in one layer it is impossible to solve the problems containing more than \\(2^S\\) classes.\nPerceptron Learning Rule Perceptron learning rule here is a kind of supervised learning rule. Recall some results above: 1. supervised learning had both input and corresponding correct target. 2. target, the output produced by the current model, and input produced the information on how to modify the model 3. \\(W\\) always points to the region where the output is \\(a=1\\)\nConstructing Learning Rules With the above results we try to design the rule and assume training dates are:\n\\[ \\{\\begin{bmatrix}1\\\\2\\end{bmatrix},1\\},\\{\\begin{bmatrix}-1\\\\2\\end{bmatrix},0\\},\\{\\begin{bmatrix}0\\\\-1\\end{bmatrix},0\\}\\tag{5} \\]\nAssigning Some Initial Values To modify the model, we need the output which is produced by the weights and input. And for this supervised learning algorithm we have both input and correct outputs (targets), what we need to do is just assign some values to weights(here we omit the bias). Like:\n\\[ _1W=\\begin{bmatrix}1.0\\\\-0.8\\end{bmatrix}\\tag{6} \\]\nCalculating output The output is easy to be calculated: \\[ a=f(_1W^T\\mathbf{p}_1)\\tag{7} \\]\nwhen \\(\\mathbf{p}_1=\\begin{bmatrix}1\\\\ 2\\end{bmatrix}\\), we have:\n\\[ n=\\begin{bmatrix}1.0 \u0026amp;-0.8\\end{bmatrix}\\begin{bmatrix}1\\\\2\\end{bmatrix}=-0.6\\\\ a=f(-0.6)=0\\tag{8} \\]\nhowever, the target is \\(1\\) so this is a wrong prediction of the current model. What we would do is modify the model.\nAs we mentioned above, \\(_1W\\) points to the purple region\nwhere the output is \\(1\\). So we should modify the direction of \\(_1W\\) close to the direction of \\(p_1\\).\nThe intuitive strategy is setting \\(_1W=p_i\\) when the output is \\(0\\) while the target is \\(1\\) and setting \\(_1W=-p_i\\) when the output is \\(1\\) while the target is \\(0\\)(where \\(i\\) less than the size of the training set). However, there are only three training points in this example, so there are only three possible decision boundaries, it can not guarantee that there must be a line \\(_1W=p_i\\) that separates the input data correctly.\nAlthough this strategy does not work, it has a good inspiration: 1. if \\(t=1\\) and \\(a=0\\), modify \\(_1W\\) close to \\(p_i\\) 2. if \\(t=0\\) and \\(a=1\\), modify \\(_1W\\) away from \\(p_i\\) or modify \\(_1W\\) close to \\(-p_i\\) equally 3. if \\(t=a\\) do nothing\nThen we find that the summation of two vectors is closer to each of the two vectors. Then our algorithm becomes:\nif \\(t=1\\) and \\(a=0\\), \\(_1W^{\\text{new}}= _1W^{\\text{old}}+p_i\\) if \\(t=0\\) and \\(a=1\\), \\(_1W^{\\text{new}}= _1W^{\\text{old}}-p_i\\) if \\(t=a\\) do nothing  Unified Learning Rule The target and output product information together to modify the model. Here we introduce the most simple but useful information - ‘error’:\n\\[ e_i=t_i-a_i\\tag{9} \\]\nso, our algorithm is\nif \\(t=1\\) and \\(a=0\\) where \\(e=1-0=1\\): \\(_1W^{\\text{new}}= _1W^{\\text{old}}+p_i\\) if \\(t=0\\) and \\(a=1\\) where \\(e=0-1=-1\\): \\(_1W^{\\text{new}}= _1W^{\\text{old}}-p_i\\) if \\(t=a\\) where \\(e=t-a=0\\): do nothing  and it’s not hard to notice that \\(e\\) has the same sign with \\(p_i\\). Then the algorithm can be simplified as:\n\\[ _1W^{\\text{new}}=_1W^{\\text{old}}+e\\cdot p_i=_1W^{\\text{old}}+(t_i-a_i)\\cdot p_i\\tag{10} \\]\nThis can be easily extended to bias:\n\\[ b^{\\text{new}}=b^{\\text{old}}+e\\cdot 1=b^{\\text{old}}+(t_i-a_i)\\tag{11} \\]\nand to the multiple neurons perceptron networks, the algorithm in matrix form is:\n\\[ W^{\\text{new}}=W^{\\text{old}}+\\mathbf{e}\\cdot \\mathbf{p_i}\\\\ \\mathbf{b}^{\\text{new}}=\\mathbf{b}^{\\text{old}}+\\mathbf{e}_i\\cdot \\mathbf{1}\\tag{12} \\]\nConclusion Perceptron is still working today The learning rule of the perceptron is a good example of having a close look at the learning rules of neuron networks Perceptron has some connection with other machine learning algorithms like linear classification A single perceptron has a lot of limits but multiple layers of perceptrons are more powerful  References  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","permalink":"https://anthony-tan.com/Learning-Rules-and-Perceptron-Learning-Rule/","summary":"Preliminaries supervised learning unsupervised learning reinforcement learning ‘An Introduction to Neural Networks’  Learning Rules1 We have built some neural network models in the post ‘An Introduction to Neural Networks’ and as we know architectures and learning rules are two main aspects of designing a useful network. The architectures we have introduced could not be used yet. What we are going to do is to investigate the learning rules for different architectures.","title":"Learning Rules and Perceptron Learning Rule"},{"content":"Preliminaries linear classifier An Introduction to Neural Networks  Theory and Notation1 We are not able to build any artificial cells up to now. It seems impossible to build a neuron network through biological materials manually, either. To investigate the ability of neurons we have built mathematical models of the neuron. These models have been assigned a number of neuron-like properties. However, there must be a balance between the number of properties contained by the mathematical models and the current computational abilities of the machines.\nFrom now on, we begin our study of the neuron network, and it looks like a good idea, to begin with, the simplest but basic model – artificial neuron. Because tons of network architectures are built on these simple neurons.\nSingle Neuron Model Let’s begin with the simplest neuron, which has only one input, one synapse which is represented by weight, a bias, a threshold operation which is expressed by a transfer function, and an output.\nWe know that the cell body of neurons plays summation and threshold operation. This is very like a linear classifier\nThen our simplest model is constructed as follows:\nSynapse is represented by a scalar which is called a weight, it will be multiplied by the input as a received signal, and then the signal is transferred to the cell body Cell body here is represented by two functional properties property:  the first one is summation which is used to collect all signals in a short time interval, while in this naive example only one input is concerned so it looks redundant but in the following models, it is an important operation of a neuron; the second function is a threshold operation that acts as a gatekeeper, only the signal stronger than some value could excite this neuron. Only excited neurons could pass signals to the other neurons connected to them.  and a scalar  The scalar property represents an original faint signal of the neuron. From a biological point, it makes sense because every nerve cell has its resting membrane potential(RMP).  Axon is expressed by an output that is produced by the threshold function. It can be any form in a biological neuron, like amplitude or frequency, but here it can just be a number that is decided by the selected threshold function.  The threshold function is called an active function or transfer functions officially. And it will be listed in the next section.\nLet’s review the single input neuron model and its components:\n \\(P\\): input signal, a scalar or vector, coming from a previous nerve cell or external signal \\(w\\): weight, a scalar or vector, coming from the synapse, act as the strength of a synapse \\(b\\): bias, a scalar, a property of this neuron \\(f\\): transfer function, act as a gatekeeper, perform a threshold operation \\(a\\): the output of the neuron, a scalar, can be a signal to the next neuron or as a final output to the external system.  The final mathematical expression is:\n\\[ a=f(w\\cdot P + b)\\tag{1} \\]\nFor instance, we have input \\(P=2.0\\), synapse weight \\(w=3.0\\), nerve cell bias \\(b=-1.5\\) and then we get the output:\n\\[ a= f(2.0\\times 3.0 + (-1.5))=f(4.5)\\tag{2} \\]\nhowever, bias can be omitted, or be rewritten as a special input and weight combination:\n\\[ a=f(w_1\\cdot P + w_0\\cdot 1)\\text{ where } w_1=w \\text{ and }w_0=b\\tag{3} \\]\n\\(w\\) and \\(b\\) came from equation(1). In the model, \\(w\\) and \\(b\\) are adjustable. And the ideal procedure is: 1. computing summation of all weighted inputs 2. select a transfer function 3. put the result of step 1 into the selected function from step 2 and get a final output of the neuron 4. using the learning rule to adjust \\(w\\) and \\(b\\) to adapt the task which is our purpose.\nTransfer Functions Every part of the neuron no matter the biological or mathematical one directly affects the function of the neuron. And this also makes the design of neurons more interesting, because we can build different kinds of neurons to simulate different kinds of operations. And this also provides sufficient basic blocks for us to develop a more complicated network.\nLet’s recall the single input model above, the components are \\(w\\), \\(b\\), \\(\\sum\\), and \\(f\\), however, \\(w\\) and \\(b\\) are objectives of learning, and \\(\\sum\\) is relatively stable which is hard to be replaced by any other operations. So, we move our attention to the threshold operation-\\(f\\). Threshold operation is like a switch that when some conditions are achieved produces a special output. But when the conditions are not reached, it gives another output. A simple mathematical equation to express this function is:\n\\[ f(x) = \\begin{cases} 0, \u0026amp; \\text{if $x\u0026gt;0$} \\\\ 1, \u0026amp; \\text{else} \\end{cases}\\tag{4} \\]\nTransfer functions can be linear or nonlinear; the Following three functions are mostly used.\nHard Limit Transfer Function The first commonly used threshold function is the most intuitive one, a piecewise function, when \\(x\u0026gt;0\\) the output is ‘on’ or ‘off’ for others. And by convention, ‘on’ is replaced by \\(1\\) and ‘off’ is replaced by \\(-1\\). So it becomes:\n\\[ f(x) = \\begin{cases} 1, \u0026amp; \\text{if $x\u0026gt;0$} \\\\ -1, \u0026amp; \\text{else} \\end{cases}\\tag{5} \\]\nand it looks like this:\nThen we take equation(1) into equation(5), we get: \\[ f(w\\cdot P +b) = \\begin{cases} 1, \u0026amp; \\text{if $w\\cdot P +b\u0026gt;0$} \\\\ -1, \u0026amp; \\text{else} \\end{cases}\\tag{6} \\]\nWe always regard the input as an independent variable so we replace \\(P\\) with \\(x\\) without loss of generality. Then we get:\n\\[ g(x) = \\begin{cases} 1, \u0026amp; \\text{if $x\u0026gt; -\\frac{b}{w}$} \\\\ -1, \u0026amp; \\text{else} \\end{cases}\\tag{7} \\]\nwhere \\(w\\neq 0\\). \\(g(x)\\) is a special case for equation(5) as the transfer function of this single-input neuron.\nThis is the famous threshold operation function, the Hard Limit Transfer Function\nLinear Transfer Function Another mostly used function is the linear function, which has the simplest form:\n\\[ f(x)=x\\tag{8} \\]\nand it is a line going through the origin\nWhen tale equation(1) into equation(8) we get:\n\\[ f(w\\cdot P+b)=w\\cdot P+b\\tag{9} \\]\nand we can get the special case of the linear transfer function for the single-input neuron:\n\\[ g(x)=w\\cdot x+b\\tag{10} \\]\nThe linear transfer function just seems as if there is no transfer function in the model but in some networks, it plays an important part.\nLog-sigmoid Transfer Function Another useful transfer function is the log-sigmoid function:\n\\[ f(x)=\\frac{1}{1+e^{-x}}\\tag{11} \\]\nThis sigmoid function has a similar appearance to the ‘Hard Limit Transfer Function’ however, the sigmoid has a more mathematical advantage than the hard limit transfer function, like it has derivative everywhere while equation(5) does not.\nThe single-input neuron model’s special case of the log-sigmoid function is:\n\\[ g(x)=\\frac{1}{1+e^{-w\\cdot x+b}}\\tag{12} \\]\nand it looks like this:\nThese three transfer functions are the most common ones and also the easiest ones. More transfer functions can be found:‘Transfer Function’\nMultiple-inputs Neuron After the insight of the single-input neuron, we can easily build a more complex and powerful neuron model- a multiple-inputs neuron, whose structure is more like the biological nerve cell than the single-input neuron:\nthen, the mathematical expression is:\n\\[ a=w_{1,1}\\cdot p_1+w_{1,2}\\cdot p_2+\\dots+ w_{1,R}\\cdot p_R+b\\tag{13} \\]\nThere are two numbers of subscript of \\(w\\) which seem unnecessary in the equation because the first number does not vary anymore. But as a long concern, it is better to remain this number for it is used to label the neuron. So \\(w_{1,2}\\) represents the second synapse’s weight belonging to the first neuron. When we have \\(k\\) neurons the \\(m\\)th synapse weight of \\(n\\)th neuron is \\(w_{n,m}\\).\nLet’s go back to the equation(13). It can be rewritten as:\n\\[ n=W\\mathbf{p}+b\\tag{14} \\]\nwhere: - \\(W\\) is a matrix that has only one row containing the weights - \\(\\mathbf{p}\\) is a vector representing inputs - \\(b\\) is a scalar representing bias - \\(n\\) is the result of the cell body operation,\nthen the output is:\n\\[ a=f(W\\mathbf{p}+b)\\tag{15} \\]\nThe diagram is a very powerful tool to express a neuron or a network because it’s good at showing the topological structure of the network. And for further research, an abbreviated notation had been designed. To the multiple-inputs neuron, we have:\na feature of this kind of notation is that the dimensions of each variable are labeled and the input dimension \\(R\\) is decided by the designer.\nNetwork Architecture A single neuron is not sufficient, even though it has multiple inputs.\nA layer of neurons To perform a more complicated function, we need more than one neuron and construct a network that contains a layer of neurons:\nin this model, we have \\(R\\)-dimensions input and \\(S\\) neurons then we get:\n\\[ a_i=f(\\sum_{j=1}^{R}w_{i,j}\\cdot p_{j}+b_j)\\tag{16} \\]\nthis is the output of \\(j\\) the neuron in the whole network, and we can rewrite the whole network in a metrical form:\n\\[ \\mathbf{a}=\\mathbf{f}(W\\mathbf{p}+\\mathbf{b})\\tag{17} \\]\nwhere\n \\(W\\) is a matrix \\(\\begin{bmatrix}w_{1,1}\u0026amp;\\cdots\u0026amp;w_{1,R}\\\\ \\vdots\u0026amp;\u0026amp;\\vdots\\\\w_{S,1}\u0026amp;\\cdots\u0026amp;w_{S_R}\\end{bmatrix}\\), where \\(w_{i,j}\\) is the \\(j\\)th weight of the \\(i\\)th neuron \\(\\mathbf{p}\\) is the vector of input \\(\\begin{bmatrix}p_1\\\\ \\vdots\\\\p_R\\end{bmatrix}\\) \\(\\mathbf{a}\\) is the vector of output \\(\\begin{bmatrix}a_1\\\\ \\vdots\\\\a_S\\end{bmatrix}\\) \\(\\mathbf{f}\\) is the vector of transfer functions \\(\\begin{bmatrix}f_1\\\\ \\vdots\\\\f_S\\end{bmatrix}\\) where each \\(f_i\\) can be different.  This network is much more powerful than the single neuron but they have a very similar abbreviated notation:\nthe only distinction is the dimension of each variable.\nMultiple Layers of Neurons The next stage of extending a single-layer network is multiple layers:\nand, its final output is:\n\\[ \\mathbf{a}=\\mathbf{f}^3(W^3\\mathbf{f}^2(W^2\\mathbf{f}^1(W^1\\mathbf{p}+\\mathbf{b}^1)+\\mathbf{b}^2)+\\mathbf{b}^3)\\tag{18} \\]\nthe numbers on the right-top of the variable are the layer number, for example, \\(w^1_{2,3}\\) is the weight of \\(2\\) nd synapse of the \\(3\\) rd neuron at the 1st layer.\nEach layer has also its name, for instance, the first layer whose input is external input is called the input layer. The layer whose output is external output is called the output layer. Other layers are called hidden layers. Its abbreviated notation is:\nThe new model with multiple layers is powerful but it is hard to design because the layer number is arbitrary and the neurons number in each layer is also untractable. So it becomes an experimental work. However, the input layer and output layer usually have a certain number and they are decided by the specialized task. Transfer functions are decided by the designer, and each neuron can have its transfer function different from any other neurons in the network.\nBias can be omitted but this can cause a problem that it will always output \\(\\mathbf{0}\\) when the input is \\(\\mathbf{0}\\). This phenomenon could not make sense in some tasks, so bias plays an important part in the \\(\\mathbf{0}\\) input situation. But to some other input, bias seems not so important.\nRecurrent Networks It seems possible that a neuron’s output also connects to its input. It acts somehow like\n\\[ \\mathbf{a}=\\mathbf{f}(W\\mathbf{f}(W\\mathbf{p}+\\mathbf{b})+\\mathbf{b})\\tag{19} \\]\nto illustrate the procedure, we present the delay block\nwhere the output is the input delayed 1-time unit:\n\\[ a(t)=u(t-1)\\tag{20} \\]\nand the block is initialized by \\(a(0)\\)\nAnother useful operation for the recurrent network is integrator:\nwhose output is:\n\\[ a(t)=\\int^t_0u(t)dt +a(0)\\tag{21} \\]\nA recurrent network is a network in which there is a feedback connection. Here we just list some basic concepts and more details would be researched in the following posts. The recurrent network works more powerful than a feedforward network because it exhibits temporal behavior which is a fundamental property of the biological brain. A typical recurrent network is:\nwhere:\n\\[ a(0)=\\mathbf{p} \\\\ a(t+1)=f(W\\mathbf{p}+\\mathbf{b})\\tag{22} \\]\nReferences  Demuth, H.B., Beale, M.H., De Jess, O. and Hagan, M.T., 2014. Neural network design. Martin Hagan.↩︎\n   ","permalink":"https://anthony-tan.com/Neuron-Model-and-Network-Architecture/","summary":"Preliminaries linear classifier An Introduction to Neural Networks  Theory and Notation1 We are not able to build any artificial cells up to now. It seems impossible to build a neuron network through biological materials manually, either. To investigate the ability of neurons we have built mathematical models of the neuron. These models have been assigned a number of neuron-like properties. However, there must be a balance between the number of properties contained by the mathematical models and the current computational abilities of the machines.","title":"Neuron Model and Network Architecture"},{"content":"Preliminaries Nothing  Neural Networks1 Neural Networks are a model of our brain that is built with neurons and is considered the source of intelligence. There are almost \\(10^{11}\\) neurons in the human brain and \\(10^4\\) connections of each neuron to other neurons. Some of these brilliant structures were given when we were born. Some other structures could be established by experience, and this progress is called learning. Learning is also considered as the establishment or modification of the connections between neurons.\nA biological Neural Network is a system of intelligence. Memories and other neural functions are stored in the neurons and their connections. Up to now, neurons and their connections are taken as the main direction of research on intelligence.\nArtificial Neural network(ANN for short) is the name of a mathematical model which is a tool for studying and simulating biological neural networks, and what we do here is to build a small neural network and observe their performance. However, these small models have amazing capacities for solving difficult problems which are hard or impossible to achieve by traditional methods. Traditional methods are not the old ones but the ones without learning progress or the ones dealing with traditional problems like sorting, solving equations, etc. What we say small model here is the model with much fewer neurons and connections than the human brain because a small model can be investigated easily and efficiently. However, the bigger models are constructed with small ones. So when we gain an insight into the smaller building blocks, we can predict the bigger ones’ performance precisely.\nAnother fundamental distinction between ANNs and biological neural networks is that ANNs are built of silicon.\nBiological Inspiration This figure represents the abstraction of two neurons. Although it looks humble, it has all the components of our best performance ANNs. This is the strong evidence that tells us that real intelligence is not so easy to simulate.\nLet’s look at this simplified structure. Three principal components:\n Dendrites Cell body Axon  Dendrites are tree-like receptive networks of nerve fibers, that carry electrical signals into the cell body. The cell body, sums, and thresholds these incoming signals. Axon is a single long fiber carrying electrical signals to other neurons.\nThe contact between dendrites and axons in the structure is called synapse. This is an interesting structure for its properties largely influence the performance of the whole network.\nMore details of biological neural science should be found in their subject textbooks. However, in my personal opinion, we can never build artificial intelligence by just studying ANNs, what we should do is investigate our brain and neural science. In other words. to find artificial intelligence, go to biological intelligence. However, until today, our models are far from any known brains on earth.\nBut there are still two similarities between artificial neural networks and biological ones:\nbuilding blocks of both networks are simple computational devices connection between neurons determines the function of the networks   ‘there is also the superiority of ANNs(or more rigorous of the computer) that is the speed. Biological neurons are slower than electrical circuits(\\(10^{-3}\\) to \\(10^{-10}\\)).’\n However, I don’t agree with this point, for we don’t even know what computation has been done during the period of \\(10^{-3}\\) seconds in the biological neurons. So this comparison made no sense. But the parallel structure in brains is beyond the reach of any computer right now.\nA Brief History of Artificial Neural Networks This is just a brief history of ANNs because so many researchers had finished so many works during the last 100 years. The following timeline is just some big events in the last 50 years.\n’Neurocomputing: foundations of research is a book written by John Anderson. It contains 43 papers on neural networks representing special historical interest.\nANNs come from the building background of physics, psychology, and neurophysiology:\n From the late 19th to early 20th: general theories of learning, vision, and conditioning were built, but there was no mathematical model of neuron operation 1943: Warren McCulloch and Walter Pitts found neurons could compute any arithmetic or logic function and this is considered the origin of the neural network field 1949: Donald Hebb proposed that classical conditioning is presented because of an individual neuron. He proposed a mechanism for learning in biological neurons. 1958: First practical application of ANN which is perceptron was proposed by Rosenblatt. This model was able to perform pattern recognition. 1960: Bernard Widrow and Ted Hoff developed a new learning algorithm and train adaptive linear neuron networks which are similar to Rosenblatt’s perceptron in both structure and capability. 1969: Marvin Minsky and Seymour Papert proved the limitation of Rosenblatt’s perceptron and Bernard Widrow and Ted Hoff’s learning algorithm. And they thought further research on neural networks is a dead end. This coursed a lot of researchers gave up. 1972: Teuvo Kohonen and James Anderson built the neural networks acting as memories independently. 1976: Stephen Grossberg built a self-organizing network 1982: Statistical mechanics was used to explain the recurrent network by John Hopfield which was also known as an associative memory 1986: Backpropagation is proposed by David Rumelhart and James McClelland which broke the limitation given by Minsky  This history ended in 1990. This is just the beginning of the neural network to us now, however, what we do today is also the beginning of the future. This progress is not “slow but sure”, it was dramatic sometimes but almost stop most of the time.\nNew concepts of neural networks come from the following aspects:\n innovative architectures training rules  Conclusion We took a look at the structure of a neuron, ANN is a simple model of a biological neural network. A brief history of neural network  References  Demuth, Howard B., Mark H. Beale, Orlando De Jess, and Martin T. Hagan. Neural network design. Martin Hagan, 2014.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Neural-Networks/","summary":"Preliminaries Nothing  Neural Networks1 Neural Networks are a model of our brain that is built with neurons and is considered the source of intelligence. There are almost \\(10^{11}\\) neurons in the human brain and \\(10^4\\) connections of each neuron to other neurons. Some of these brilliant structures were given when we were born. Some other structures could be established by experience, and this progress is called learning. Learning is also considered as the establishment or modification of the connections between neurons.","title":"An Introduction to Neural Networks"},{"content":"Preliminaries Linear Algebra(the concepts of space, vector) Calculus An Introduction to Linear Regression  Notations of Linear Regression1 We have already created a simple linear model in the post “An Introduction to Linear Regression”. According to the definition of linearity, we can develop the simplest linear regression model:\n\\[ Y\\sim w_1X+w_0\\tag{1} \\]\nwhere the symbol \\(\\sim\\) is read as “is approximately modeled as”. Equation (1) can also be described as “regressing \\(Y\\) on \\(X\\)(or \\(Y\\) onto \\(X\\))”.\nGo back to the example that was given in “An Introduction to Linear Regression”. Combining with the equation (1), we get a model of the budget for TV advertisement and sales:\n\\[ \\text{Sales}=w_1\\times \\text{TV}+ w_0\\tag{2} \\]\nAssuming we have a machine here, which can turn grain into flour, the input is the grain, \\(X\\) in equation (1), and the output is flour, \\(Y\\). Accordingly, \\(\\mathbf{w}\\) is the gears in the machine.\nThen the mathematically model is:\n\\[ y=\\hat{w_1}x+\\hat{w_0}\\tag{3} \\]\nThe hat symbol “\\(\\;\\hat{}\\;\\)” is used to present that this variable is a prediction, which means it is not the true value of the variable but a conjecture through certain mathematical strategies or methods else.\nThen, a new input \\(x_i\\) has its prediction:\n\\[ \\hat{y}_i=\\hat{w_1}x_i+\\hat{w_0}\\tag{4} \\]\nStatistical learning mainly studies \\(\\begin{bmatrix}\\hat{w_0}\\\\\\hat{w_1}\\end{bmatrix}\\) but machine learning concerns more about \\(\\hat{y}\\) All of them were based on the observed data.\nOnce we got this model, what we do next is estimating the parameters\nEstimating the Parameters For the advertisement task, what we have are a linear regression model equation(2) and a set of observations:\n\\[ \\{(x_1,y_1),(x_2,y_2),(x_3,y_3),\\dots,(x_n,y_n)\\}\\tag{5} \\]\nwhich is also known as training set. By the way, \\(x_i\\) in equation (5) is a sample of \\(X\\) and so is \\(y_i\\) of \\(Y\\). \\(n\\) is the size of the training set, the number of observations pairs.\nThe method we employed here is based on a measure of the “closeness” of the outputs of the model to the observed target (\\(y\\)s in set (5)). By far, the most used method is the “least squares criterion”.\nThe outputs \\(\\hat{y}_i\\) of current model(parameters) to every input \\(x_i\\) are:\n\\[ \\{(x_1,\\hat{y}_1),(x_2,\\hat{y}_2),(x_3,\\hat{y}_3),\\dots,(x_n,\\hat{y}_n)\\}\\tag{6} \\]\nand the difference between \\(\\hat{y}_i\\) and \\(y_i\\) is called residual and written as \\(e_i\\):\n\\[ e_i=y_i-\\hat{y}_i\\tag{7} \\]\n\\(y_i\\) is the target, which is the value our model is trying to achieve. So, the smaller the \\(|e_i|\\) is, the better the model is. Because the absolute operation is not a good analytic operation, we replace it with the quadratic operation:\n\\[ \\mathcal{L}_\\text{RSS}=e_1^2+e_2^2+\\dots+e_n^2\\tag{8} \\]\n\\(\\mathcal{L}_\\text{RSS}\\) means “Residual Sum of Squares”, the sum of total square residual. And to find a better model, we need to minimize the sum of the total residual. In machine learning, this is called loss function.\nNow we take equations (4),(7) into (8):\n\\[ \\begin{aligned} \\mathcal{L}_\\text{RSS}=\u0026amp;(y_1-\\hat{w_1}x_1-\\hat{w_0})^2+(y_2-\\hat{w_1}x_2-\\hat{w_0})^2+\\\\ \u0026amp;\\dots+(y_n-\\hat{w_1}x_n-\\hat{w_0})^2\\\\ =\u0026amp;\\sum_{i=1}^n(y_i-\\hat{w_1}x_i-\\hat{w_0})^2 \\end{aligned}\\tag{9} \\]\nTo minimize the function “\\(\\mathcal{L}_\\text{RSS}\\)”, the calculus told us the possible minimum(maximum) points always stay at stationary points. And the stationary points are the points where the derivative of the function is zero. Remember that the minimum(maximum) points must be stationary points, but the stationary point is not necessary to be a minimum(maximum) point. For more information, ‘Numerical Optimization’ is a good book.\nSince the ‘\\(\\mathcal{L}_\\text{RSS}\\)’ is a function of a vector \\(\\begin{bmatrix}w_0\u0026amp;w_1\\end{bmatrix}^T\\), the derivative is replaced by partial derivative. As the ‘\\(\\mathcal{L}_\\text{RSS}\\)’ is just a simple quadric surface, the minimum or maximum exists, and there is one and only one stationary point.\nThen our mission to find the best parameters for the regression has been converted to calculus the solution of the function system that the derivative(partial derivative) is set to zero.\nThe partial derivative of \\(\\hat{w_1}\\) is\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}=\u0026amp;-2\\sum_{i=1}^nx_i(y_i-\\hat{w_1}x_i-\\hat{w_0})\\\\ =\u0026amp;-2(\\sum_{i=1}^nx_iy_i-\\hat{w_1}\\sum_{i=1}^nx_i^2-\\hat{w_0}\\sum_{i=1}^nx_i) \\end{aligned}\\tag{10} \\]\nand derivative of \\(\\hat{w_0}\\) is:\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_0}}}=\u0026amp;-2\\sum_{i=1}^n(y_i-\\hat{w_1}x_i-\\hat{w_0})\\\\ =\u0026amp;-2(\\sum_{i=1}^ny_i-\\hat{w_1}\\sum_{i=1}^nx_i-\\sum_{i=1}^n\\hat{w_0}) \\end{aligned}\\tag{11} \\]\nSet both of them to zero and we can get:\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_0}}}\u0026amp;=0\\\\ \\hat{w_0} \u0026amp;=\\frac{\\sum_{i=1}^ny_i-\\hat{w_1}\\sum_{i=1}^nx_i}{n}\\\\ \u0026amp;=\\bar{y}-\\hat{w_1}\\bar{x} \\end{aligned}\\tag{12} \\]\nand\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}\u0026amp;=0\\\\ \\hat{w_1}\u0026amp;=\\frac{\\sum_{i=1}^nx_iy_i-\\hat{w_0}\\sum_{i=1}^nx_i}{\\sum_{i=1}^nx_i^2} \\end{aligned}\\tag{13} \\]\nTo get a equation of \\(\\hat{w_1}\\) independently, we take equation(13) to equation(12):\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}\u0026amp;=0\\\\ \\hat{w_1}\u0026amp;=\\frac{\\sum_{i=1}^nx_i(y_i-\\bar{y})}{\\sum_{i=1}^nx_i(x_i-\\bar{x})} \\end{aligned}\\tag{14} \\]\nwhere \\(\\bar{x}=\\frac{\\sum_{i=1}^nx_i}{n}\\) and \\(\\bar{y}=\\frac{\\sum_{i=1}^ny_i}{n}\\)\nBy the way, equation (14) has another form:\n\\[ \\hat{w_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})(x_i-\\bar{x})}\\tag{15} \\]\nand they are equal.\nDiagrams and Code Using python to demonstrate our result Equ. (12)(14) is correct:\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt  # load data from csv file by pandas AdvertisingFilepath=\u0026#39;./data/Advertising.csv\u0026#39; data=pd.read_csv(AdvertisingFilepath)  # convert original data to numpy array data_TV=np.array(data[\u0026#39;TV\u0026#39;]) data_sale=np.array(data[\u0026#39;sales\u0026#39;])  # calculate mean of x and y y_sum=0 y_mean=0 x_sum=0 x_mean=0 for x,y in zip(data_TV,data_sale):  y_sum+=y  x_sum+=x if len(data_sale)!=0:  y_mean=y_sum/len(data_sale) if len(data_TV)!=0:  x_mean=x_sum/len(data_TV)  # calculate w_1 w_1=0 a=0 b=0 for x,y in zip(data_TV,data_sale):  a += x*(y-y_mean)  b += x*(x-x_mean) if b!=0:  w_1=a/b  # calculate w_0 w_0=y_mean-w_1*x_mean  # draw a picture plt.xlabel(\u0026#39;TV\u0026#39;) plt.ylabel(\u0026#39;Sales\u0026#39;) plt.title(\u0026#39;TV and Sales\u0026#39;) plt.scatter(data_TV,data_sale,s=8,c=\u0026#39;g\u0026#39;, alpha=0.5) x=np.arange(-10,350,0.1) plt.plot(x,w_1*x+w_0,\u0026#39;r-\u0026#39;) plt.show() After running the code, we got:\nReference  James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.↩︎\n   ","permalink":"https://anthony-tan.com/A-Simple-Linear-Regression/","summary":"Preliminaries Linear Algebra(the concepts of space, vector) Calculus An Introduction to Linear Regression  Notations of Linear Regression1 We have already created a simple linear model in the post “An Introduction to Linear Regression”. According to the definition of linearity, we can develop the simplest linear regression model:\n\\[ Y\\sim w_1X+w_0\\tag{1} \\]\nwhere the symbol \\(\\sim\\) is read as “is approximately modeled as”. Equation (1) can also be described as “regressing \\(Y\\) on \\(X\\)(or \\(Y\\) onto \\(X\\))”.","title":"A Simple Linear Regression"},{"content":"Preliminariess Linear Algebra(the concepts of space, vector) Calculus  What is Linear Regression Linear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors).\nMachine Learning and Statistical Learning Machine learning and statistical learning are similar but have some distinctions. In machine learning, models, regression models, or classification models, are used to predict the outputs of the new incoming inputs.\nIn contrast, in statistical learning, regression and classification are employed to model the data to find out the hidden relations among the inputs. In other words, the models of data, no matter they are regression or classification, or something else. They are used to analyze the mechanism behind the data.\n“Linear” Linear is a property of the operation \\(f(\\cdot)\\), which has the following two properties:\n\\(f(a\\mathbf{x})=af(\\mathbf{x})\\) \\(f(\\mathbf{x}+\\mathbf{y})=f(\\mathbf{x})+f(\\mathbf{y})\\)  where \\(a\\) is a scalar. Then we say \\(f(\\cdot)\\) is linear or \\(f(\\cdot)\\) has a linearity property.\nThe linear operation can be represented as a matrix. And when a 2-dimensional linear operation was drawn on the paper, it is a line. Maybe that is why it is named linear, I guess.\n“Regression” In statistical or machine learning, regression is a crucial part of the whole field. And, the other part is the well-known classification. If we have a close look at the outputs data type, one distinction between them is that the output of regression is continuous but the output of classification is discrete.\nWhat is linear regression Linear regression is a regression model. All parameters in the model are linear, like:\n\\[ f(\\mathbf{x})=w_1x_1+w_2x_2+w_3x_3\\tag{1} \\]\nwhere the \\(w_n\\) where \\(n=1,2,3\\) are the parameters of the model, the output \\(f(\\mathbf{x})\\) can be written as \\(t\\) for 1-deminsional outputs (or \\(\\mathbf{t}\\) for multi-deminsional outputs).\n\\(f(\\mathbf{w})\\) is linear:\n\\[ \\begin{aligned} f(a\\cdot\\mathbf{w})\u0026amp;=aw_1x_1+aw_2x_2+aw_3x_3=a\\cdot f(\\mathbf{w}) \\\\ f(\\mathbf{w}+\\mathbf{v})\u0026amp;=(w_1+v_1)x_1+(w_2+v_2)x_2+(w_3+v_3)x_3\\\\ \u0026amp;=w_1x_1+v_1x_1+w_2x_2+v_2x_2+w_3x_3+v_3x_3\\\\ \u0026amp;=f(\\mathbf{w})+f(\\mathbf{v}) \\end{aligned}\\tag{2} \\]\nwhere \\(a\\) is a scalar, and \\(\\mathbf{v}\\) is in the same space with \\(\\mathbf{w}\\)\nQ.E.D\nThere is also another view that the linear property of models is also for \\(\\mathbf{x}\\), the input.\nBut the following model\n\\[ t=f(\\mathbf{x})=w_1\\log(x_1)+w_2\\sin(x_2)\\tag{3} \\]\nis a case of linear regression problem in our definition. But from the second point of view, it is not linear for inputs \\(\\mathbf{x}\\). However, this is not an unsolvable contradiction. If we use:\n\\[ y_1= \\log(x_1)\\\\ y_2= \\sin(x_2)\\tag{4} \\]\nto replace the \\(\\log\\) and \\(\\sin\\) in equation (3), we get again\n\\[ t=f(\\mathbf{y})=w_1y_1+w_2y_2\\tag{5} \\]\na linear operation for both input \\(\\mathbf{y}=\\begin{bmatrix}y_1\\;y_2\\end{bmatrix}^T\\) and parameters \\(\\mathbf{z}\\) .\nThe tranformation, equation(4), is called feature extraction. \\(\\mathbf{y}\\) is called features, and \\(\\log\\) and \\(\\sin\\) are called basis functions\nAn Example This example is taken from (James20131), It is about the sale between different kinds of advertisements. I downloaded the data set from http://faculty.marshall.usc.edu/gareth-james/ISL/data.html. It’s a CSV file, including 200 rows. Here I draw 3 pictures using ‘matplotlib’ to make the data more visible. They are advertisements for ‘TV’, ‘Radio’, ‘Newspaper’ to ‘Sales’ respectively.\nFrom these figures, we can find TV ads and Sales looks like having a stronger relationship than radio ads and sales. However, the Newspaper ads and Sales look independent.\nFor statistical learning, we should take statistical methods to investigate the relation in the data. And in machine learning, to a certain input, predicting an output is what we are concerned.\nWhy Linear Regression Linear regression has been used for more than 200 years, and it’s always been our first class of statistical learning or machine learning. Here we list 3 practical elements of linear regression, which are essential for the whole subject:\nIt is still working in some areas. Although more complicated models have been built, they could not be replaced totally. It is a good jump-off point to the other more feasible and adorable models, which may be an extension or generation of naive linear regression Linear regression is easy, so it is possible to be analyzed mathematically.  This is why linear regression is always our first step to learn machine learning and statistical learning. And by now, this works pretty well.\nA Probabilistic View Machine learning or statistical learning can be described from two different views - Bayesian and Frequentist. They both worked well for some different instances, but they also have their limitations. The Bayesian view of the linear regression will be talked about as well later.\nBayesian statisticians thought the input \\(\\mathbf{x}\\), the output \\(t\\), and the parameter \\(\\mathbf{w}\\) are all random variables, while the frequentist does not think so. Bayesian statisticians predict the unknown input \\(\\mathbf{x}_0\\) by forming the distribution \\(\\mathbb{P}(t_0|\\mathbf{x}_0)\\) and then sampling from it. To achieve this goal, we must build the \\(\\mathbb{P}(t|\\mathbf{x})\\) firstly. This is the modeling progress, or we can call it learning progress.\nReferences  James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Linear-Regression/","summary":"Preliminariess Linear Algebra(the concepts of space, vector) Calculus  What is Linear Regression Linear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors).\nMachine Learning and Statistical Learning Machine learning and statistical learning are similar but have some distinctions. In machine learning, models, regression models, or classification models, are used to predict the outputs of the new incoming inputs.","title":"An Introduction to Linear Regression"},{"content":"About Me Hi, I\u0026rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years. So far, I\u0026rsquo;ve learned and reviewed some papers that are necessary for reinforcement learning research, and some mathematics, like calculus, linear algebra, probability, and so on.\nHowever the bigger one in the picture is me, Tony, and the smaller one is my dog, Potato(He was too young to take a shower when we took the picture, and now he is no more a dirty puppy😀)\nWhy the blogs These blogs here are used to simply explain what I have learned, according to the Feyman Technique. And blogging what I\u0026rsquo;ve just learned is the most important part of learning. The whole process is:\n Choosing a concept or theory I would like to know(collecting necessary materials )  Outlining what prior knowledge of this concept or theory Taking note of this prior knowledge   Try to explain the new theory to readers of the website without any new words and concepts in the theory (draft) Go back to the source and fill in the gap in understanding Simplify the explaining(rewrite and post)  What in blogs These blogs contain:\n Mathematics Neuroscience Algorithms  Deep Learning Reinforcement Learning    And some of these posts might also be represented by videos on my YouTube channel.\n","permalink":"https://anthony-tan.com/about/","summary":"About Me Hi, I\u0026rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years.","title":""}]