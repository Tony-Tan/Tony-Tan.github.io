[{"content":"Preliminaries linear algebra   inner multiplication projection  Idea of Fisher linear discriminant1 ‚ÄòLeast-square method‚Äô in classification can only deal with a small set of tasks. That is because it was designed for the regression task. Then we come to the famous Fisher linear discriminant. This method is also discriminative for it gives directly the class to which the input \\(\\mathbf{x}\\) belongs. Assuming that the linear function\n\\[ y=\\mathbf{w}^T\\mathbf{x}+w_0\\tag{1} \\]\nis employed as before. Then the threshold function \\(f(\\mathbf{y})=\\begin{cases}1 \u0026amp;\\text{ if } y\\leq 0\\\\0 \u0026amp;\\text{ otherwise }\\end{cases}\\) was employed. If \\(y\u0026lt;0\\) or equivalenttly \\(\\mathbf{w}^T\\mathbf{x}\\leq -w_0\\) , \\(\\mathbf{x}\\) belongs to \\(\\mathcal{C}_1\\), or it belongs to \\(\\mathcal{C}_2\\).\nThis is an intuitive classification framework, but if such kinds of parameters exist and how to find them out is still a hard problem.\nFrom the linear algebra view, when we set \\(w_0=0\\) the equation (1) can be viewed as the vector \\(\\mathbf{x}\\) projecting on vector \\(\\mathbf{w}\\).\nAnd a good parameter vector direction and a threshold may solve this problem. And the measurement of how good the parameter vector direction is has some different candidates.\nDistance Between Class Center The first strategy that comes to us is to maximize the distance between the projections of the centers of different classes.\nThe first step is to get the center of a class \\(\\mathcal{C}_k\\) whose size is \\(N_k\\) by:\n\\[ \\mathbf{m}_k=\\frac{1}{N_k}\\sum_{x_i\\in \\mathcal{C}_k}\\mathbf{x}_i\\tag{2} \\]\nSo the distance between the projections \\(m_1\\), \\(m_2\\) of centers of the two classes, \\(\\mathbf{m}_1\\), \\(\\mathbf{m}_2\\) is:\n\\[ m_1-m_2=\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2)\\tag{3} \\]\nAnd for the \\(\\mathbf{w}\\) here is referred to as the direction vector, its margin is \\(1\\):\n\\[ ||\\mathbf{w}||=1\\tag{4} \\]\nWhen \\(\\mathbf{w}\\) has the same direction with \\(\\mathbf{m}_1-\\mathbf{m}_2\\), equation(3) get its maximum value.\nThen the result looks like this:\nThe blue star and blue circle are the center of red stars and green circles, respectively. and the blue arrow is the direction we want in which the projections of center points have the longest distance.\nWith our observation, this line does not give the optimum solution. Because, although the projections of centers have the longest distance on this line, the projections of all sample points scatter into a relatively large region and some of them from different classes have been mixed up.\nThis phenomenon exists in our daily life. For example, when two seats are closed, the people sitting on them do not have enough space(this is the original condition). Then we move the seats and make them a little farther from each other(make the projections of centers far from each other). But this time, two big guys come to sit on them(the projection has a big variance), and the space is still not enough.\nIn our problem, the projections of the points of a class are the big guys. We need to make the projection of centers far away from each other and make the projections of points in one class slender (which means a lower variance) at the same time.\nThe variance of the projected points of class \\(k\\) can be calculated by:\n\\[ s^2_k=\\sum_{n\\in \\mathcal{C}_k}(y_n - m_k)^2\\tag{5} \\]\nand it is also called within-class variance.\nTo make the seat comfortable for people who sit on them, we need to make the seat as far as possible from each other(maximize \\((m_2-m_1)^2\\)) and only allow children to sit(minimize the sum of within-class variance).\nFisher criterion satisfies this requirement:\n\\[ J(\\mathbf{w})=\\frac{(m_2-m_1)^2}{s_1^2+s_2^2}\\tag{6} \\]\nAnd \\(J(\\mathbf{w})\\) in details is:\n$$ \\begin{aligned} J()\u0026amp;=\\ \u0026amp;=\\ \u0026amp;= {_{n_1}(^T_n - ^T1)^2+{n_2}(^T_n - ^T_2)^2}\\\n\u0026amp;= {^T(_{n_1}(_n - _1)(_n - 1)^T+{n_2}(_n - _2)(_n - _2)^T)} \\end{aligned} $$\nAnd we can set:\n\\[ \\begin{aligned} S_B \u0026amp;= (\\mathbf{m}_1-\\mathbf{m}_2)(\\mathbf{m}_1-\\mathbf{m}_2)^T\\\\ S_W \u0026amp;= \\sum_{n\\in \\mathcal{C}_1}(\\mathbf{x}_n - \\mathbf{m}_1)(\\mathbf{x}_n - \\mathbf{m}_1)^T+\\sum_{n\\in \\mathcal{C}_2}(\\mathbf{x}_n - \\mathbf{m}_2)(\\mathbf{x}_n - \\mathbf{m}_2)^T \\end{aligned}\\tag{8} \\]\nwhere \\(S_B\\) represents the covariance matrix Between classes, and \\(S_W\\) represents the Within classes covariance. Then the equation(8) becomes:\n\\[ \\begin{aligned} J(\\mathbf{w})\u0026amp;=\\frac{\\mathbf{w}^TS_B\\mathbf{w}}{\\mathbf{w}^TS_W\\mathbf{w}} \\end{aligned}\\tag{9} \\]\nTo maximise the \\(J(\\mathbf{w})\\), we should differentiat equation (9) with respect to \\(\\mathbf{w}\\) firstly:\n\\[ \\begin{aligned} \\frac{\\partial }{\\partial \\mathbf{w}}J(\\mathbf{w})\u0026amp;=\\frac{\\partial }{\\partial \\mathbf{w}}\\frac{\\mathbf{w}^TS_B\\mathbf{w}}{\\mathbf{w}^TS_W\\mathbf{w}}\\\\ \u0026amp;=\\frac{(S_B+S_B^T)\\mathbf{w}(\\mathbf{w}^TS_W\\mathbf{w})-(\\mathbf{w}^TS_B\\mathbf{w})(S_W+S_W^T)\\mathbf{w}}{(\\mathbf{w}^TS_W\\mathbf{w})^T(\\mathbf{w}^TS_W\\mathbf{w})} \\end{aligned}\\tag{10} \\]\nand set it to zero:\n\\[ \\frac{(S_B+S_B^T)\\mathbf{w}(\\mathbf{w}^TS_W\\mathbf{w})-(\\mathbf{w}^TS_B\\mathbf{w})(S_W+S_W^T)\\mathbf{w}}{(\\mathbf{w}^TS_W\\mathbf{w})^T(\\mathbf{w}^TS_W\\mathbf{w})}=0\\tag{11} \\]\nand then:\n\\[ \\begin{aligned} (S_B+S_B^T)\\mathbf{w}(\\mathbf{w}^TS_W\\mathbf{w})\u0026amp;=(\\mathbf{w}^TS_B\\mathbf{w})(S_W+S_W^T)\\mathbf{w}\\\\ (\\mathbf{w}^TS_W\\mathbf{w})S_B\\mathbf{w}\u0026amp;=(\\mathbf{w}^TS_B\\mathbf{w})S_W\\mathbf{w} \\end{aligned}\\tag{12} \\]\nBecause \\((\\mathbf{w}^TS_W\\mathbf{w})\\) and \\((\\mathbf{w}^TS_B\\mathbf{w})\\) are scalars and according equation (8) when we multiply both sides by \\(\\mathbf{w}\\) and we have\n\\[ S_B \\mathbf{w}= (\\mathbf{m}_1-\\mathbf{m}_2)((\\mathbf{m}_1-\\mathbf{m}_2)^T\\mathbf{w})\\tag{13} \\]\nand \\((\\mathbf{m}_1-\\mathbf{m}_2)^T\\mathbf{w}\\) is a scalar and \\(S_B\\mathbf{w}\\) have the same direction with \\((\\mathbf{m}_1-\\mathbf{m}_2)\\)\nso the equation (12) can be written as:\n\\[ \\begin{aligned} \\mathbf{w}\u0026amp;\\propto S^{-1}_WS_B\\mathbf{w}\\\\ \\mathbf{w}\u0026amp;\\propto S^{-1}_W(\\mathbf{m}_1-\\mathbf{m}_2) \\end{aligned}\\tag{14} \\]\nSo, up to now, we have had a result parameter vector \\(\\mathbf{w}\\) based on maximizing Fisher Criterion.\nCode The code of the process is relatively easy:\nclass LinearClassifier():   def fisher(self, x, y):  x = np.array(x)  x_dim = x.shape[1]  m_1 = np.zeros(x_dim)  m_1_size = 0  m_2 = np.zeros(x_dim)  m_2_size = 0  for i in range(len(y)):  if y[i] == 0:  m_1 = m_1 + x[i]  m_1_size += 1  else:  m_2 = m_2 + x[i]  m_2_size += 1  if m_1_size != 0 and m_2_size != 0:  m_1 = (m_1/m_1_size).reshape(-1, 1)  m_2 = (m_2/m_2_size).reshape(-1, 1)  s_c_1 = np.zeros([x_dim, x_dim])  s_c_2 = np.zeros([x_dim, x_dim])  for i in range(len(y)):  if y[i] == 0:  s_c_1 += (x[i] - m_1).dot((x[i] - m_1).transpose())  else:  s_c_2 += (x[i] - m_2).dot((x[i] - m_2).transpose())  s_w = s_c_1 + s_c_2   return np.linalg.inv(s_w).dot(m_2-m_1) The entire project can be found https://github.com/Tony-Tan/ML and please star me.\nThe results of the code(where the line is the one to which the points are projected):\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006. And what we should do is estimate the parameters of the model.‚Ü©Ô∏é\n   ","permalink":"https://anthony-tan.com/Fisher-Linear-Discriminant/","summary":"Preliminaries linear algebra   inner multiplication projection  Idea of Fisher linear discriminant1 ‚ÄòLeast-square method‚Äô in classification can only deal with a small set of tasks. That is because it was designed for the regression task. Then we come to the famous Fisher linear discriminant. This method is also discriminative for it gives directly the class to which the input \\(\\mathbf{x}\\) belongs. Assuming that the linear function\n\\[ y=\\mathbf{w}^T\\mathbf{x}+w_0\\tag{1} \\]","title":"Fisher Linear Discriminant(LDA)"},{"content":"Preliminaries convex definition linear algebra   vector length vector direction  Discriminant Function in Classification The discriminant function or discriminant model is on the other side of the generative model. And we, here, have a look at the behavior of the discriminant function in linear classification.1\nIn the post ‚ÄòLeast Squares Classification‚Äô, we have seen, in a linear classification task, the decision boundary is a line or hyperplane by which we separate two classes. And if our model is based on the decision boundary or, in other words, we separate inputs by a function and a threshold, the model is a discriminant model and the decision boundary is formed by the function and a threshold.\nNow, we are going to talk about what the decision boundaries look like in the \\(K\\)-classes problem when \\(K=2\\) and \\(K\u0026gt;2\\). To illustrate the boundaries, we only consider the 2D(two dimensional) input vector \\(\\mathbf{x}\\) who has only two components.\nTwo classes The easiest decision boundary comes from 2-dimensional input space which is separated into 2 regions:\nwhose decision boundary is:\n\\[ \\mathbf{w}^T\\mathbf{x}+w_0=\\text{ constant }\\tag{1} \\]\nThis equation is equal to \\(\\mathbf{w}^T\\mathbf{x}+w_0=0\\) because \\(w_0\\) is also a constant, so it can be merged with the r.h.s. constant. Of course, the 1-dimensional input space is easier than 2-dimensional, and its decision boundary is a point.\nLet‚Äôs go back to the line, and it has the following properties:\nThe vector \\(\\mathbf{w}\\) always points to a certain region and is perpendicular to the line. \\(w_0\\) decides the location of the boundary relative to the origin. The perpendicular distance \\(r\\) to the line of a point \\(\\mathbf{x}\\) can be calculated by \\(r=\\frac{y(\\mathbf{x})}{||\\mathbf{w}||}\\) where \\(y(\\mathbf{x})=\\mathbf{w}^T\\mathbf{x}+w_0\\)  Because these three properties are all basic concepts of a line, we just prove the third point roughly:\nproof: We set \\(\\mathbf{x}_{\\perp}\\) is the projection of \\(\\mathbf{x}\\) on the line.\nWe using the first point that \\(\\mathbf{w}\\) is perpendicular to the line and \\(\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\) is the union vector:\n\\[ \\mathbf{x}=\\mathbf{x}_{\\perp}+r\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\tag{2} \\]\nand we substitute equation (2) to the line function \\(y(\\mathbf{x})=\\mathbf{w}^T\\mathbf{x}+w_0\\) :\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;=\\mathbf{w}^T(\\mathbf{x}_{\\perp}+r\\frac{\\mathbf{w}}{||\\mathbf{w}||})+w_0\\\\ \u0026amp;=\\mathbf{w}^T\\mathbf{x}_{\\perp}+\\mathbf{w}^Tr\\frac{\\mathbf{w}}{||\\mathbf{w}||}+w_0\\\\ \u0026amp;=\\mathbf{w}^Tr\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\\\ \u0026amp;=r\\frac{||\\mathbf{w}||^2}{||\\mathbf{w}||}\\\\ \\end{aligned}\\tag{3} \\]\nSo we have\n\\[ r=\\frac{y(\\mathbf{x})}{||\\mathbf{w}||}\\tag{4} \\]\nQ.E.D.\nHowever, augmented vectors \\(\\mathbf{w}= \\begin{bmatrix}w_0\u0026amp;w_1\u0026amp; \\cdots\u0026amp;w_d\\end{bmatrix}^T\\) and \\(\\mathbf{x}= \\begin{bmatrix}1\u0026amp;x_1\u0026amp; \\cdots\u0026amp;x_d\\end{bmatrix}^T\\) can cancel \\(w_0\\) of the original boundary equation. So a \\(d+1\\)-dimensional hyperplane that went through the origin could be instea replaced by an \\(d\\)-dimensional hyperplane.\nMultiple Classes Things changed when we consider more than 2 classes. Their boundaries become more complicated, and we have 3 different strategies for this problem intuitively:\n1-versus-the-rest Classifier This strategy needs at least \\(K-1\\) classifiers(boundaries). Each classifier \\(k\\) just decides which side belongs to class \\(k\\) and the other side does not belong to \\(k\\). So when we have two boundaries, like:\nwhere the region \\(R_4\\) is embarrassed, based on the properties of the decision boundary, and the definition of classification in the post‚ÄòFrom Linear Regression to Linear Classification‚Äô, region \\(R_4\\) can not belong to \\(\\mathcal{C}_1\\) and \\(\\mathbb{C}_2\\) simultaneously.\nSo the first strategy can work for some regions, but there are some black whole regions where the input \\(\\mathbf{x}\\) belongs to more than one class and some white whole regions where the input \\(\\mathbf{x}\\) belongs to no classes(region \\(R_3\\) could be such a region)\n1-versus-1 classifier Another kind of multiple class boundary is the combination of several 1-versus-1 linear decision boundaries. Both sides of a decision boundary belong to a certain class, not like the 1-versus-rest classifier. And to a \\(K\\) class task, it needs \\(K(K-1)/2\\) binary discriminant functions.\nHowever, the contradiction still exists. Region \\(R_4\\) belongs to class \\(\\mathcal{C}_1\\), \\(\\mathcal{C}_2\\), and \\(\\mathcal{C}_3\\) simultaneously.\nSo this is also not good for all situations.\n\\(K\\) Linear functions We use a set of \\(K\\) linear functions: \\[ \\begin{aligned} y_1(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_1\\mathbf{x}+w_{10}\\\\ y_2(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_2\\mathbf{x}+w_{20}\\\\ \u0026amp;\\vdots \\\\ y_K(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_K\\mathbf{x}+w_{K0}\\\\ \\end{aligned}\\tag{5} \\]\nand an input belongs to \\(k\\) when \\(y_k(\\mathbf{x})\u0026gt;y_j(\\mathbf{x})\\) where \\(j\\in \\{1,2,\\cdots,K\\}\\) that \\(j\\neq k\\). According to this definition, the decision boundary between class \\(k\\) and class \\(j\\) is \\(y_k(\\mathbf{x})=y_j(\\mathbf{x})\\) where \\(k,j\\in\\{1,2,\\cdots,K\\}\\) and \\(j\\neq k\\). Then a decision hyperplane is defined as:\n\\[ (\\mathbf{w}_k-\\mathbf{w}_j)^T\\mathbf{x}+(w_{k0}-w_{j0})=0\\tag{6} \\]\nThese decision boundaries separate the input spaces into \\(K\\) single connect, convex regions.\nproof: choose two points in the region \\(k\\) that \\(k\\in \\{1,2,\\cdots,K\\}\\). \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) are two points in the region. An arbitrary point on the line between \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) can be written as \\(\\mathbf{x}\u0026#39;=\\lambda \\mathbf{x}_A + (1-\\lambda)\\mathbf{x}_B\\) where \\(0\\leq\\lambda\\leq1\\). For the linearity of \\(y_k(\\mathbf{x})\\) we have:\n\\[ y_k(\\mathbf{x}\u0026#39;)=\\lambda y_k(\\mathbf{x}_A) + (1-\\lambda)y_k(\\mathbf{x}_B)\\tag{7} \\]\nBecause \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) belong to class \\(k\\), \\(y_k(\\mathbf{x}_A)\u0026gt;y_j(\\mathbf{x}_A)\\) and \\(y_k(\\mathbf{x}_B)\u0026gt;y_j(\\mathbf{x}_B)\\) where \\(j\\neq k\\). Then \\(y_k(\\mathbf{x}\u0026#39;)\u0026gt;y_j(\\mathbf{x}\u0026#39;)\\) and the region of class \\(k\\) is convex.\nQ.E.D\nThe last strategy seems good. And what we should do is estimate the parameters of the model. The most famous approaches that will study are: 1. Least square 2. Fisher‚Äôs linear discriminant 3. Perceptron algorithm\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.‚Ü©Ô∏é\n   ","permalink":"https://anthony-tan.com/Discriminant-Functions-and-Decision-Boundary/","summary":"Preliminaries convex definition linear algebra   vector length vector direction  Discriminant Function in Classification The discriminant function or discriminant model is on the other side of the generative model. And we, here, have a look at the behavior of the discriminant function in linear classification.1\nIn the post ‚ÄòLeast Squares Classification‚Äô, we have seen, in a linear classification task, the decision boundary is a line or hyperplane by which we separate two classes.","title":"Discriminant Functions and Decision Boundary"},{"content":"Preliminaries A Simple Linear Regression Least Squares Estimation From Linear Regression to Linear Classification pseudo-inverse  Least Squares for Classification1 Least-squares for linear regression had been talked about in ‚ÄòSimple Linear Regression‚Äô. And in this post, we want to find out whether this powerful algorithm can be used in classification.\nRecalling the distinction between the properties of classification and regression, two points need to be emphasized again(‚ÄòFrom Linear Regression to Linear Classification‚Äô):\nthe targets of regression are continuous but the targets of classification are discrete. the output of the classification hypothesis could be \\(\\mathbb{P}(\\mathcal{C}_k|\\mathbf{x})\\) generatively or the output is just a class label \\(\\mathcal{C}_k\\) discriminatively.  The generative model will be talked about in other posts. And we focus on discriminative models in these posts which means our hypothesis directly gives which class the input belongs to.\nWe want to use least-squares methods which had been designed and proved for linear regression. And what we could do to extend the least-squares method to classification are:\nmodifying the type of output and designing a discriminative model  Modifying the type of output is to convert the class label into a number, like ‚Äòapple‚Äô to \\(1\\), ‚Äòorange‚Äô to 0. And when we use the 1-of-K label scheme(https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/), we could build the model with \\(K\\) linear functions:\n\\[ \\begin{aligned} y_1(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_1\\mathbf{x}\\\\ y_2(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_2\\mathbf{x}\\\\ \\vdots\u0026amp;\\\\ y_K(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_K\\mathbf{x}\\\\ \\end{aligned}\\tag{1} \\]\nwhere \\(\\mathbf{x}=\\begin{bmatrix}1\u0026amp;x_1\u0026amp;x_2\u0026amp;\\cdots\u0026amp;x_n\\end{bmatrix}^T\\) and \\(\\mathbf{w}_i=\\begin{bmatrix}w_0\u0026amp;w_1\u0026amp;w_2\u0026amp;\\cdots\u0026amp;w_n\\end{bmatrix}^T\\) for \\(i=1,2,\\cdots,K\\). And \\(y_i\\) is the \\(i\\) th component of 1-of-K output for \\(i=1,2,\\cdots,K\\). Clearly, the output of each \\(y_i(\\mathbf{x})\\) is continuous and could not be just \\(0\\) or \\(1\\). So we set the largest value to be 1 and others 0.\nWe had discussed the linear regression with the least squares in a ‚Äòsingle-target‚Äô regression problem. And that idea can also be employed in the multiple targets regression. And these \\(K\\) parameter vectors \\(\\mathbf{w}_i\\) can be calculated simultaneously. We can rewrite the equation (1) into the matrix form:\n\\[ \\mathbf{y}(\\mathbf{x})=W^T\\mathbf{x}\\tag{2} \\]\nwhere the \\(i\\)th column of \\(W\\) is \\(\\mathbf{w}_i\\)\nThen we employ the least square method for a sample:\n\\[ \\{(\\mathbf{x}_1,\\mathbf{t}_1),(\\mathbf{x}_2,\\mathbf{t}_2),\\cdots,(\\mathbf{x}_m,\\mathbf{t}_m)\\} \\tag{3} \\]\nwhere \\(\\mathbf{t}\\) is a \\(K\\)-dimensional target consisting of \\(k-1\\) 0‚Äôs and one ‚Äò1‚Äô. And each diminsion of output \\(\\mathbf{y}(\\mathbf{x})_i\\) is the regression result of the corresponding dimension of target \\(t_i\\). And we build up the input matrix \\(X\\) of all \\(m\\) input consisting of \\(\\mathbf{x}^T\\) as rows:\n\\[ X=\\begin{bmatrix} -\u0026amp;\\mathbf{x}^T_1\u0026amp;-\\\\ -\u0026amp;\\mathbf{x}^T_2\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\mathbf{x}^T_K\u0026amp;- \\end{bmatrix}\\tag{4} \\]\nThe sum of square errors is:\n\\[ E(W)=\\frac{1}{2}\\mathrm{Tr}\\{(XW-T)^T(XW-T)\\} \\tag{5} \\]\nwhere the matrix \\(T\\) is the target matrix whose \\(i\\) th row in target vevtor \\(\\mathbf{t}^T_i\\). The trace operation is employed because the only the value \\((W\\mathbf{x}^T_i-\\mathbf{t}_i)^T(W\\mathbf{x}_i^T-\\mathbf{t}_i)\\) for \\(i=1,2,\\cdots,m\\) is meaningful, but \\((W\\mathbf{x}^T_i-\\mathbf{t}_i)^T(W\\mathbf{x}_j^T-\\mathbf{t}_j)\\) where \\(i\\neq j\\) and \\(i,j = 1,2,\\cdots,m\\) is useless.\nTo minimize the linear equation in equation(5), we can get its derivative\n\\[ \\begin{aligned} \\frac{dE(W)}{dW}\u0026amp;=\\frac{d}{dW}(\\frac{1}{2}\\mathrm{Tr}\\{(XW-T)^T(XW-T)\\})\\\\ \u0026amp;=\\frac{1}{2}\\frac{d}{dW}(\\mathrm{Tr}\\{W^TX^TXW-T^TXW-W^TX^TT+T^TT\\})\\\\ \u0026amp;=\\frac{1}{2}\\frac{d}{dW}(\\mathrm{Tr}\\{W^TX^TXW\\}-\\mathrm{Tr}\\{T^TXW\\}\\\\ \u0026amp;-\\mathrm{Tr}\\{W^TX^TT\\}+\\mathrm{Tr}\\{T^TT\\})\\\\ \u0026amp;=\\frac{1}{2}\\frac{d}{dW}(\\mathrm{Tr}\\{W^TX^TXW\\}-2\\mathrm{Tr}\\{T^TXW\\}+\\mathrm{Tr}\\{T^TT\\})\\\\ \u0026amp;=\\frac{1}{2}(X^TXW-X^TT) \\end{aligned}\\tag{6} \\]\nand set it equal to \\(\\mathbf{0}\\):\n\\[ \\begin{aligned} \\frac{1}{2}(X^TXW-X^TT )\u0026amp;= \\mathbf{0}\\\\ W\u0026amp;=(X^TX)^{-1}X^TT \\end{aligned}\\tag{7} \\]\nwhere we assume \\(X^TX\\) can be inverted. The component \\((X^TX)^{-1}X^T\\) is also called pseudo-inverse of the matrix \\(X\\) and it is always denoted as \\(X^{\\dagger}\\).\nCode and Result The code of this algorithm is relatively simple because we have programmed the linear regression before which has the same form of equation (7).\nWhat we should care about is the formation of these matrices \\(W\\), \\(X\\), and \\(T\\).\nwe should first convert the target value into the 1-of-K form:\ndef label_convert(y, method =\u0026#39;1-of-K\u0026#39;):  if method == \u0026#39;1-of-K\u0026#39;:  label_dict = {}  number_of_label = 0  for i in y:  if i not in label_dict:  label_dict[i] = number_of_label  number_of_label += 1  y_ = np.zeros([len(y),number_of_label])  for i in range(len(y)):  y_[i][label_dict[y[i]]] = 1  return y_,number_of_label what we do is count the total number of labels(\\(K\\))and we set the \\(i\\) th component of the 1-of-K target to 1 and other components to 0.\nThe kernel of the algorithm is:\nclass LinearClassifier():  def least_square(self, x, y):  x = np.array(x)  x_dim = x.shape[0]  x = np.c_[np.ones(x_dim), x]  w = np.linalg.pinv(x.transpose().dot(x)).dot(x.transpose()).dot(y)  return w.transpose() the line x = np.c_[np.ones(x_dim), x] is to augment the input vector \\(\\mathbf{x}\\) with a dummy value \\(1\\). And the transpose of the result is to make each row represent a weight vector of eqation (2). The entire project can be found The entire project can be found https://github.com/Tony-Tan/ML and please star me ü•∞.\nI have tested the algorithm in several training sets, and the result is like the following figures:\nProblems of Least Squares Lack of robustness if outliers (Figure 2 illustrates this problem) Sum of squares error penalizes the predictions that are too correct(the decision boundary will be tracked to the outlinear as the points at right bottom corner in figure 2) Least-squares workes for regression when we assume the target data has a Gaussian distribution and then the least-squares method maximizes the likelihood function. The distribution of targets in these classification tasks is not Gaussian.  References  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.‚Ü©Ô∏é\n   ","permalink":"https://anthony-tan.com/Least-Squares-in-Classification/","summary":"Preliminaries A Simple Linear Regression Least Squares Estimation From Linear Regression to Linear Classification pseudo-inverse  Least Squares for Classification1 Least-squares for linear regression had been talked about in ‚ÄòSimple Linear Regression‚Äô. And in this post, we want to find out whether this powerful algorithm can be used in classification.\nRecalling the distinction between the properties of classification and regression, two points need to be emphasized again(‚ÄòFrom Linear Regression to Linear Classification‚Äô):","title":"Least Squares in Classification"},{"content":"Preliminaries An Introduction to Linear Regression A Simple Linear Regression Bayesian theorem Feature extraction  Recall Linear Regression The goal of a regression problem is to find out a function or hypothesis that given an input \\(\\mathbf{x}\\), it can make a prediction \\(\\hat{y}\\) to estimate the target. Both the target \\(y\\) and prediction \\(\\hat{y}\\) here are continuous. They have the properties of numbers1:\n Consider 3 inputs \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) and \\(\\mathbf{x}_3\\) and their coresponding targets are \\(y_1=0\\), \\(y_2=1\\) and \\(y_3=2\\). Then a good predictor should give the predictions \\(\\hat{y}_1\\), \\(\\hat{y}_2\\) and \\(\\hat{y}_3\\) where the distance between \\(\\hat{y}_1\\) and \\(\\hat{y}_2\\) is larger than the one between \\(\\hat{y}_1\\) and \\(\\hat{y}_3\\)\n Some properties of regression tasks we should pay attention to are:\nThe goal of regression is to produce a hypothesis that can give a prediction as close to the target as possible The output of the hypothesis and target are continuous numbers and have numerical meanings, like distance, velocity, weights, and so on.  General Classification On the other side, we met more classification tasks in our life than regression. Such as in the supermarket we can tell the apple and the orange apart easily. And we can even verify whether this apple is tasty or not.\nThen the goal of classification is clear:\n Assign input \\(\\mathbf{x}\\) to a certain class of \\(K\\) available classes. And \\(\\mathbf{x}\\) must belong to one and only one class.\n The input \\(\\mathbf{x}\\), like the input of regression, can be a feature or basis function and can be continuous or discrete. However, its output is discrete. Let‚Äôs go back to the example that we can tell apple, orange, and pineapple apart. The difference between apple and orange and the difference between apple and pineapple can not be compared, because the distance(it is the mathematical name of difference) itself had no means.\nA Binary Code Scheme we can not calculate apple and orange directly. So a usual first step in the classification task is mapping the target or labels of an example into a number, like \\(1\\) for the apple and \\(2\\) for the orange.\nA binary code scheme is another way to code targets.\nFor a two classes mission, the numerical labels can be:\n\\[ \\mathcal{C}_1=0 \\text{ and }\\mathcal{C}_2=1\\tag{1} \\]\nIt‚Äôs equal to:\n\\[ \\mathcal{C}_1=1 \\text{ and }\\mathcal{C}_2=0\\tag{2} \\]\nAnd to a \\(K\\) classes target, the binary code scheme is:\n\\[ \\begin{aligned} \\mathcal{C}_1 \u0026amp;= \\{1,0,\\cdots,0\\}\\\\ \\mathcal{C}_2 \u0026amp;= \\{0,1,\\cdots,0\\}\\\\ \\vdots \u0026amp; \\\\ \\mathcal{C}_K \u0026amp;= \\{0,0,\\cdots,1\\}\\\\ \\end{aligned}\\tag{3} \\]\nThe \\(n\\)-dimensional input \\(\\mathbf{x}\\in \\mathbb{R}^n\\) and \\(\\mathbb{R}^n\\) is called the input space. In the classification task, the input points can be separated by the targets, and these parts of space are called decision regions and the boundaries between decision regions are called decision boundaries or decision surfaces. When the decision boundary is linear, the task is called linear classification.\nThere are roughly two kinds of procedures for classification:\nDiscriminant Function: assign input \\(\\mathbf{x}\\) to a certain class directly. We infer \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) firstly and then make a decision based on the posterior probability. Inference of \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) was calculated firstly \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) can also be calculated by Bayesian Theorem \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})=\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}{\\Pr(\\mathbf{x})}=\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}{\\sum_k \\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}\\)   They are the discriminate model and generative model, respectively.\nLinear Classification In the regression problem, the output of the linear function:\n\\[ \\mathbf{w}^T\\mathbf{x}+b\\tag{4} \\]\nis approximate of the target. But in the classification task, we want the output to be the class to which the input \\(\\mathbf{x}\\) belongs. However, the output of the linear function is always continuous. This output is more like the posterior probability, say \\(\\Pr({\\mathcal{C}_i|\\mathbf{x}})\\) rather than the discrete class label. To generate a class label output, function \\(f(\\cdot)\\) which is called ‚Äòaction function‚Äô in machine learning was employed. For example, we can choose a threshold function as the active function:\n\\[ y(\\mathbf{x})=f(\\mathbf{w}^T\\mathbf{x}+b)\\tag{5} \\]\nwhere \\(f(\\cdot)\\) is the threshold function:\n\\[ f(x) = \\begin{cases}1\u0026amp;x\\geq c\\\\0\u0026amp;\\text{otherwise}\\end{cases}\\tag{6} \\] where \\(c\\) is a constant.\nIn this case, the boundary is \\(\\mathbf{w}^T\\mathbf{x}+b = c\\), and it is a line. So we call this kind of model ‚Äòlinear classification‚Äô. The input \\(\\mathbf{x}\\) can be replaced by a basis function \\(\\phi(\\mathbf{x})\\) as mentioned in the polynomial regression.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.‚Ü©Ô∏é\n   ","permalink":"https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/","summary":"Preliminaries An Introduction to Linear Regression A Simple Linear Regression Bayesian theorem Feature extraction  Recall Linear Regression The goal of a regression problem is to find out a function or hypothesis that given an input \\(\\mathbf{x}\\), it can make a prediction \\(\\hat{y}\\) to estimate the target. Both the target \\(y\\) and prediction \\(\\hat{y}\\) here are continuous. They have the properties of numbers1:\n Consider 3 inputs \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) and \\(\\mathbf{x}_3\\) and their coresponding targets are \\(y_1=0\\), \\(y_2=1\\) and \\(y_3=2\\).","title":"From Linear Regression to Linear Classification"},{"content":"Priliminaries A Simple Linear Regression Least Squares Estimation  Extending Linear Regression with Features1 The original linear regression is in the form:\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;= b + \\mathbf{w}^T \\mathbf{x}\\\\ \u0026amp;=w_01 + w_1x_1+ w_2x_2+\\cdots + w_{m+1}x_{m+1} \\end{aligned}\\tag{1} \\]\nwhere the input vector \\(\\mathbf{x}\\) and parameter \\(\\mathbf{w}\\) are \\(m\\)-dimension vectors whose first components are \\(1\\) and bias \\(w_0=b\\) respectively. This equation is linear for both the input vector and parameter vector. Then an idea come to us, if we set \\(x_i=\\phi_i(\\mathbf{x})\\) then equation (1) convert to:\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;= b + \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x})\\\\ \u0026amp;=w_01 + w_1\\phi_1(\\mathbf{x})+\\cdots + w_{m+1}\\phi_{m+1}(\\mathbf{x}) \\end{aligned}\\tag{2} \\]\nwhere \\(\\phi(\\mathbf{x})=\\begin{bmatrix}\\phi_1(\\mathbf{x})\\\\\\phi_2(\\mathbf{x})\\\\ \\vdots\\\\\\phi_m(\\mathbf{x})\\end{bmatrix}\\) and \\(\\mathbf{w}=\\begin{bmatrix}w_1\\\\w_2\\\\ \\vdots\\\\ w_m\\end{bmatrix}\\) the function with input \\(\\mathbf{x}\\), \\(\\mathbf{\\phi}(\\cdot)\\) is called feature.\nThis feature function was used widely, especially in reducing dimensions of original input(such as in image processing) and increasing the flexibility of the predictor(such as in extending linear regression to polynomial regression).\nPolynomial Regression When we set the feature as:\n\\[ \\phi(x) = \\begin{bmatrix}x\\\\x^2\\end{bmatrix}\\tag{3} \\]\nthe linear regression converts to:\n\\[ y(\\mathbf{x})=b+ w_1x+w_2x^2\\tag{4} \\]\nHowever, the estimation of the parameter \\(\\mathbf{w}\\) is not changed by the extension of the feature function. Because in the least-squares or other optimization algorithms the parameters or random variables are \\(\\mathbf{w}\\), and we do not care about the change of input space. And when we use the algorithm described in ‚Äòleast squares estimation‚Äô:\n\\[ \\mathbf{w}=(X^TX)^{-1}X^T\\mathbf{y}\\tag{5} \\]\nto estimate the parameter, we got:\n\\[ \\mathbf{w}=(\\Phi^T\\Phi)^{-1}\\Phi^T\\mathbf{y}\\tag{6} \\]\nwhere \\[ \\Phi=\\begin{bmatrix} -\u0026amp;\\phi(\\mathbf{x_1})^T\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\phi(\\mathbf{x_m})^T\u0026amp;-\\end{bmatrix}\\tag{7} \\]\nCode for polynomial regression To the same task in the ‚Äòleast squares estimation‚Äô, regression of the weights of the newborn baby with days is like:\nThe linear regression result of a male baby is :\nAnd code of the least square polynomial regression with power \\(d\\) is\ndef fit_polynomial(self, x, y, d):  x_org = np.array(x).reshape(-1, 1)  # add a column which is all 1s to calculate bias  x = np.c_[np.ones(x.size).reshape(-1, 1), x_org]  x_org_d = x_org  # building polynomial with highest power d  for i in range(1, d):  x_org_d = x_org_d * x_org  x = np.c_[x, x_org_d]  y = np.array(y).reshape(-1, 1)  w = np.linalg.inv(x.transpose().dot(x)).dot(x.transpose()).dot(y)  return w The entire project can be found The entire project can be found https://github.com/Tony-Tan/ML and please star me.\nAnd the result of the regression is:\nThe blue regression line looks pretty well compared to the right line.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.‚Ü©Ô∏é\n   ","permalink":"https://anthony-tan.com/Polynomial-Regression-and-Features-Extension-of-Linear-Regression/","summary":"Priliminaries A Simple Linear Regression Least Squares Estimation  Extending Linear Regression with Features1 The original linear regression is in the form:\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;= b + \\mathbf{w}^T \\mathbf{x}\\\\ \u0026amp;=w_01 + w_1x_1+ w_2x_2+\\cdots + w_{m+1}x_{m+1} \\end{aligned}\\tag{1} \\]\nwhere the input vector \\(\\mathbf{x}\\) and parameter \\(\\mathbf{w}\\) are \\(m\\)-dimension vectors whose first components are \\(1\\) and bias \\(w_0=b\\) respectively. This equation is linear for both the input vector and parameter vector. Then an idea come to us, if we set \\(x_i=\\phi_i(\\mathbf{x})\\) then equation (1) convert to:","title":"Polynomial Regression and Features-Extension of Linear Regression"},{"content":"Priliminaries A Simple Linear Regression Least Squares Estimation linear algebra  Square Loss Function for Regression1 For any input \\(\\mathbf{x}\\), our goal in a regression task is to give a prediction \\(\\hat{y}=f(\\mathbf{x})\\) to approximate target \\(t\\) where the function \\(f(\\cdot)\\) is the chosen hypothesis or model as mentioned in the post https://anthony-tan.com/A-Simple-Linear-Regression/.\nThe difference between \\(t\\) and \\(\\hat{y}\\) can be called ‚Äòerror‚Äô or more precisely ‚Äòloss‚Äô. Because in an approximation task, ‚Äòerror‚Äô occurs by chance and always exists, and ‚Äòloss‚Äô is a good word to represent the difference. The loss can be written generally as function \\(\\ell(f(\\mathbf{x}),t)\\). Intuitively, the smaller the loss, the better the approximation.\nSo the expectation of loss:\n\\[ \\mathbb E[\\ell]=\\int\\int \\ell(f(\\mathbf{x}),t)p(\\mathbf{x},t)d \\mathbf{x}dt\\tag{1} \\]\nshould be as small as possible.\nIn probability viewpoint, the input vector \\(\\mathbf{x}\\), target \\(t\\) and parameters in function(model) \\(f(\\cdot)\\) are all random variables. Then the expectation of loss function may exist.\nConsidering the square error loss function \\(e=(f(\\mathbf{x})-t)^2\\), it is a usual measure of the difference between the prediction and the target. And substitute the loss function into equation (1), we have:\n\\[ \\mathbb E[\\ell]=\\int\\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)d \\mathbf{x}dt\\tag{2} \\]\nTo minimize this function, we could use Euler-Lagrange equation, Fundamental theorem of calculus and Fubini‚Äôs theorem:\nFubini‚Äôs theorem told us that we can change the order of integration: \\[ \\begin{aligned} \\mathbb E[\\ell]\u0026amp;=\\int\\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)d \\mathbf{x}dt\\\\ \u0026amp;=\\int\\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)dtd \\mathbf{x} \\end{aligned}\\tag{3} \\]\nAccording to the Euler-Lagrange equation, we first create a new function \\(G(x,f,f\u0026#39;)\\): \\[ G(x,f,f\u0026#39;)= \\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)dt\\tag{4} \\]\nThe Euler-Lagrange equation is used to minimize the equation (2): \\[ \\frac{\\partial G}{\\partial f}-\\frac{d}{dx}\\frac{\\partial G}{\\partial f\u0026#39;}=0\\tag{5} \\]\nBecause there is no \\(y\u0026#39;\\) component in function \\(G()\\). Then the equation: \\[ \\frac{\\partial G}{\\partial f}=0\\tag{6} \\] becomes the necessary condition to minimize the equation (2):\n\\[ 2\\int (f(\\mathbf{x})-t)p(\\mathbf{x},t)dt=0 \\tag{7} \\]\nRearrange the equation (7), and we get a good predictor that can minimize the square loss function :\n\\[ \\begin{aligned} \\int (f(\\mathbf{x})-t)p(\\mathbf{x},t)dt\u0026amp;=0\\\\ \\int f(\\mathbf{x})p(\\mathbf{x},t)dt-\\int tp(\\mathbf{x},t)dt\u0026amp;=0\\\\ f(\\mathbf{x})\\int p(\\mathbf{x},t)dt\u0026amp;=\\int tp(\\mathbf{x},t)dt\\\\ f(\\mathbf{x})\u0026amp;=\\frac{\\int tp(\\mathbf{x},t)dt}{\\int p(\\mathbf{x},t)dt}\\\\ f(\\mathbf{x})\u0026amp;=\\frac{\\int tp(\\mathbf{x},t)dt}{p(\\mathbf{x})}\\\\ f(\\mathbf{x})\u0026amp;=\\int tp(t|\\mathbf{x})dt\\\\ f(\\mathbf{x})\u0026amp;= \\mathbb{E}_t[t|\\mathbf{x}] \\end{aligned}\\tag{8} \\]\nWe finally find the expectation of \\(t\\) given \\(\\mathbf{x}\\) is the optimum solution. The expectation of \\(t\\) given \\(\\mathbf{x}\\) is also called the regression function.\nA small summary: \\(\\mathbb{E}[t| \\mathbf{x}]\\) is a good estimate of \\(f(\\mathbf{x})\\)\nMaximum Likelihood Estimation Generally, we assume that there is a generator behind the data:\n\\[ t=g(\\mathbf{x},\\mathbf{w})+\\epsilon\\tag{9} \\]\nwhere the function \\(g(\\mathbf{x},\\mathbf{w})\\) is a deterministic function, \\(t\\) is the target variable and \\(\\epsilon\\) is zero mean Gaussian random variable with percision \\(\\beta\\) which is the inverse variance. Because of the property of Gaussian distribution, \\(t\\) has a Gaussian distribution, with mean(expectation) \\(g(\\mathbf{x},\\mathbf{w})\\) and percesion \\(\\beta\\). And recalling the standard form of Gaussian distribution:\n\\[ \\begin{aligned} \\Pr(t|\\mathbf{x},\\mathbf{w},\\beta)\u0026amp;=\\mathcal{N}(t|g(\\mathbf{x},\\mathbf{w}),\\beta^{-1})\\\\ \u0026amp;=\\frac{\\beta}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{1}{2}(\\beta(x-\\mu)^2)} \\end{aligned}\\tag{10} \\]\nOur task here is to approximate the generator in equation (9) with a linear function. Somehow, when we use the square loss function, the optimum solution for this task is \\(\\mathbb{E}[t|\\mathbf{x}]\\) to equation (8).\nthe solution to equation (10) is:\n\\[ \\mathbb{E}[t|\\mathbf{x}]=g(\\mathbf{x},\\mathbf{w})\\tag{11} \\]\nWe set the linear model as: \\[ f(x)=\\mathbf{w}^T\\mathbf{x}+b\\tag{12} \\]\nand this can be converted to:\n\\[ f(x)= \\begin{bmatrix} b\u0026amp;\\mathbf{w}^T \\end{bmatrix} \\begin{bmatrix} 1\\\\ \\mathbf{x} \\end{bmatrix}=\\mathbf{w}_a^T\\mathbf{x}_a \\tag{13} \\]\nfor short, we just write the \\(\\mathbf{w}_a\\) and \\(\\mathbf{x}_a\\) as \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\). Then the linear model becomes:\n\\[ f(x)=\\mathbf{w}^T\\mathbf{x}\\tag{14} \\]\nAs we mentioned above we consider all the parameter as a random variable, then the conditioned distribution of \\(\\mathbf{w}\\) is \\(\\Pr(\\mathbf{w}|\\mathbf{t},\\beta)\\). \\(X\\) or \\(\\mathbf{x}\\) was omitted in the condition because it does not affect the result at all. And the Bayesian theorem told us:\n\\[ \\Pr(\\mathbf{w}|\\mathbf{t},\\beta)=\\frac{\\Pr( \\mathbf{t}|\\mathbf{w},\\beta) \\Pr(\\mathbf{w})} {\\Pr(\\mathbf{t})}=\\frac{\\text{Likelihood}\\times \\text{Prior}}{\\text{Evidence}}\\tag{15} \\]\nWe want to find the \\(\\mathbf{w}^{\\star}\\) that maximise the posterior probability \\(\\Pr(\\mathbf{w}|\\mathbf{t},\\beta)\\). Because \\(\\Pr(\\mathbf{t})\\) and \\(\\Pr(\\mathbf{w})\\) are constant. Then the maximum of likelihood \\(\\Pr(\\mathbf{t}|\\mathbf{w},\\beta)\\) maximise the posterior probability.\n\\[ \\begin{aligned} \\Pr(\\mathbf{t}|\\mathbf{w},\\beta)\u0026amp;=\\Pi_{i=0}^{N}\\mathcal{N}(t_i|\\mathbf{w}^T\\mathbf{x}_i,\\beta^{-1})\\\\ \\ln \\Pr(\\mathbf{t}|\\mathbf{w},\\beta)\u0026amp;=\\sum_{i=0}^{N}\\ln \\mathcal{N}(t_i|\\mathbf{w}^T\\mathbf{x}_i,\\beta^{-1})\\\\ \u0026amp;=\\sum_{i=0}^{N}\\ln \\frac{\\beta}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{1}{2}(\\beta(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2)}\\\\ \u0026amp;=\\sum_{i=0}^{N} \\ln \\beta - \\sum_{i=0}^{N} \\ln \\sqrt{2\\pi} - \\frac{1}{2}\\beta\\sum_{i=0}^{N}(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2 \\end{aligned}\\tag{16} \\]\nThis gives us a wonderful result.\nWe can only control the component \\(\\frac{1}{2}\\beta\\sum_{i=0}^{N}(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2\\) of the last line of equation(16), because \\(\\sum_{i=0}^{N} \\ln \\beta\\) and \\(- \\sum_{i=0}^{N} \\ln \\sqrt{2\\pi}\\) were decided by the assumptions. In other words, to maximise the likelihood, we just need to minimise:\n\\[ \\sum_{i=0}^{N}(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2\\tag{17} \\]\nThis was just to minimize the sum of squares. Then this optimization problem went back to the least square problem.\nLeast Square Estimation and Maximum Likelihood Estimation When we assume there is a generator:\n\\[ t=g(\\mathbf{x},\\mathbf{w})+\\epsilon\\tag{18} \\]\nbehind the data, and \\(\\epsilon\\) has a zero-mean Gaussian distribution with any precision \\(\\beta\\), the maximum likelihood estimation finally converts to the least square estimation. This is not only worked for linear regression because we did not assume what \\(g(\\mathbf{x},\\mathbf{w})\\) is.\nHowever, when the \\(\\epsilon\\) has a different distribution but not Gaussian distribution, the least square estimation will not be the optimum solution for maximum likelihood estimation.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.‚Ü©Ô∏é\n   ","permalink":"https://anthony-tan.com/Maximum-Likelihood-Estimation/","summary":"Priliminaries A Simple Linear Regression Least Squares Estimation linear algebra  Square Loss Function for Regression1 For any input \\(\\mathbf{x}\\), our goal in a regression task is to give a prediction \\(\\hat{y}=f(\\mathbf{x})\\) to approximate target \\(t\\) where the function \\(f(\\cdot)\\) is the chosen hypothesis or model as mentioned in the post https://anthony-tan.com/A-Simple-Linear-Regression/.\nThe difference between \\(t\\) and \\(\\hat{y}\\) can be called ‚Äòerror‚Äô or more precisely ‚Äòloss‚Äô. Because in an approximation task, ‚Äòerror‚Äô occurs by chance and always exists, and ‚Äòloss‚Äô is a good word to represent the difference.","title":"Maximum Likelihood Estimation"},{"content":"Priliminaries A Simple Linear Regression the column space  Another Example of Linear Regression 1 In the blog A Simple Linear Regression, squares of the difference between the output of a predictor and the target were used as a loss function in a regression problem. And it could be also written as:\n\\[ \\ell(\\hat{\\mathbf{y}}_i,\\mathbf{y}_i)=(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i)^T(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i) \\tag{1} \\]\nThe linear regression model in a matrix form is:\n\\[ y=\\mathbf{w}^T\\mathbf{x}+\\mathbf{b}\\tag{2} \\]\nWhat we do in this post is analyze the least-squares methods from two different viewpoints\nConsider a new training set, newborn weights, and time from the WHO:\n  Day Male(kg) Female(kg)    0 3.5 3.4  15 4.0 3.8  45 4.9 4.5  75 5.7 5.2  105 6.4 5.9  135 7.0 6.4  165 7.6 7.0  195 8.2 7.5  225 8.6 7.9  255 9.1 8.3  285 9.5 8.7  315 9.8 9.0  345 10.2 9.4  375 10.5 9.7    View of algebra This is just what the post A Simple Linear Regression did. The core idea of this view is that the loss function is quadratic so its stationary point is the minimum or maximum. Then what to do is just find the stationary point.\nAnd its result is: View of Geometric Such a simple example with just two parameters above had almost messed us up in calculation. However, the practical task may have more parameters, say hundreds or thousands of parameters. It is impossible for us to solve that in a calculus way.\nNow let‚Äôs review the linear relation in equation (2) and when we have a training set of \\(m\\) points : \\[ \\{(\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),\\dots,(\\mathbf{x}_m,y_m)\\}\\tag{3} \\]\nBecause they are sampled from an identity ‚Äúmachine‚Äù. They can be stacked together in a matrix form as:\n\\[ \\begin{bmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_m \\end{bmatrix}=\\begin{bmatrix} -\u0026amp;\\mathbf{x}_1^T\u0026amp;-\\\\ -\u0026amp;\\mathbf{x}_2^T\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\mathbf{x}_m^T\u0026amp;- \\end{bmatrix}\\mathbf{w}+I_m\\mathbf{b}\\tag{4} \\]\nwhere \\(I_m\\) is an identical matrix whose column and row is \\(m\\) and \\(\\mathbf{b}\\) is \\(b\\) repeating \\(m\\) times. To make the equation shorter and easier to calculate, we can put \\(b\\) into the vector \\(\\mathbf{w}\\) like:\n\\[ \\begin{bmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_m \\end{bmatrix}=\\begin{bmatrix} 1\u0026amp;-\u0026amp;\\mathbf{x}_1^T\u0026amp;-\\\\ 1\u0026amp;-\u0026amp;\\mathbf{x}_2^T\u0026amp;-\\\\ 1\u0026amp;\u0026amp;\\vdots\u0026amp;\\\\ 1\u0026amp;-\u0026amp;\\mathbf{x}_m^T\u0026amp;- \\end{bmatrix} \\begin{bmatrix} b\\\\ \\mathbf{w} \\end{bmatrix} \\tag{5} \\]\nWe use a simplified equation to represent the relation in equation(5): \\[ \\mathbf{y} = X\\mathbf{w}\\tag{6} \\]\nFrom the linear algebra points, equation(6) represents that \\(\\mathbf{y}\\) is in the column space of \\(X\\). However, when \\(\\mathbf{y}\\) isn‚Äôt, the equation (6) does not hold anymore. And what we need to do next is to find a vector \\(\\mathbf{\\hat{y}}\\) in the column space which is the closest one to the vector \\(\\mathbf{y}\\):\n\\[ \\arg\\min_{\\mathbf{\\hat{y}}=X\\mathbf{w}} ||\\mathbf{y}-\\mathbf{\\hat{y}}||\\tag{7} \\]\nAnd as we have known, the projection of \\(\\mathbf{y}\\) to the column space of \\(X\\) has the shortest distance to \\(\\mathbf{y}\\)\nAccording to linear algebra, the closest vector in a subspace to a vector is its projection in that subspace. Then our mission now is to find \\(\\mathbf{w}\\) to make:\n\\[ \\mathbf{\\hat{y}} = X\\mathbf{w}\\tag{8} \\]\nwhere \\(\\mathbf{\\hat{y}}\\) is the projection of \\(\\mathbf{y}\\) in the column space of \\(X\\).\nAccording to the projection equation in linear algebra:\n\\[ \\mathbf{\\hat{y}}=X(X^TX)^{-1}X^T\\mathbf{y}\\tag{22} \\]\nThen substitute equation (8) into equation (9) and assuming \\((X^TX)^{-1}\\) exists:\n\\[ \\begin{aligned} X\\mathbf{w}\u0026amp;=X(X^TX)^{-1}X^T\\mathbf{y}\\\\ X^TX\\mathbf{w}\u0026amp;=X^TX(X^TX)^{-1}X^T\\mathbf{y}\\\\ X^TX\\mathbf{w}\u0026amp;=X^T\\mathbf{y}\\\\ \\mathbf{w}\u0026amp;=(X^TX)^{-1}X^T\\mathbf{y} \\end{aligned}\\tag{10} \\]\nTo a thin and tall matrix, \\(X\\), which means that the number of sample points in the training set is far more than the dimension of a sample point, \\((X^TX)^{-1}\\) exists usually.\nCode of Linear Regression(Matrix Form) import pandas as pds import numpy as np import matplotlib.pyplot as plt   class LeastSquaresEstimation():  def __init__(self, method=\u0026#39;OLS\u0026#39;):  self.method = method   def fit(self, x, y):  x = np.array(x).reshape(-1, 1)  # add a column which is all 1s to calculate bias of linear function  x = np.c_[np.ones(x.size).reshape(-1, 1), x]  y = np.array(y).reshape(-1, 1)  if self.method == \u0026#39;OLS\u0026#39;:  w = np.linalg.inv(x.transpose().dot(x)).dot(x.transpose()).dot(y)  b = w[0][0]  w = w[1][0]  return w, b   if __name__ == \u0026#39;__main__\u0026#39;:  data_file = pds.read_csv(\u0026#39;./data/babys_weights_by_months.csv\u0026#39;)  lse = LeastSquaresEstimation()  weight_male, bias_male = lse.fit(data_file[\u0026#39;day\u0026#39;],data_file[\u0026#39;male\u0026#39;])  day_0 = data_file[\u0026#39;day\u0026#39;][0]  day_end = list(data_file[\u0026#39;day\u0026#39;])[-1]  days = np.array([day_0,day_end])  plt.scatter(data_file[\u0026#39;day\u0026#39;], data_file[\u0026#39;male\u0026#39;], c=\u0026#39;r\u0026#39;, label=\u0026#39;male\u0026#39;, alpha=0.5)  plt.scatter(data_file[\u0026#39;day\u0026#39;], data_file[\u0026#39;female\u0026#39;], c=\u0026#39;b\u0026#39;, label=\u0026#39;female\u0026#39;, alpha=0.5)  plt.xlabel(\u0026#39;days\u0026#39;)  plt.ylabel(\u0026#39;weight(kg)\u0026#39;)  plt.legend()  plt.show() the entire project can be found at https://github.com/Tony-Tan/ML and please star me üòÄ.\nIts output is also like:\nReference  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.‚Ü©Ô∏é\n   ","permalink":"https://anthony-tan.com/Least-Squares-Estimation/","summary":"Priliminaries A Simple Linear Regression the column space  Another Example of Linear Regression 1 In the blog A Simple Linear Regression, squares of the difference between the output of a predictor and the target were used as a loss function in a regression problem. And it could be also written as:\n\\[ \\ell(\\hat{\\mathbf{y}}_i,\\mathbf{y}_i)=(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i)^T(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i) \\tag{1} \\]\nThe linear regression model in a matrix form is:\n\\[ y=\\mathbf{w}^T\\mathbf{x}+\\mathbf{b}\\tag{2} \\]\nWhat we do in this post is analyze the least-squares methods from two different viewpoints","title":"Least Squares Estimation"},{"content":"Preliminaries Linear Algebra(the concepts of space, vector) Calculus An Introduction to Linear Regression  Notations of Linear Regression1 We have already created a simple linear model in the post ‚ÄúAn Introduction to Linear Regression‚Äù. According to the definition of linearity, we can develop the simplest linear regression model:\n\\[ Y\\sim w_1X+w_0\\tag{1} \\]\nwhere the symbol \\(\\sim\\) is read as ‚Äúis approximately modeled as‚Äù. Equation (1) can also be described as ‚Äúregressing \\(Y\\) on \\(X\\)(or \\(Y\\) onto \\(X\\))‚Äù.\nGo back to the example that was given in ‚ÄúAn Introduction to Linear Regression‚Äù. Combining with the equation (1), we get a model of the budget for TV advertisement and sales:\n\\[ \\text{Sales}=w_1\\times \\text{TV}+ w_0\\tag{2} \\]\nAssuming we have a machine here, which can turn grain into flour, the input is the grain, \\(X\\) in equation (1), and the output is flour, \\(Y\\). Accordingly, \\(\\mathbf{w}\\) is the gears in the machine.\nThen the mathematically model is:\n\\[ y=\\hat{w_1}x+\\hat{w_0}\\tag{3} \\]\nThe hat symbol ‚Äú\\(\\;\\hat{}\\;\\)‚Äù is used to present that this variable is a prediction, which means it is not the true value of the variable but a conjecture through certain mathematical strategies or methods else.\nThen, a new input \\(x_i\\) has its prediction:\n\\[ \\hat{y}_i=\\hat{w_1}x_i+\\hat{w_0}\\tag{4} \\]\nStatistical learning mainly studies \\(\\begin{bmatrix}\\hat{w_0}\\\\\\hat{w_1}\\end{bmatrix}\\) but machine learning concerns more about \\(\\hat{y}\\) All of them were based on the observed data.\nOnce we got this model, what we do next is estimating the parameters\nEstimating the Parameters For the advertisement task, what we have are a linear regression model equation(2) and a set of observations:\n\\[ \\{(x_1,y_1),(x_2,y_2),(x_3,y_3),\\dots,(x_n,y_n)\\}\\tag{5} \\]\nwhich is also known as training set. By the way, \\(x_i\\) in equation (5) is a sample of \\(X\\) and so is \\(y_i\\) of \\(Y\\). \\(n\\) is the size of the training set, the number of observations pairs.\nThe method we employed here is based on a measure of the ‚Äúcloseness‚Äù of the outputs of the model to the observed target (\\(y\\)s in set (5)). By far, the most used method is the ‚Äúleast squares criterion‚Äù.\nThe outputs \\(\\hat{y}_i\\) of current model(parameters) to every input \\(x_i\\) are:\n\\[ \\{(x_1,\\hat{y}_1),(x_2,\\hat{y}_2),(x_3,\\hat{y}_3),\\dots,(x_n,\\hat{y}_n)\\}\\tag{6} \\]\nand the difference between \\(\\hat{y}_i\\) and \\(y_i\\) is called residual and written as \\(e_i\\):\n\\[ e_i=y_i-\\hat{y}_i\\tag{7} \\]\n\\(y_i\\) is the target, which is the value our model is trying to achieve. So, the smaller the \\(|e_i|\\) is, the better the model is. Because the absolute operation is not a good analytic operation, we replace it with the quadratic operation:\n\\[ \\mathcal{L}_\\text{RSS}=e_1^2+e_2^2+\\dots+e_n^2\\tag{8} \\]\n\\(\\mathcal{L}_\\text{RSS}\\) means ‚ÄúResidual Sum of Squares‚Äù, the sum of total square residual. And to find a better model, we need to minimize the sum of the total residual. In machine learning, this is called loss function.\nNow we take equations (4),(7) into (8):\n\\[ \\begin{aligned} \\mathcal{L}_\\text{RSS}=\u0026amp;(y_1-\\hat{w_1}x_1-\\hat{w_0})^2+(y_2-\\hat{w_1}x_2-\\hat{w_0})^2+\\\\ \u0026amp;\\dots+(y_n-\\hat{w_1}x_n-\\hat{w_0})^2\\\\ =\u0026amp;\\sum_{i=1}^n(y_i-\\hat{w_1}x_i-\\hat{w_0})^2 \\end{aligned}\\tag{9} \\]\nTo minimize the function ‚Äú\\(\\mathcal{L}_\\text{RSS}\\)‚Äù, the calculus told us the possible minimum(maximum) points always stay at stationary points. And the stationary points are the points where the derivative of the function is zero. Remember that the minimum(maximum) points must be stationary points, but the stationary point is not necessary to be a minimum(maximum) point. For more information, ‚ÄòNumerical Optimization‚Äô is a good book.\nSince the ‚Äò\\(\\mathcal{L}_\\text{RSS}\\)‚Äô is a function of a vector \\(\\begin{bmatrix}w_0\u0026amp;w_1\\end{bmatrix}^T\\), the derivative is replaced by partial derivative. As the ‚Äò\\(\\mathcal{L}_\\text{RSS}\\)‚Äô is just a simple quadric surface, the minimum or maximum exists, and there is one and only one stationary point.\nThen our mission to find the best parameters for the regression has been converted to calculus the solution of the function system that the derivative(partial derivative) is set to zero.\nThe partial derivative of \\(\\hat{w_1}\\) is\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}=\u0026amp;-2\\sum_{i=1}^nx_i(y_i-\\hat{w_1}x_i-\\hat{w_0})\\\\ =\u0026amp;-2(\\sum_{i=1}^nx_iy_i-\\hat{w_1}\\sum_{i=1}^nx_i^2-\\hat{w_0}\\sum_{i=1}^nx_i) \\end{aligned}\\tag{10} \\]\nand derivative of \\(\\hat{w_0}\\) is:\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_0}}}=\u0026amp;-2\\sum_{i=1}^n(y_i-\\hat{w_1}x_i-\\hat{w_0})\\\\ =\u0026amp;-2(\\sum_{i=1}^ny_i-\\hat{w_1}\\sum_{i=1}^nx_i-\\sum_{i=1}^n\\hat{w_0}) \\end{aligned}\\tag{11} \\]\nSet both of them to zero and we can get:\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_0}}}\u0026amp;=0\\\\ \\hat{w_0} \u0026amp;=\\frac{\\sum_{i=1}^ny_i-\\hat{w_1}\\sum_{i=1}^nx_i}{n}\\\\ \u0026amp;=\\bar{y}-\\hat{w_1}\\bar{x} \\end{aligned}\\tag{12} \\]\nand\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}\u0026amp;=0\\\\ \\hat{w_1}\u0026amp;=\\frac{\\sum_{i=1}^nx_iy_i-\\hat{w_0}\\sum_{i=1}^nx_i}{\\sum_{i=1}^nx_i^2} \\end{aligned}\\tag{13} \\]\nTo get a equation of \\(\\hat{w_1}\\) independently, we take equation(13) to equation(12):\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}\u0026amp;=0\\\\ \\hat{w_1}\u0026amp;=\\frac{\\sum_{i=1}^nx_i(y_i-\\bar{y})}{\\sum_{i=1}^nx_i(x_i-\\bar{x})} \\end{aligned}\\tag{14} \\]\nwhere \\(\\bar{x}=\\frac{\\sum_{i=1}^nx_i}{n}\\) and \\(\\bar{y}=\\frac{\\sum_{i=1}^ny_i}{n}\\)\nBy the way, equation (14) has another form:\n\\[ \\hat{w_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})(x_i-\\bar{x})}\\tag{15} \\]\nand they are equal.\nDiagrams and Code Using python to demonstrate our result Equ. (12)(14) is correct:\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt  # load data from csv file by pandas AdvertisingFilepath=\u0026#39;./data/Advertising.csv\u0026#39; data=pd.read_csv(AdvertisingFilepath)  # convert original data to numpy array data_TV=np.array(data[\u0026#39;TV\u0026#39;]) data_sale=np.array(data[\u0026#39;sales\u0026#39;])  # calculate mean of x and y y_sum=0 y_mean=0 x_sum=0 x_mean=0 for x,y in zip(data_TV,data_sale):  y_sum+=y  x_sum+=x if len(data_sale)!=0:  y_mean=y_sum/len(data_sale) if len(data_TV)!=0:  x_mean=x_sum/len(data_TV)  # calculate w_1 w_1=0 a=0 b=0 for x,y in zip(data_TV,data_sale):  a += x*(y-y_mean)  b += x*(x-x_mean) if b!=0:  w_1=a/b  # calculate w_0 w_0=y_mean-w_1*x_mean  # draw a picture plt.xlabel(\u0026#39;TV\u0026#39;) plt.ylabel(\u0026#39;Sales\u0026#39;) plt.title(\u0026#39;TV and Sales\u0026#39;) plt.scatter(data_TV,data_sale,s=8,c=\u0026#39;g\u0026#39;, alpha=0.5) x=np.arange(-10,350,0.1) plt.plot(x,w_1*x+w_0,\u0026#39;r-\u0026#39;) plt.show() After running the code, we got:\nReference  James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.‚Ü©Ô∏é\n   ","permalink":"https://anthony-tan.com/A-Simple-Linear-Regression/","summary":"Preliminaries Linear Algebra(the concepts of space, vector) Calculus An Introduction to Linear Regression  Notations of Linear Regression1 We have already created a simple linear model in the post ‚ÄúAn Introduction to Linear Regression‚Äù. According to the definition of linearity, we can develop the simplest linear regression model:\n\\[ Y\\sim w_1X+w_0\\tag{1} \\]\nwhere the symbol \\(\\sim\\) is read as ‚Äúis approximately modeled as‚Äù. Equation (1) can also be described as ‚Äúregressing \\(Y\\) on \\(X\\)(or \\(Y\\) onto \\(X\\))‚Äù.","title":"A Simple Linear Regression"},{"content":"Preliminariess Linear Algebra(the concepts of space, vector) Calculus  What is Linear Regression Linear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors).\nMachine Learning and Statistical Learning Machine learning and statistical learning are similar but have some distinctions. In machine learning, models, regression models, or classification models, are used to predict the outputs of the new incoming inputs.\nIn contrast, in statistical learning, regression and classification are employed to model the data to find out the hidden relations among the inputs. In other words, the models of data, no matter they are regression or classification, or something else. They are used to analyze the mechanism behind the data.\n‚ÄúLinear‚Äù Linear is a property of the operation \\(f(\\cdot)\\), which has the following two properties:\n\\(f(a\\mathbf{x})=af(\\mathbf{x})\\) \\(f(\\mathbf{x}+\\mathbf{y})=f(\\mathbf{x})+f(\\mathbf{y})\\)  where \\(a\\) is a scalar. Then we say \\(f(\\cdot)\\) is linear or \\(f(\\cdot)\\) has a linearity property.\nThe linear operation can be represented as a matrix. And when a 2-dimensional linear operation was drawn on the paper, it is a line. Maybe that is why it is named linear, I guess.\n‚ÄúRegression‚Äù In statistical or machine learning, regression is a crucial part of the whole field. And, the other part is the well-known classification. If we have a close look at the outputs data type, one distinction between them is that the output of regression is continuous but the output of classification is discrete.\nWhat is linear regression Linear regression is a regression model. All parameters in the model are linear, like:\n\\[ f(\\mathbf{x})=w_1x_1+w_2x_2+w_3x_3\\tag{1} \\]\nwhere the \\(w_n\\) where \\(n=1,2,3\\) are the parameters of the model, the output \\(f(\\mathbf{x})\\) can be written as \\(t\\) for 1-deminsional outputs (or \\(\\mathbf{t}\\) for multi-deminsional outputs).\n\\(f(\\mathbf{w})\\) is linear:\n\\[ \\begin{aligned} f(a\\cdot\\mathbf{w})\u0026amp;=aw_1x_1+aw_2x_2+aw_3x_3=a\\cdot f(\\mathbf{w}) \\\\ f(\\mathbf{w}+\\mathbf{v})\u0026amp;=(w_1+v_1)x_1+(w_2+v_2)x_2+(w_3+v_3)x_3\\\\ \u0026amp;=w_1x_1+v_1x_1+w_2x_2+v_2x_2+w_3x_3+v_3x_3\\\\ \u0026amp;=f(\\mathbf{w})+f(\\mathbf{v}) \\end{aligned}\\tag{2} \\]\nwhere \\(a\\) is a scalar, and \\(\\mathbf{v}\\) is in the same space with \\(\\mathbf{w}\\)\nQ.E.D\nThere is also another view that the linear property of models is also for \\(\\mathbf{x}\\), the input.\nBut the following model\n\\[ t=f(\\mathbf{x})=w_1\\log(x_1)+w_2\\sin(x_2)\\tag{3} \\]\nis a case of linear regression problem in our definition. But from the second point of view, it is not linear for inputs \\(\\mathbf{x}\\). However, this is not an unsolvable contradiction. If we use:\n\\[ y_1= \\log(x_1)\\\\ y_2= \\sin(x_2)\\tag{4} \\]\nto replace the \\(\\log\\) and \\(\\sin\\) in equation (3), we get again\n\\[ t=f(\\mathbf{y})=w_1y_1+w_2y_2\\tag{5} \\]\na linear operation for both input \\(\\mathbf{y}=\\begin{bmatrix}y_1\\;y_2\\end{bmatrix}^T\\) and parameters \\(\\mathbf{z}\\) .\nThe tranformation, equation(4), is called feature extraction. \\(\\mathbf{y}\\) is called features, and \\(\\log\\) and \\(\\sin\\) are called basis functions\nAn Example This example is taken from (James20131), It is about the sale between different kinds of advertisements. I downloaded the data set from http://faculty.marshall.usc.edu/gareth-james/ISL/data.html. It‚Äôs a CSV file, including 200 rows. Here I draw 3 pictures using ‚Äòmatplotlib‚Äô to make the data more visible. They are advertisements for ‚ÄòTV‚Äô, ‚ÄòRadio‚Äô, ‚ÄòNewspaper‚Äô to ‚ÄòSales‚Äô respectively.\nFrom these figures, we can find TV ads and Sales looks like having a stronger relationship than radio ads and sales. However, the Newspaper ads and Sales look independent.\nFor statistical learning, we should take statistical methods to investigate the relation in the data. And in machine learning, to a certain input, predicting an output is what we are concerned.\nWhy Linear Regression Linear regression has been used for more than 200 years, and it‚Äôs always been our first class of statistical learning or machine learning. Here we list 3 practical elements of linear regression, which are essential for the whole subject:\nIt is still working in some areas. Although more complicated models have been built, they could not be replaced totally. It is a good jump-off point to the other more feasible and adorable models, which may be an extension or generation of naive linear regression Linear regression is easy, so it is possible to be analyzed mathematically.  This is why linear regression is always our first step to learn machine learning and statistical learning. And by now, this works pretty well.\nA Probabilistic View Machine learning or statistical learning can be described from two different views - Bayesian and Frequentist. They both worked well for some different instances, but they also have their limitations. The Bayesian view of the linear regression will be talked about as well later.\nBayesian statisticians thought the input \\(\\mathbf{x}\\), the output \\(t\\), and the parameter \\(\\mathbf{w}\\) are all random variables, while the frequentist does not think so. Bayesian statisticians predict the unknown input \\(\\mathbf{x}_0\\) by forming the distribution \\(\\mathbb{P}(t_0|\\mathbf{x}_0)\\) and then sampling from it. To achieve this goal, we must build the \\(\\mathbb{P}(t|\\mathbf{x})\\) firstly. This is the modeling progress, or we can call it learning progress.\nReferences  James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.‚Ü©Ô∏é\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Linear-Regression/","summary":"Preliminariess Linear Algebra(the concepts of space, vector) Calculus  What is Linear Regression Linear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors).\nMachine Learning and Statistical Learning Machine learning and statistical learning are similar but have some distinctions. In machine learning, models, regression models, or classification models, are used to predict the outputs of the new incoming inputs.","title":"An Introduction to Linear Regression"},{"content":"About Me Hi, I\u0026rsquo;m Anthony Tan(Ë∞≠Âçá). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years. So far, I\u0026rsquo;ve learned and reviewed some papers that are necessary for reinforcement learning research, and some mathematics, like calculus, linear algebra, probability, and so on.\nHowever the bigger one in the picture is me, Tony, and the smaller one is my dog, Potato(He was too young to take a shower when we take the picture, and now he is no more a dirty puppyüòÄ)\nWhy the blogs These blogs here are used to simply explain what I have learned, according to the Feyman Technique. And blogging what I\u0026rsquo;ve just learned is the most important part of learning. The whole process is:\n Choosing a concept or theory I would like to know(collecting necessary materials )  Outlining what prior knowledge of this concept or theory Taking note of this prior knowledge   Try to explain the new theory to readers of the website without any new words and concepts in the theory (draft) Go back to the source and fill in the gap in understanding Simplify the explaining(rewrite and post)  What in blogs These blogs contain:\n Mathematics Neuroscience Algorithms  Deep Learning Reinforcement Learning    And some of these posts might also be represented by videos on my YouTube channel.\n","permalink":"https://anthony-tan.com/about/","summary":"About Me Hi, I\u0026rsquo;m Anthony Tan(Ë∞≠Âçá). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years.","title":""}]