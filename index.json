[{"content":"Preliminaries Committee  Boosting1 The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:\nperformances better than any of the base classifiers and each base classifier has a different weight factor  Adaboost Adaboost is short for adaptive boosting. It is a method combining several weak classifiers which are just better than random guesses and it gives a better performance than the committee. The base classifiers in AdaBoost are trained sequentially, and their training set is the same but with different weights for each sample. So when we consider the distribution of training data, every weak classifier was trained on different sample distribution. This might be an important reason for the improvement of AdaBoost from the committee. And the weights for weak classifiers are generated depending on the performance of the previous classifier.\nDuring the prediction process, the input data flows from classifier to classifier and the final result is some kind of combination of all output of weak classifiers.\nImportant ideas in the AdaBoost algorithm are:\nthe data points are predicted incorrectly in the current classifier giving a greater weight once the algorithm was trained, the prediction of each classifier is combined through a weighted majority voting scheme as:  where \\(w_n^{(1)}\\) is the initial weights of input data of the \\(1\\) st weak classifier, \\(y_1(x)\\) is the prediction of the \\(1\\) st weak classifier, \\(\\alpha_m\\) is the weight of each prediction(notably this weight works to \\(y_m(x)\\) and \\(w_n^{(1)}\\) is the weight of input data of the first classifier.). And the final output is the sign function of the weighted sum of all predictions.\nThe procedure of the algorithm is:\n Initial data weighting coefficients \\(\\{\\boldsymbol{w}_n\\}\\) by \\(w_n^{(1)}=\\frac{1}{N}\\) for \\(n=1,2,\\cdots,N\\) For \\(m=1,\\dots,M\\):   Fit a classifier \\(y_m(\\boldsymbol{x})\\) to training set by minimizing the weighted error function: \\[J_m=\\sum_{}^{}w_n^{(m)}I(y_m(\\boldsymbol{x}_n)\\neq t_n)\\] where \\(I(y_m(\\boldsymbol{x})\\neq t_n)\\) is the indicator function and equals 1 when \\(y_m(\\boldsymbol{x})\\neq t_n\\) and 0 otherwise Evaluate the quatities: \\[\\epsilon_m=\\frac{\\sum_{n=1}^Nw_n^{(m)}I(y_m(\\boldsymbol{x})\\neq t_n)}{\\sum_{n=1}^{N}w_n^{(m)}}\\] and then use this to evaluate \\(\\alpha_m=\\ln \\{\\frac{1-\\epsilon_m}{\\epsilon_m}\\}\\) Updata the data weighting coefficients: \\[w_n^{(m+1)}=w_n^{(m)}\\exp\\{\\alpha_mI(y_m(\\boldsymbol{x})\\neq t_n)\\}\\]  Make predictions using the final model, which is given by: \\[Y_M = \\mathrm{sign} (\\sum_{m=1}^{M}\\alpha_my_m(x))\\]   This procedure comes from ‘Pattern recognition and machine learning’2\nPython Code of Adaboost # weak classifier # test each dimension and each value and each direction to find a # best threshold and direction(\u0026#39;\u0026lt;\u0026#39; or \u0026#39;\u0026gt;\u0026#39;) class Stump():  def __init__(self):  self.feature = 0  self.threshold = 0  self.direction = \u0026#39;\u0026lt;\u0026#39;   def loss(self,y_hat, y, weights):  \u0026quot;\u0026quot;\u0026quot; :param y_hat: prediction :param y: target :param weights: weight of each data :return: loss \u0026quot;\u0026quot;\u0026quot;  sum = 0  example_size = y.shape[0]  for i in range(example_size):  if y_hat[i] != y[i]:  sum += weights[i]  return sum   def test_in_traing(self, x, feature, threshold, direction=\u0026#39;\u0026lt;\u0026#39;):  \u0026quot;\u0026quot;\u0026quot; test during training :param x: input data :param feature: classification on which dimension :param threshold: threshold :param direction: \u0026#39;\u0026lt;\u0026#39; or \u0026#39;\u0026gt;\u0026#39; to threshold :return: classification result \u0026quot;\u0026quot;\u0026quot;  example_size = x.shape[0]  classification_result = -np.ones(example_size)  for i in range(example_size):  if direction == \u0026#39;\u0026lt;\u0026#39;:  if x[i][feature] \u0026lt; threshold:  classification_result[i] = 1  else:  if x[i][feature] \u0026gt; threshold:  classification_result[i] = 1  return classification_result   def test(self,x):  \u0026quot;\u0026quot;\u0026quot; test during prediction :param x: input :return: classification result \u0026quot;\u0026quot;\u0026quot;  return self.test_in_traing(x, self.feature, self.threshold, self.direction)   def training(self, x, y, weights):  \u0026quot;\u0026quot;\u0026quot; main training process :param x: input :param y: target :param weights: weights :return: none \u0026quot;\u0026quot;\u0026quot;  example_size = x.shape[0]  example_dimension = x.shape[1]  loss_matrix_less = np.zeros(np.shape(x))  loss_matrix_more = np.zeros(np.shape(x))  for i in range(example_dimension):  for j in range(example_size):  results_ji_less = self.test_in_traing(x, i, x[j][i], \u0026#39;\u0026lt;\u0026#39;)  results_ji_more = self.test_in_traing(x, i, x[j][i], \u0026#39;\u0026gt;\u0026#39;)  loss_matrix_less[j][i] = self.loss(results_ji_less, y, weights)  loss_matrix_more[j][i] = self.loss(results_ji_more, y, weights)  loss_matrix_less_min = np.min(loss_matrix_less)  loss_matrix_more_min = np.min(loss_matrix_more)  if loss_matrix_less_min \u0026gt; loss_matrix_more_min:  minimum_position = np.where(loss_matrix_more == loss_matrix_more_min)  self.threshold = x[minimum_position[0][0]][minimum_position[1][0]]  self.feature = minimum_position[1][0]  self.direction = \u0026#39;\u0026gt;\u0026#39;  else:  minimum_position = np.where(loss_matrix_less == loss_matrix_less_min)  self.threshold = x[minimum_position[0][0]][minimum_position[1][0]]  self.feature = minimum_position[1][0]  self.direction = \u0026#39;\u0026lt;\u0026#39;   class Adaboost():  def __init__(self, maximum_classifier_size):  self.max_classifier_size = maximum_classifier_size  self.classifiers = []  self.alpha = np.ones(self.max_classifier_size)   def training(self, x, y, classifier_class):  \u0026quot;\u0026quot;\u0026quot; training adaboost main steps :param x: input :param y: target :param classifier_class: what can classifier would be used, here we use stump above :return: none \u0026quot;\u0026quot;\u0026quot;  example_size = x.shape[0]  weights = np.ones(example_size)/example_size   for i in range(self.max_classifier_size):  classifier = classifier_class()  classifier.training(x, y, weights)  test_res = classifier.test(x)  indicator = np.zeros(len(weights))  for j in range(len(indicator)):  if test_res[j] != y[j]:  indicator[j] = 1   cost_function = np.sum(weights*indicator)  epsilon = cost_function/np.sum(weights)  self.alpha[i] = np.log((1-epsilon)/epsilon)  self.classifiers.append(classifier)  weights = weights * np.exp(self.alpha[i]*indicator)   def predictor(self, x):  \u0026quot;\u0026quot;\u0026quot; prediction :param x: input data :return: prediction result \u0026quot;\u0026quot;\u0026quot;  example_size = x.shape[0]  results = np.zeros(example_size)  for i in range(example_size):  y = np.zeros(self.max_classifier_size)  for j in range(self.max_classifier_size):  y[j] = self.classifiers[j].test(x[i].reshape(1,-1))  results[i] = np.sign(np.sum(self.alpha*y))  return results the entire project can be found https://github.com/Tony-Tan/ML. And please star me! Thanks!\nWhen we use different numbers of classifiers, the results of the algorithm are like this:\nwhere the blue circles are the correct classification of class 1 and red circles are the correct classification of class 2. And the blue crosses belong to class 2 but were classified into class 1, and so do the red crosses.\nA 40-classifiers AdaBoost gives a relatively good prediction:\nwhere there is only one misclassified point.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Boosting-and-AdaBoost/","summary":"Preliminaries Committee  Boosting1 The committee has an equal weight for every prediction from all models, and it gives little improvement than a single model. Then boosting was built for this problem. Boosting is a technique of combining multiple ‘base’ classifiers to produce a form of the committee that:\nperformances better than any of the base classifiers and each base classifier has a different weight factor  Adaboost Adaboost is short for adaptive boosting.","title":"Boosting and AdaBoost"},{"content":"Preliminaries Basic machine learning concepts Probability Theory concepts   expectation correlated random variable  Analysis of Committees1 The committee is a native inspiration for how to combine several models(or we can say how to combine the outputs of several models). For example, we can combine all the models by:\n\\[ y_{COM}(X)=\\frac{1}{M}\\sum_{m=1}^My_m(X)\\tag{1} \\]\nThen we want to find out whether this average prediction of models is better than every one of them.\nTo compare the committee and a single model, we need first to build a criterion depending on which we can distinguish which result is better. Assuming that the true generator of the training data \\(x\\) is:\n\\[ h(x)\\tag{2} \\]\nSo our prediction of the \\(m\\)th model for \\(m=1,2\\cdots,M\\) can be represented as:\n\\[ y_m(x) = h(x) +\\epsilon_m(x)\\tag{3} \\] where \\(\\epsilon_m(x)\\) is the error of \\(m\\) th model.Then the average sum-of-squares of error can be a nice criterion.\nThe criterion of a single model is:\n\\[ \\mathbb{E}_x[(y_m(x)-h(x))^2] = \\mathbb{E}_x[\\epsilon_m(x)^2] \\tag{4} \\]\nwhere the \\(\\mathbb{E}[\\cdot]\\) is the frequentist expectation(or average for usual saying).\nNow we consider the average of the error over \\(M\\) models:\n\\[ E_{AV} = \\frac{1}{M}\\sum_{m=1}^M\\mathbb{E}_x[\\epsilon_m(x)^2]\\tag{5} \\]\nAnd on the other hand, the committees have the error given by equations (1), (3), and (4):\n\\[ \\begin{aligned} E_{COM}\u0026amp;=\\mathbb{E}_x[(\\frac{1}{M}\\sum_{m=1}^My_m(x)-h(x))^2] \\\\ \u0026amp;=\\mathbb{E}_x[\\{\\frac{1}{M}\\sum_{m=1}^M\\epsilon_m(x)\\}^2] \\end{aligned} \\tag{6} \\]\nNow we assume that the random variables \\(\\epsilon_i(x)\\) for \\(i=1,2,\\cdots,M\\) have mean 0 and uncorrelated, so that:\n\\[ \\begin{aligned} \\mathbb{E}_x[\\epsilon_m(x)]\u0026amp;=0 \u0026amp;\\\\ \\mathbb{E}_x[\\epsilon_m(x)\\epsilon_l(x)]\u0026amp;=0,\u0026amp;m\\neq l \\end{aligned} \\tag{7} \\]\nThen substitute equation (7) into equation (6), we can get:\n\\[ E_{COM}=\\frac{1}{M^2}\\mathbb{E}_x[\\epsilon_m(x)]\\tag{8} \\]\nAccording to the equation (5) and (8):\n\\[ E_{AV}=\\frac{1}{M}E_{COM}\\tag{9} \\]\nAll the mathematics above is based on the assumption that the error of each model is uncorrelated. However, most time they are highly correlated and the reduction of error is generally small. But the relation:\n\\[ E_{COM}\\leq E_{AV}\\tag{10} \\]\nexists definitely which means committees can produce better predictions than a single model.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Committees/","summary":"Preliminaries Basic machine learning concepts Probability Theory concepts   expectation correlated random variable  Analysis of Committees1 The committee is a native inspiration for how to combine several models(or we can say how to combine the outputs of several models). For example, we can combine all the models by:\n\\[ y_{COM}(X)=\\frac{1}{M}\\sum_{m=1}^My_m(X)\\tag{1} \\]\nThen we want to find out whether this average prediction of models is better than every one of them.","title":"Committees"},{"content":"Preliminaries Bayesian Theorem  Bayesian Model Averaging(BMA)1 Bayesian model averaging(BMA) is another wildly used method that is very like a combining model. However, the difference between BMA and combining models is also significant.\nA Bayesian model averaging is a Bayesian formula in which the random variable are models(hypothesizes) \\(h=1,2,\\cdots,H\\) with prior probability \\(\\Pr(h)\\), then the marginal distribution over data \\(X\\) is:\n\\[ \\Pr(X)=\\sum_{h=1}^{H}\\Pr(X|h)\\Pr(h) \\]\nAnd the MBA is used to select a model(hypothesis) that can model the data best through Bayesian theory. When we have a larger size of \\(X\\), the posterior probability\n\\[ \\Pr(h|X)=\\frac{\\Pr(X|h)\\Pr(h)}{\\sum_{i=1}^{H}\\Pr(X|i)\\Pr(i)} \\]\nbecome sharper. Then we got a good hypothesis.\nMixture of Gaussian(Combining Models) In post ‘Mixtures of Gaussians’, we have seen how a mixture of Gaussians works. Then the joint distribution of input data \\(\\mathbf{x}\\) and latent variable \\(\\mathbf{z}\\) is:\n\\[ \\Pr(\\mathbf{x},\\mathbf{z}) \\]\nand the margin distribution of \\(\\mathbf{x}\\) is\n\\[ \\Pr(\\mathbf{x})=\\sum_{\\mathbf{z}}\\Pr(\\mathbf{x},\\mathbf{z}) \\]\nFor the mixture of Gaussians: \\[ \\Pr(\\mathbf{x})=\\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k) \\] the latent variable \\(\\mathbf{z}\\) is designed: \\[ \\Pr(z_k) = \\pi_k \\] for \\(k=\\{1,2,\\cdots,K\\}\\). And \\(z_k\\in\\{0,1\\}\\) is a \\(1\\)-of-\\(K\\) representation.\nThis mixture of Gaussians is a kind of combining models. Each time, only one \\(k\\) is selected(for \\(\\mathbf{z}\\) is \\(1\\)-of-\\(K\\) representation). An example of a mixture of Gaussians, and its original curve is like:\nAnd the latent variables \\(\\mathbf{z}\\) separate the whole distribution into several Gaussian distributions:\nThis is the simplest model of combining models where each expert is a Gaussian model. And during the voting, only one model was selected by \\(\\mathbf{z}\\) to make the final decision.\nDistinction between BMA and Combining Methods A combining model method contains several models and predicts by voting or other rules. However, Bayesian model averaging can be used to generate a hypothesis from several candidates.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Bayesian-Model-Averaging-and-Combining-Models/","summary":"Preliminaries Bayesian Theorem  Bayesian Model Averaging(BMA)1 Bayesian model averaging(BMA) is another wildly used method that is very like a combining model. However, the difference between BMA and combining models is also significant.\nA Bayesian model averaging is a Bayesian formula in which the random variable are models(hypothesizes) \\(h=1,2,\\cdots,H\\) with prior probability \\(\\Pr(h)\\), then the marginal distribution over data \\(X\\) is:\n\\[ \\Pr(X)=\\sum_{h=1}^{H}\\Pr(X|h)\\Pr(h) \\]\nAnd the MBA is used to select a model(hypothesis) that can model the data best through Bayesian theory.","title":"Bayesian Model Averaging(BMA) and Combining Models"},{"content":"Preliminaries ‘Mixtures of Gaussians’ Basic machine learning concepts  Combining Models1 The mixture of Gaussians had been discussed in the post ‘Mixtures of Gaussians’. It was used to introduce the ‘EM algorithm’ but it gave us the inspiration of improving model performance.\nAll models we have studied, besides neural networks, are all single-distribution models. That is just like that, to solve a problem we invite an expert who is very good at this kind of problem, then we just do whatever the expert said. However, if our problem is too hard that no expert can solve it completely by himself, inviting more experts is a good choice. This inspiration gives a new way to improve performance by combining multiple models but not just by improving the performance of a single model.\nOrganising Models A naive idea is voting by several models equally, which means averaging the prediction of all models. However, different models may have different abilities, and voting equally is not a good idea. Then boosting and other methods were introduced.\nIn some combining methods, such as AdaBoost(boosting), bootstrap, bagging, and e.t.c, the input data has an identical distribution with the training set. However, in some methods, the training set is cut into several subsets with different distributions from the original training set. The decision tree is such a method. A decision tree is a sequence of binary selection and it worked well in both regression and classification tasks.\nWe will briefly discuss: - committees - boosting - decision tree\nin the following posts.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Combining-Models/","summary":"Preliminaries ‘Mixtures of Gaussians’ Basic machine learning concepts  Combining Models1 The mixture of Gaussians had been discussed in the post ‘Mixtures of Gaussians’. It was used to introduce the ‘EM algorithm’ but it gave us the inspiration of improving model performance.\nAll models we have studied, besides neural networks, are all single-distribution models. That is just like that, to solve a problem we invite an expert who is very good at this kind of problem, then we just do whatever the expert said.","title":"An Introduction to Combining Models"},{"content":"Preliminaries Gaussian distribution log-likelihood Calculus  partial derivative Lagrange multiplier   EM Algorithm for Gaussian Mixture1 Analysis Maximizing likelihood could not be used in the Gaussian mixture model directly, because of its severe defects which we have come across at ‘Maximum Likelihood of Gaussian Mixtures’. With the inspiration of K-means, a two-step algorithm was developed.\nThe objective function is the log-likelihood function:\n\\[ \\begin{aligned} \\ln \\Pr(\\mathbf{x}|\\mathbf{\\pi},\\mathbf{\\mu},\\Sigma)\u0026amp;=\\ln (\\Pi_{n=1}^N\\sum_{j=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k))\\\\ \u0026amp;=\\sum_{n=1}^{N}\\ln \\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)\\\\ \\end{aligned}\\tag{1} \\] ### \\(\\mu_k\\)\nThe condition that must be satisfied at a maximum of log-likelihood is the derivative(partial derivative) of parameters are \\(0\\). So we should calculate the partial derivatives of \\(\\mu_k\\):\n\\[ \\begin{aligned} \\frac{\\partial \\ln \\Pr(X|\\pi,\\mu,\\Sigma)}{\\partial \\mu_k}\u0026amp;=\\sum_{n=1}^N\\frac{-\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)\\Sigma_k^{-1}(\\mathbf{x}_n-\\mathbf{\\mu}_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}\\\\ \u0026amp;=-\\sum_{n=1}^N\\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}\\Sigma_k^{-1}(\\mathbf{x}_n-\\mathbf{\\mu}_k) \\end{aligned}\\tag{2} \\]\nand then set equation (2) equal to 0 and rearrange it as:\n\\[ \\begin{aligned} \\sum_{n=1}^N\\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}\\mathbf{x}_n\u0026amp;=\\sum_{n=1}^N\\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}\\mathbf{\\mu}_k \\end{aligned}\\tag{3} \\]\nIn the post ‘Mixtures of Gaussians’, we had defined:\n\\[ \\gamma_{nk}=\\Pr(k=1|\\mathbf{x}_n)=\\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}\\tag{4} \\] as responsibility. And substitute equation(4) into equation(3):\n\\[ \\begin{aligned} \\sum_{n=1}^N\\gamma_{nk}\\mathbf{x}_n\u0026amp;=\\sum_{n=1}^N\\gamma_{nk}\\mathbf{\\mu}_k\\\\ \\sum_{n=1}^N\\gamma_{nk}\\mathbf{x}_n\u0026amp;=\\mathbf{\\mu}_k\\sum_{n=1}^N\\gamma_{nk}\\\\ {\\mu}_k\u0026amp;=\\frac{\\sum_{n=1}^N\\gamma_{nk}\\mathbf{x}_n}{\\sum_{n=1}^N\\gamma_{nk}} \\end{aligned}\\tag{5} \\]\nand to simplify equation (5) we define:\n\\[ N_k = \\sum_{n=1}^N\\gamma_{nk}\\tag{6} \\]\nThen the equation (5) can be simplified as:\n\\[ {\\mu}_k=\\frac{1}{N_k}\\sum_{n=1}^N\\gamma_{nk}\\mathbf{x}_n\\tag{7} \\]\n\\(\\Sigma_k\\) The same calcualtion would be done to \\(\\frac{\\partial \\ln \\Pr(X|\\pi,\\mu,\\Sigma)}{\\partial \\Sigma_k}=0\\) :\n\\[ \\Sigma_k = \\frac{1}{N_k}\\sum_{n=1}^N\\gamma_{nk}(\\mathbf{x}_n - \\mathbf{\\mu_k})(\\mathbf{x}_n - \\mathbf{\\mu_k})^T\\tag{8} \\]\n\\(\\pi_k\\) However, the situation of \\(\\pi_k\\) is a little complex, for it has a constrain:\n\\[ \\sum_k^K \\pi_k = 1 \\tag{9} \\]\nthen Lagrange multiplier is employed and the objective function is:\n\\[ \\ln \\Pr(X|\\mathbf{\\pi},\\mathbf{\\mu},\\Sigma)+\\lambda (\\sum_k^K \\pi_k-1)\\tag{10} \\]\nand set the partial derivative of equation (10) to \\(\\pi_k\\) to 0:\n\\[ 0 = \\sum_{n=1}^N\\frac{\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}+\\lambda\\tag{11} \\]\nAnd multiply both sides by \\(\\pi_k\\) and sum over \\(k\\):\n\\[ \\begin{aligned} 0 \u0026amp;= \\sum_{k=1}^K(\\sum_{n=1}^N\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}+\\lambda\\pi_k)\\\\ 0\u0026amp;=\\sum_{k=1}^K\\sum_{n=1}^N\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}+\\sum_{k=1}^K\\lambda\\pi_k\\\\ 0\u0026amp;=\\sum_{n=1}^N\\sum_{k=1}^K\\gamma_{nk}+\\lambda\\sum_{k=1}^K\\pi_k\\\\ \\lambda \u0026amp;= -N \\end{aligned}\\tag{12} \\]\nthe last step of equation(12) is because \\(\\sum_{k=1}^K\\pi_k=1\\) and \\(\\sum_{k=1}^K\\gamma_{nk}=1\\)\nThen we substitute equation(12) into eqa(11): \\[ \\begin{aligned} 0 \u0026amp;= \\sum_{n=1}^N\\frac{\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)}-N\\\\ N \u0026amp;= \\frac{1}{\\pi_k}\\sum_{n=1}^N\\gamma_{nk}\\\\ \\pi_k\u0026amp;=\\frac{N_k}{N} \\end{aligned}\\tag{13} \\]\nthe last step of equation (13) is because of the definition of equation (6).\nAlgorithm Equations (5), (8), and (13) could not construct a closed-form solution. The reason is that for example in equation (5), both side of the equation contains parameter \\(\\mu_k\\).\nHowever, the equations suggest an iterative scheme for finding a solution which includes two-step: expectation and maximization:\nE step: calculating the posterior probability of equation (4) with the current parameter M step: update parameters by equations (5), (8), and (13)  The initial value of the parameters could be randomly selected. But some other tricks are always used, such as K-means. And the stop conditions can be one of:\nincrease of log-likelihood falls below some threshold change of parameters less than some threshold.  Python Code for EM The input data should be normalized as what we did in ‘K-means algorithm’\ndef Gaussian( x, u, variance):  k = len(x)  return np.power(2*np.pi, -k/2.)*np.power(np.linalg.det(variance),  -1/2)*np.exp(-0.5*(x-u).dot(np.linalg.inv(variance)).dot((x-u).transpose()))   class EM():  def mixed_Gaussian(self,x,pi,u,covariance):  res = 0  for i in range(len(pi)):  res += pi[i]*Gaussian(x,u[i],covariance[i])  return res   def clusturing(self, x, d, initial_method=\u0026#39;K_Means\u0026#39;):  data_dimension = x.shape[1]  data_size = x.shape[0]  if initial_method == \u0026#39;K_Means\u0026#39;:  km = k_means.K_Means()  # k_means initial mean vector, each row is a mean vector\u0026#39;s transpose  centers, cluster_for_each_point = km.clusturing(x, d)  # initial latent variable pi  pi = np.ones(d)/d  # initial covariance  covariance = np.zeros((d,data_dimension,data_dimension))  for i in range(d):  covariance[i] = np.identity(data_dimension)/10.0  # calculate responsibility  responsibility = np.zeros((data_size,d))  log_likelihood = 0  log_likelihood_last_time = 0  for dummy in range(1,1000):  log_likelihood_last_time = log_likelihood  # E step:  # points in each class  k_class_dict = {i: [] for i in range(d)}  for i in range(data_size):  responsibility_numerator = np.zeros(d)  responsibility_denominator = 0  for j in range(d):  responsibility_numerator[j] = pi[j]*Gaussian(x[i],centers[j],covariance[j])  responsibility_denominator += responsibility_numerator[j]  for j in range(d):  responsibility[i][j] = responsibility_numerator[j]/responsibility_denominator   # M step:  N_k = np.zeros(d)  for j in range(d):  for i in range(data_size):  N_k[j] += responsibility[i][j]  for i in range(d):  # calculate mean  # sum of responsibility multiply x  sum_r_x = 0  for j in range(data_size):  sum_r_x += responsibility[j][i]*x[j]  if N_k[i] != 0:  centers[i] = 1/N_k[i]*sum_r_x  # covariance  # sum of responsibility multiply variance  sum_r_v = np.zeros((data_dimension,data_dimension))  for j in range(data_size):  temp = (x[j]-centers[i]).reshape(1,-1)  temp_T = (x[j]-centers[i]).reshape(-1,1)  sum_r_v += responsibility[j][i]*(temp_T.dot(temp))  if N_k[i] != 0:  covariance[i] = 1 / N_k[i] * sum_r_v  # latent pi  pi[i] = N_k[i]/data_size  log_likelihood = 0  for i in range(data_size):  log_likelihood += np.log(self.mixed_Gaussian(x[i], pi, centers, covariance))   if np.abs(log_likelihood - log_likelihood_last_time)\u0026lt;0.001:  break  print(log_likelihood_last_time)  return pi,centers,covariance The entire project can be found https://github.com/Tony-Tan/ML and please star me.\nThe progress of EM(initial with K-means):\nand the final result is:\nwhere the ellipse represents the covariance matrix and the axes of the ellipse are in the direction of eigenvectors of the covariance matrix, and their length is corresponding eigenvalues.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/EM-Algorithm/","summary":"Preliminaries Gaussian distribution log-likelihood Calculus  partial derivative Lagrange multiplier   EM Algorithm for Gaussian Mixture1 Analysis Maximizing likelihood could not be used in the Gaussian mixture model directly, because of its severe defects which we have come across at ‘Maximum Likelihood of Gaussian Mixtures’. With the inspiration of K-means, a two-step algorithm was developed.\nThe objective function is the log-likelihood function:\n\\[ \\begin{aligned} \\ln \\Pr(\\mathbf{x}|\\mathbf{\\pi},\\mathbf{\\mu},\\Sigma)\u0026amp;=\\ln (\\Pi_{n=1}^N\\sum_{j=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k))\\\\ \u0026amp;=\\sum_{n=1}^{N}\\ln \\sum_{j=1}^{K}\\pi_j\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_j,\\Sigma_j)\\\\ \\end{aligned}\\tag{1} \\] ### \\(\\mu_k\\)","title":"EM Algorithm"},{"content":"Preliminaries Probability Theory   multiplication principle joint distribution the Bayesian theory Gaussian distribution log-likelihood function  ‘Maximum Likelihood Estimation’  Maximum Likelihood1 Gaussian mixtures had been discussed in ‘Mixtures of Gaussians’. And once we have a training data set and a certain hypothesis, what we should do next is estimate the parameters of the model. Both kinds of parameters from a mixture of Gaussians \\(\\Pr(\\mathbf{x})= \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\\): - the parameters of Gaussian: \\(\\mathbf{\\mu}_k,\\Sigma_k\\) - and latent variables: \\(\\mathbf{z}\\)\nneed to be estimated. When we investigated the linear regression problems, ‘Maximum Likelihood Estimation’ method assumed the output of the linear model also has a Gaussian distribution. So, we could try the maximum likelihood again in the Gaussian mixture task, and find whether it could solve the problem.\nNotations: - input data: \\(\\{\\mathbf{x}_1,\\cdots,\\mathbf{x}_N\\}\\) for \\(\\mathbf{x}_i\\in \\mathbb{R}^D\\) and \\(i=\\{1,2,\\cdots,N\\}\\) and assuming they are i.i.d. Rearranging them in a matrix: \\[ X = \\begin{bmatrix} -\u0026amp;\\mathbf{x}_1^T\u0026amp;-\\\\ -\u0026amp;\\mathbf{x}_2^T\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\mathbf{x}_N^T\u0026amp;-\\\\ \\end{bmatrix}\\tag{1} \\]\n Latent varibales \\(\\mathbf{z}_i\\), for \\(i\\in\\{1,\\cdots,N\\}\\). And similar to matrix (3), the matrix of latent varibales is \\[ Z = \\begin{bmatrix} -\u0026amp;\\mathbf{z}_1^T\u0026amp;-\\\\ -\u0026amp;\\mathbf{z}_2^T\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\mathbf{z}_N^T\u0026amp;-\\\\ \\end{bmatrix}\\tag{2} \\]  Once we got these two matrices, based on the definition of ‘Mixtures of Gaussians’:\n\\[ \\Pr(\\mathbf{x})= \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\\tag{3} \\]\nthe log-likelihood function is given by:\n\\[ \\begin{aligned} \\ln \\Pr(\\mathbf{x}|\\mathbf{\\pi},\\mathbf{\\mu},\\Sigma)\u0026amp;=\\ln (\\Pi_{n=1}^N\\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k))\\\\ \u0026amp;=\\sum_{n=1}^{N}\\ln \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\Sigma_k)\\\\ \\end{aligned}\\tag{4} \\]\nThis looks different from the single Gaussian model where the logarithm operates directly on \\(\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\\) who is an exponential function. The existence of summation in the logarithm makes the problem hard to solve.\nBecause the combination order of the Gaussian mixture could be arbitrary, we could have \\(K!\\) equivalent solutions of \\(\\mathbf{z}\\). So which one we get did not affect our model.\nMaximum Likelihood Could Fail Covariance Matrix \\(\\Sigma\\) is not Singular In the Gaussian distribution, the covariance matrix must be able to be inverted. So in our following discussion, we assume all the covariance matrice are invisible. For simplicity we take \\(\\Sigma_k=\\delta_k^2 I\\) where \\(I\\) is identity matrix.\nWhen \\(\\mathbf{x}_n=\\mathbf{\\mu}_j\\) When a point in the sample accidentally equals the mean \\(\\mu_j\\), the Gaussian distribution of the random variable \\(\\mathbf{x}_n\\) is:\n\\[ \\mathcal{N}(\\mathbf{x}_n|\\mathbf{\\mu}_k,\\delta_j^2I)=\\frac{1}{2\\pi^{\\frac{1}{2}}}\\frac{1}{\\delta_j}\\tag{5} \\]\nWhen the variance \\(\\delta_j\\to 0\\), this part goes to infinity and the whole algorithm failed.\nThis problem does not exist in a single Gaussian model, for the \\(\\mathbf{x}_n-\\mathbf{\\mu}_j=0\\) is not an exponent in its log-likelihood.\nSummary The maximum likelihood method is not suited for a Gaussian mixture model. Then we introduce the EM algorithm in the next post.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Maximum-Likelihood-of-Gaussian-Mixtures/","summary":"Preliminaries Probability Theory   multiplication principle joint distribution the Bayesian theory Gaussian distribution log-likelihood function  ‘Maximum Likelihood Estimation’  Maximum Likelihood1 Gaussian mixtures had been discussed in ‘Mixtures of Gaussians’. And once we have a training data set and a certain hypothesis, what we should do next is estimate the parameters of the model. Both kinds of parameters from a mixture of Gaussians \\(\\Pr(\\mathbf{x})= \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\\): - the parameters of Gaussian: \\(\\mathbf{\\mu}_k,\\Sigma_k\\) - and latent variables: \\(\\mathbf{z}\\)","title":"Maximum Likelihood of Gaussian Mixtures"},{"content":"Preliminaries Probability Theory   multiplication principle joint distribution the Bayesian theory Gaussian distribution  Calculus 1,2  A Formal Introduction to Mixtures of Gaussians1 We have introduced a mixture distribution in the post ‘An Introduction to Mixture Models’. And the example in that post was just two components Gaussian Mixture. However, in this post, we would like to talk about Gaussian mixtures formally. And it severs to motivate the development of the expectation-maximization(EM) algorithm.\nGaussian mixture distribution can be written as:\n\\[ \\Pr(\\mathbf{x})= \\sum_{k=1}^{K}\\pi_k\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)\\tag{1} \\]\nwhere \\(\\sum_{k=1}^K \\pi_k =1\\) and \\(0\\leq \\pi_k\\leq 1\\).\nAnd then we introduce a random variable(vector) called latent variable(vector) \\(\\mathbf{z}\\), that each component of \\(\\mathbf{z}\\):\n\\[ z_k\\in\\{0,1\\}\\tag{2} \\]\nand \\(\\mathbf{z}\\) is a \\(1\\)-of-\\(K\\) representation, which means there is one and only one component is \\(1\\) and others are \\(0\\).\nTo build a joint distribution \\(\\Pr(\\mathbf{x},\\mathbf{z})\\), we should build \\(\\Pr(\\mathbf{x}|\\mathbf{z})\\) and \\(\\Pr(\\mathbf{z})\\) firstly. We define the distribution of \\(\\mathbf{z}\\):\n\\[ \\Pr(z_k=1)=\\pi_k\\tag{3} \\]\nfor \\(\\{\\pi_k\\}\\) for \\(k=1,\\cdots,K\\). And equation (3) is now written as:\n\\[ \\Pr(\\mathbf{z}) = \\Pi_{k=1}^K \\pi_k^{z_k}\\tag{4} \\]\nAnd according to the definition of \\(\\Pr(\\mathbf{z})\\) we can get the condition distribution of \\(\\mathbf{x}\\) given \\(\\mathbf{z}\\). Under the condition \\(z_k=1\\), we have:\n\\[ \\Pr(\\mathbf{x}|z_k=1)=\\mathcal{N}(\\mathbf{x}|\\mu_k,\\Sigma_k)\\tag{5} \\]\nand then we can derive the vector form of conditional distribution:\n\\[ \\Pr(\\mathbf{x}|\\mathbf{z})=\\Pi_{k=1}^{K}\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\Sigma_k)^{z_k}\\tag{6} \\]\nOnce we have both the probability distribution of \\(\\mathbf{z}\\), \\(\\Pr(\\mathbf{z})\\), and conditional distribution, \\(\\Pr(\\mathbf{x}|\\mathbf{z})\\), we can build joint distribution by multiplication principle:\n\\[ \\Pr(x,z) = \\Pr(\\mathbf{z})\\cdot \\Pr(\\mathbf{x}|\\mathbf{z})\\tag{7} \\]\nHowever, what we are concerning is still the distribution of \\(\\mathbf{x}\\). We can calculate the probability of \\(\\mathbf{x}\\) by:\n\\[ \\Pr(\\mathbf{x}) = \\sum_{j}\\Pr(x,\\mathbf{z}_j) = \\sum_{j}\\Pr(\\mathbf{z}_j)\\cdot \\Pr(\\mathbf{x}|\\mathbf{z_j})\\tag{8} \\]\nwhere \\(\\mathbf{z_j}\\) is every possible value of random variable \\(\\mathbf{z}\\).\nThis is how latent variables construct mixture Gaussians. And this form is easy for us to analyze the distribution of a mixture model.\n‘Responsibility’ of Gaussian Mixtures The Bayesian formula can help us produce posterior. And the posterior probability of latent variable \\(\\mathbf{z}\\) by equation (7) can be calculated:\n\\[ \\Pr(z_k=1|\\mathbf{x})=\\frac{\\Pr(z_k=1)\\Pr(\\mathbf{x}|z_k=1)}{\\sum_j^K \\Pr(z_j=1)\\Pr(\\mathbf{x}|z_j=1)}\\tag{9} \\]\nand substitute equation (3),(5) into equation (9) and we get:\n\\[ \\Pr(z_k=1|\\mathbf{x})=\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\mu_k,\\Sigma_k)}{\\sum^K_j \\pi_j\\mathcal{N}(\\mathbf{x}|\\mu_j,\\Sigma_j)}\\tag{10} \\]\nAnd \\(\\Pr(z_k=1|\\mathbf{x})\\) is also called reponsibility, and denoted as:\n\\[ \\gamma(z_k)=\\Pr(z_k=1|\\mathbf{x})\\tag{11} \\]\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Mixtures-of-Gaussians/","summary":"Preliminaries Probability Theory   multiplication principle joint distribution the Bayesian theory Gaussian distribution  Calculus 1,2  A Formal Introduction to Mixtures of Gaussians1 We have introduced a mixture distribution in the post ‘An Introduction to Mixture Models’. And the example in that post was just two components Gaussian Mixture. However, in this post, we would like to talk about Gaussian mixtures formally. And it severs to motivate the development of the expectation-maximization(EM) algorithm.","title":"Mixtures of Gaussians"},{"content":"Preliminaries Numerical Optimization  necessary conditions for maximum  K-means algorithm Fisher Linear Discriminant  Clustering Problem1 The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.\nClustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete. However, during training classifiers, we always have a certain target corresponding to every input. On the contrary, clustering has no target at all, and what we have is only\n\\[ \\{x_1,\\cdots, x_N\\}\\tag{1} \\]\nwhere \\(x_i\\in\\Re^D\\) for \\(i=1,2,\\cdots,N\\). And our mission is to separate the dataset into \\(K\\) groups where \\(K\\) has been given before task\nAn intuitive strategy of clustering is based on two considerations: 1. the distance between data points in the same group should be as small as possible. 2. the distance between data points in the different groups should be as large as possible.\nThis is a little like Fisher Linear Discriminant. Based on these two points, some concepts could be formed.\nThe first one is how to represent a group. We take\n\\[ \\mu_i:i\\in\\{1,2,\\cdots, K\\}\\tag{2} \\]\nas the prototype associated with \\(i\\) th group. A group always contains several points, and a spontaneous idea is using the center of all the points belonging to one group as its prototype. To represent which group \\(\\mathbf{x}_i\\) in equation (1) belongs to, an indicator is necessary, and a 1-of-K coding scheme is used:\n\\[ r_{nk}\\in\\{0,1\\}\\tag{3} \\]\nfor \\(k=1,2,\\cdots,K\\) representing the group number and \\(n = 1,2,\\cdots,N\\) denoting the number of sample point, and where \\(r_{nk}=1\\) then \\(r_{nj}=0\\) for all \\(j\\neq k\\).\nObjective Function A loss function is a good way to measure the quantity of our model during both the training and testing stages. And in the clustering task loss function could not be used because we have no idea about what is correct. However, we can build another function that plays the same role as the loss function and it is also the target of what we want to optimize.\nAccording to the two base points above, we build our objective function:\n\\[ J=\\sum_{n=1}^{N}\\sum_{k=1}^{K}r_{nk}||\\mathbf{x}_n-\\mu_k||^2\\tag{4} \\]\nIn this objective function, the distance is defined as Euclidean distance(However, other measurements of similarity could also be used). Then the mission is to minimize \\(J\\) by finding some certain \\(\\{r_{nk}\\}\\) and \\(\\{\\mu_k\\}\\)\nK-Means Algorithm Now, let’s represent the famous K-Means algorithm. The method includes two steps:\nMinimising \\(J\\) respect to \\(r_{nk}\\) keeping \\(\\mu_k\\) fixed Minimising \\(J\\) respect to \\(\\mu_k\\) keeping \\(r_{nk}\\) fixed  In the first step, according to equation (4), the objective function is linear of \\(r_{nk}\\). So there is a close solution. Then we set:\n\\[ r_{nk}=\\begin{cases} 1\u0026amp;\\text{ if } k=\\arg\\min_{j}||x_n-\\mu_j||^2\\\\ 0\u0026amp;\\text{otherwise} \\end{cases}\\tag{5} \\]\nAnd in the second step, \\(r_{nk}\\) is fixed and we minimize objective function \\(J\\). For it is quadratic, the minimum point is on the stationary point where:\n\\[ \\frac{\\partial J}{\\partial \\mu_k}=-\\sum_{n=1}^{N}r_{nk}(x_n-\\mu_k)=0\\tag{6} \\]\nand we get:\n\\[ \\mu_k = \\frac{\\sum_{n=1}^{N}r_{nk}x_n}{\\sum_{n=1}^{N}r_{nk}}\\tag{7} \\]\n\\(\\sum_{n=1}^{N} r_{nk}\\) is the total number of points from the sample \\(\\{x_1,\\cdots, x_N\\}\\) who belong to prototype \\(\\mu_k\\) or group \\(k\\) at current step. And \\(\\mu_k\\) is just the average of all the points in the group \\(k\\).\nThis two-step, which was calculated by equation (5),(7), would repeat until \\(r_{nk}\\) and \\(\\mu_k\\) not change.\nThe K-means algorithm guarantees to converge because at every step the objective function \\(J\\) is reduced. So when there is only one minimum, the global minimum, the algorithm must converge.\nInput Data Preprocessing before K-means Most algorithms need their input data to obey some rules. To the K-means algorithm, we rescale the input data to mean 0 and variance 1. This is always done by\n\\[ x_n^{(i)} = \\frac{x_n^{(i)}- \\bar{x}^{(i)}}{\\delta^{i}} \\]\nwhere \\(x_n^{(i)}\\) is the \\(i\\) th component of the \\(n\\) th data point, and \\(x_n\\) comes from equation (1), \\(\\bar{x}^{(i)}\\) and \\(\\delta^{i}\\) is the \\(i\\) th mean and standard deviation repectively\nPython code of K-means  class K_Means():  \u0026quot;\u0026quot;\u0026quot; input data should be normalized: mean 0, variance 1 \u0026quot;\u0026quot;\u0026quot;  def clustering(self, x, K):  \u0026quot;\u0026quot;\u0026quot; :param x: inputs :param K: how many groups :return: prototype(center of each group), r_nk, which group k does the n th point belong to \u0026quot;\u0026quot;\u0026quot;  data_point_dimension = x.shape[1]  data_point_size = x.shape[0]  center_matrix = np.zeros((K, data_point_dimension))  for i in range(len(center_matrix)):  center_matrix[i] = x[np.random.randint(0, len(x)-1)]   center_matrix_last_time = np.zeros((K, data_point_dimension))  cluster_for_each_point = np.zeros(data_point_size, dtype=np.int32)  # -----------------------------------visualization-----------------------------------  # the part can be deleted  center_color = np.random.randint(0,1000, (K, 3))/1000.  plt.scatter(x[:, 0], x[:, 1], color=\u0026#39;green\u0026#39;, s=30, marker=\u0026#39;o\u0026#39;, alpha=0.3)  for i in range(len(center_matrix)):  plt.scatter(center_matrix[i][0], center_matrix[i][1], marker=\u0026#39;x\u0026#39;, s=65, color=center_color[i])  plt.show()  # -----------------------------------------------------------------------------------  while (center_matrix_last_time-center_matrix).all() != 0:  # E step  for i in range(len(x)):  distance_to_center = np.zeros(K)  for k in range(K):  distance_to_center[k] = (center_matrix[k]-x[i]).dot((center_matrix[k]-x[i]))  cluster_for_each_point[i] = int(np.argmin(distance_to_center))  # M step  number_of_point_in_k = np.zeros(K)  center_matrix_last_time = center_matrix  center_matrix = np.zeros((K, data_point_dimension))  for i in range(len(x)):  center_matrix[cluster_for_each_point[i]] += x[i]  number_of_point_in_k[cluster_for_each_point[i]] += 1   for i in range(len(center_matrix)):  if number_of_point_in_k[i] != 0:  center_matrix[i] /= number_of_point_in_k[i]  # -----------------------------------visualization-----------------------------------  # the part can be deleted  print(center_matrix)  plt.cla()  for i in range(len(center_matrix)):  plt.scatter(center_matrix[i][0], center_matrix[i][1], marker=\u0026#39;x\u0026#39;, s=65, color=center_color[i])  for i in range(len(x)):  plt.scatter(x[i][0], x[i][1], marker=\u0026#39;o\u0026#39;,s=30, color=center_color[cluster_for_each_point[i]],alpha=0.7)  plt.show()  # -----------------------------------------------------------------------------------  return center_matrix, cluster_for_each_point and the entire project can be found : https://github.com/Tony-Tan/ML and please star me(^_^).\nResults during K-means We use a tool https://github.com/Tony-Tan/2DRandomSampleGenerater to generate the input data from:\nThere are two classes the brown circle and the green circle. Then the K-means algorithm initial two prototypes, the centers of groups, randomly:\nthe two crosses represent the initial centers \\(\\mu_i\\). And then we iterate the two steps:\n Iteration 1\n  Iteration 2\n  Iteration 3\n  Iteration 4\n The result of iterations 3 and 4 do not vary for both objective function value \\(J\\) and parameters. Then the algorithm stopped.\nAnd different initial centers may have different convergence speeds, but they always have the same stop positions.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/K-means-Clustering/","summary":"Preliminaries Numerical Optimization  necessary conditions for maximum  K-means algorithm Fisher Linear Discriminant  Clustering Problem1 The first thing we should do before introducing the algorithm is to make the task clear. A mathematical form is usually the best way.\nClustering is a kind of unsupervised learning task. So there is no correct or incorrect solution because there is no teacher or target in the task. Clustering is similar to classification during predicting since the output of clustering and classification are discrete.","title":"K-means Clustering"},{"content":"Preliminaries linear regression Maximum Likelihood Estimation Gaussian Distribution Conditional Distribution  From Supervised to Unsupervised Learning1 We have discussed many machine learning algorithms, including linear regression, linear classification, neural network models, and e.t.c, till now. However, most of them are supervised learning, which means a teacher is leading the models to bias toward a certain task. In these problems our attention was on the probability distribution of parameters given inputs, outputs, and models:\n\\[ \\Pr(\\mathbf{\\theta}|\\mathbf{x},\\mathbf{y},M)\\tag{1} \\]\nwhere \\(\\mathbf{\\theta}\\) is a vector of the parameters in the model \\(M\\) and \\(\\mathbf{x}\\), \\(\\mathbf{y}\\) are inputs vector and output vector respectively. As what Bayesian equation told us, we can maximize equation (1) by maximizing the likelihood. And the probability\n\\[ \\Pr(\\mathbf{y}|\\mathbf{x},\\mathbf{\\theta},M)\\tag{2} \\]\nis the important component of the method. More details about the maximum likelihood method can be found Maximum Likelihood Estimation.\nIn today’s discussion, another probability will come to our’s minds. If we have no information about the target in the training set, it is to say we have no teacher in the training stage. We concerned about:\n\\[ \\Pr(\\mathbf{x})\\tag{3} \\]\nOur task, now, could not be called classification or regression anymore. It is referred to as ‘cluster’ which is a progress of identifying which group the data point belongs to. What we have here is just a set of training points \\(\\mathbf{x}\\) without targets and the probability \\(\\Pr(\\mathbf{x})\\).\nAlthough this probability equation (3) is over one random variable, it can be arbitrarily complex. And sometimes, bringing in another random variable as assistance and combining them could get a new distribution that is relatively more tractable. That is to say, a joint distribution of observed variable \\(\\mathbf{x}\\) and another created random variable \\(\\mathbf{z}\\) could be more clear than the original distribution of \\(\\mathbf{x}\\). And sometimes under this combination, the conditional distribution of \\(\\mathbf{x}\\) given \\(\\mathbf{z}\\) is very clear, too.\nLet’s have look at a very simple example. \\(\\mathbf{x}\\) has a distribution:\n\\[ a\\cdot\\exp(-\\frac{(x-\\mu_1)^2}{\\delta_1})+b\\cdot\\exp(-\\frac{(x-\\mu_2)^2}{\\delta_2})\\tag{4} \\]\nwhere \\(a\\) and \\(b\\) are coeficients and \\(\\mu_1\\), \\(\\mu_2\\) are means of the Gaussian distributions and \\(\\delta_1\\) and \\(\\delta_2\\) are variances. It looks like this:\nThe random variable of equation (4) is just \\(x\\). However, now, we introduce another random variable \\(z\\in \\{0,1\\}\\) as assistance into the distribution, in which \\(z\\) has a uniform distribution. Then the distribution (4) can be rewritten in a joint form:\n\\[ \\Pr(x,z)=z\\cdot a\\cdot\\exp(-\\frac{(x-\\mu_1)^2}{\\delta_1})+(1-z)\\cdot b\\cdot\\exp(-\\frac{(x-\\mu_2)^2}{\\delta_2})\\tag{5} \\]\nfor \\(\\mathbf{z}\\) is discrete random variable vector who has a uniform distribution so \\(\\Pr(z=0)=\\Pr(z=1)=0.5\\) and the conditional distribution is \\(\\Pr(x|z=1)\\) is\n\\[ a\\cdot\\exp(-\\frac{(x-\\mu_1)^2}{\\delta_1})\\tag{6} \\]\nlooks like:\nAnd the conditional distribution is \\(\\Pr(x|z=0)\\) is \\[ b\\cdot\\exp(-\\frac{(x-\\mu_2)^2}{\\delta_2})\\tag{7} \\] looks like:\nAnd the margin distribution of \\(x\\) is still equation (4)\nand its 3D vision is:\nSo the conditional distribution of \\(x\\) given \\(z\\) is just a single-variable Gaussian model, which is much simple to deal with. And so is the margin \\(\\Pr(x)\\) by summing up all \\(z\\) (the rule of computing the margin distribution). Here the created random variable \\(z\\) is called a latent variable. It can be considered as an assistant input. However, it can also be considered as a kind of parameter of the model(we will discuss the details later). And the example above is the simplest Gaussian mixture which is wildly used in machine learning, statistics, and other fields. Here \\(z\\), the latent variable, is a discrete random variable, and the continuous latent variable will be introduced later as well.\nMixture distribution can be used to cluster data and what we are going to study are:\nNon probabilistic version: K-means algorithm Discrete latent variable and a relative algorithm which is known as EM algorithm  References  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Mixture-Models/","summary":"Preliminaries linear regression Maximum Likelihood Estimation Gaussian Distribution Conditional Distribution  From Supervised to Unsupervised Learning1 We have discussed many machine learning algorithms, including linear regression, linear classification, neural network models, and e.t.c, till now. However, most of them are supervised learning, which means a teacher is leading the models to bias toward a certain task. In these problems our attention was on the probability distribution of parameters given inputs, outputs, and models:","title":"An Introduction to Mixture Models"},{"content":"Preliminaries ‘An Introduction to Probabilistic Generative Models for Linear Classification’  Idea of logistic regression1 Logistic sigmoid function(logistic function for short) had been introduced in post ‘An Introduction to Probabilistic Generative Models for Linear Classification’. It has an elegant form:\n\\[ \\delta(a)=\\frac{1}{1+e^{-a}}\\tag{1} \\]\nand when \\(a=0\\), \\(\\delta(a)=\\frac{1}{2}\\) and this is just the half of the range of logistic function. This gives us a strong implication that we can set \\(a\\) equals to some functions \\(y(\\mathbf{x})\\), and then\n\\[ a=y(\\mathbf{x})=0\\tag{2} \\]\nbecomes a decision boundary. Here the logistic function plays the same role as the threshold function described in the post ‘From Linear Regression to Linear Classification’\nLogistic Regression of Linear Classification The easiest decision boundary is a constant corresponding to a 1-deminsional input. The dicision boundary of 1-deminsional input is a degenerated line, namely, a point. Here we consider a little complex function - a 2-deminsional input vector \\(\\begin{bmatrix}x_1\u0026amp;x_2\\end{bmatrix}^T\\) and the function \\(y(\\mathbf{x})\\) is:\n\\[ y(\\mathbf{x})=w_0+w_1x_1+w_2x_2=\\mathbf{w}^T\\mathbf{x}= \\begin{bmatrix}w_0\u0026amp;w_1\u0026amp;w_2\\end{bmatrix} \\begin{bmatrix} 1\\\\ x_1\\\\ x_2 \\end{bmatrix}\\tag{3} \\]\nThen we substitute this into equation (1), we got our linear logistic regression function:\n\\[ y(\\mathbf{x})=\\delta(\\mathbf{w}^T\\mathbf{x})=\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}}}\\tag{4} \\]\nThe output of the equation (4) is a real number, its range is \\((0,1)\\). So it can be used to represent a probability of the input belonging to \\(\\mathcal{C}_1\\) whose label is \\(1\\) or \\(\\mathcal{C}_0\\) whose label is \\(0\\) in the training set.\nEstimating the Parameters in Logistic Regression Although logistic regression is called regression, it acts as a classifier. Our mission, now, is to estimate the parameters in equation(4).\nRecalling that the output of equation(4) is \\(\\Pr(\\mathcal{C}_1|\\mathbf{x},M)\\) where \\(M\\) is the model we selected. And the model sometimes can be represented by its parameters. And the mission should you chose to accept it, is to build probability \\(\\Pr(\\mathbf{w}|\\mathbf{x},t)\\) where \\(\\mathbf{x}\\) is the input vector and \\(t\\in\\{0,1\\}\\) is the corresponding label and condition \\(\\mathbf{x}\\) is always been omitted. \\(t\\) is one of \\(\\mathcal{C}_1\\) or \\(\\mathcal{C}_2\\), so the Bayesian relation of \\(\\Pr(\\mathbf{w}|t)\\) is:\n\\[ \\Pr(\\mathbf{w}|t)=\\frac{\\Pr(t|\\mathbf{w})\\Pr(\\mathbf{w})}{\\Pr(t)}=\\frac{\\Pr(\\mathcal{C}_i|\\mathbf{w})\\Pr(\\mathbf{w})}{\\Pr(t)}=\\frac{\\Pr(\\mathcal{C}_i|\\mathbf{x},M)\\Pr(\\mathbf{w})}{\\Pr(t)}\\tag{5} \\]\nThen the maximum likelihood function is employed to estimate the parameters. And the likelihood is just:\n\\[ \\Pr(\\mathcal{C}_1|\\mathbf{x},M)=\\delta(\\mathbf{w}^T\\mathbf{x})=\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}}}\\tag{6} \\]\nWhen we have had the training set:\n\\[ \\{\\mathbf{x}_1,t_1\\},\\{\\mathbf{x}_2,t_2\\},\\cdots,\\{\\mathbf{x}_N,t_N\\}\\tag{7} \\]\nthe likelihood becomes:\n\\[ \\Pi_{t_i\\in \\mathcal{C}_1}\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\Pi_{t_i\\in \\mathcal{C}_0}(1-\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}})\\tag{8} \\]\nIn the second part, when \\(\\mathbf{x}\\) belongs to \\(\\mathcal{C}_0\\) the label is \\(0\\). The output of this class should approach to \\(0\\), so minimising \\(\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\) equals to maximising \\(1-\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\). For equation(8) is not convenient to optimise, we can use the property that \\(t_n\\in{0,1}\\) and we have:\n\\[ \\begin{aligned} \u0026amp;\\Pi_{t_i\\in \\mathcal{C}_1}\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\Pi_{t_i\\in \\mathcal{C}_0}(1-\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}})\\\\ =\u0026amp;\\Pi_i^N(\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}})^{t_i}(1-\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}})^{1-t_i} \\end{aligned} \\tag{9} \\]\nFrom now on, we turn to an optimization problem. Maximizing equation(9) equals to minimize its minus logarithm(we use \\(y_i\\) retpresent \\(\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\)):\n\\[ \\begin{aligned} E\u0026amp;=-\\mathrm{ln}\\;\\Pi^N_{i=1}y_i^{t_i}(1-y_i)^{1-t_i}\\\\ \u0026amp;=-\\sum^N_{i=1}(t_i\\mathrm{ln}y_i+(1-t_i)\\mathrm{ln}(1-y_i)) \\end{aligned} \\tag{10} \\]\nEquation(10) is called cross-entropy, which is a very important concept in information theory and is called cross-entropy error in machine learning which is also a very useful function.\nFor there is no closed-form solution to the optimization problem in equation(10), ‘steepest descent algorithm’ is employed. And what we need to calculate firstly is the derivative of equation(10). For we want to get the derivative of \\(\\mathbf{w}\\)of function \\(y_i(\\mathbf{x})\\), the chain rule is necessary:\n\\[ \\begin{aligned} \\frac{dE}{dw}\u0026amp;=-\\frac{d}{dw}\\sum^N_{i=1}(t_i\\mathrm{ln}y_i+(1-t_i)\\mathrm{ln}(1-y_i))\\\\ \u0026amp;=-\\sum^N_{i=1}\\frac{d}{dw}t_i\\mathrm{ln}y_i+\\frac{d}{dw}(1-t_i)\\mathrm{ln}(1-y_i)\\\\ \u0026amp;=-\\sum^N_{i=1}t_i\\frac{y_i\u0026#39;}{y_i}+(1-t_i)\\frac{-y\u0026#39;_i}{1-y_i} \\end{aligned} \\tag{11} \\]\nand\n\\[ \\begin{aligned} y_i\u0026#39;\u0026amp;=\\frac{d}{dw}\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\\\ \u0026amp;=\\frac{\\mathbf{x}e^{-\\mathbf{w}^T\\mathbf{x_i}}}{(1+e^{-\\mathbf{w}^T\\mathbf{x_i}})^2} \\end{aligned}\\tag{12} \\]\nSubstitute equation (12) into equation (11), we have:\n\\[ \\begin{aligned} \\frac{dE}{dw}\u0026amp;=-\\sum_{i=1}^Nt_i\\frac{\\frac{\\mathbf{x}e^{-\\mathbf{w}^T\\mathbf{x_i}}}{(1+e^{-\\mathbf{w}^T\\mathbf{x_i}})^2}}{\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}}-(1-t_i)\\frac{\\frac{\\mathbf{x}e^{-\\mathbf{w}^T\\mathbf{x_i}}}{(1+e^{-\\mathbf{w}^T\\mathbf{x_i}})^2}}{\\frac{e^{-\\mathbf{w}^T\\mathbf{x_i}}}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}}\\\\ \u0026amp;=-\\sum_{i=1}^Nt_i\\frac{\\mathbf{x}e^{-\\mathbf{w}^T\\mathbf{x_i}}}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}-(1-t_i)\\frac{\\mathbf{x}}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}\\\\ \u0026amp;=-\\sum_{i=1}^N(t_i\\frac{e^{-\\mathbf{w}^T\\mathbf{x_i}}}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}}-(1-t_i)\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x_i}}})\\mathbf{x}\\\\ \u0026amp;=-\\sum_{i=1}^N(t_i(1-y_i)-(1-t_i)y_i)\\mathbf{x}\\\\ \u0026amp;=-\\sum_{i=1}^N(t_i-y_i)\\mathbf{x} \\end{aligned}\\tag{13} \\]\nThen we can update \\(\\mathbf{w}\\) :\n\\[ \\mathbf{w} = \\mathbf{w} - \\mathrm{learning\\; rate} \\times (-\\frac{1}{N}\\sum_{i=1}^N(t_i-y_i)\\mathbf{x})\\tag{14} \\]\nCode for logistic regression class LogisticRegression():   def logistic_sigmoid(self, a):  return 1./(1.0 + np.exp(-a))   def fit(self, x, y, e_threshold, lr=1):  x = np.array(x)/320.-1  # augment the input  x_dim = x.shape[0]  x = np.c_[np.ones(x_dim), x]  # initial parameters in 0.01 to 1  w = np.random.randint(1, 100, x.shape[1])/100.  number_of_points = np.size(y)  for dummy in range(1000):  y_output = self.logistic_sigmoid(w.dot(x.transpose()))  # gradient calculation  e_gradient = np.zeros(x.shape[1])  for i in range(number_of_points):  e_gradient += (y_output[i]-y[i])*x[i]  e_gradient = e_gradient / number_of_points  # update parameter  w += -e_gradient*lr  e = 0  for i in range(number_of_points):  e += -(y[i] * np.log(y_output[i]) + (1 - y[i]) * np.log(1 - y_output[i]))  e /= number_of_points  if e \u0026lt;= e_threshold:  break  return w The entire project can be found The entire project can be found https://github.com/Tony-Tan/ML and please star me.\nTwo hyperparameters are the learning rate and the stop condition. When the error is lower than the stopping threshold, the algorithm stops.\nExperiment 1 with the learning rate 1, and different learning rates lead to different convergence rates:\nExperiment 2 with the learning rate 1, and different learning rates lead to different convergence rates:\nExperiment 3 with the learning rate 1, and different learning rates lead to different convergence rates:\nSeveral Traps in Logistic Regression Input value should be normalized and centered at 0 Learning rate is chosen corroding to equation (14) but not \\(-\\sum_{i=1}^N(t_i-y_i)\\mathbf{x}\\) because the uncertain coefficient \\(N\\) parameter vector \\(\\mathbf{w}\\) identifies the direction, so its margin can be arbitrarily large. And to a large \\(\\mathbf{w}\\) , \\(y_i(\\mathbf{x})\\) is very close to \\(1\\) or \\(0\\), but it can never be \\(1\\) or \\(0\\). So there is no optimization position, and the equation(13) can never be \\(0\\) which means the algorithm can never stop by himself. The more large margin, more steppen the curve  Considering \\(\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}}}\\), when the margin of \\(\\mathbf{w}\\) grows, we can write it in a combination of margin \\(M\\) and direction vector \\(\\mathbf{w}_d\\): \\(\\frac{1}{1+e^{-M(\\mathbf{w_d}^T\\mathbf{x}})}\\). And the function \\(\\frac{1}{1+e^{-M(x)}}\\) varies according \\(M\\) is like(when \\(M\\) grows the logistic function is more like Heaviside step function):    References  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Logistic-Regression/","summary":"Preliminaries ‘An Introduction to Probabilistic Generative Models for Linear Classification’  Idea of logistic regression1 Logistic sigmoid function(logistic function for short) had been introduced in post ‘An Introduction to Probabilistic Generative Models for Linear Classification’. It has an elegant form:\n\\[ \\delta(a)=\\frac{1}{1+e^{-a}}\\tag{1} \\]\nand when \\(a=0\\), \\(\\delta(a)=\\frac{1}{2}\\) and this is just the half of the range of logistic function. This gives us a strong implication that we can set \\(a\\) equals to some functions \\(y(\\mathbf{x})\\), and then","title":"Logistic Regression"},{"content":"Preliminaries Probability   Bayesian Formular  Calculus  Probabilistic Generative Models1 The generative model used for making decisions contains an inference step and a decision step:\nInference step is to calculate \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) which means the probability of \\(\\mathbf{x}\\) belonging to the class \\(\\mathcal{C}_k\\) given \\(\\mathbf{x}\\) Decision step is to make a decision based on \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) which was calculated in step 1  In this post, we just give an introduction and a framework for the probabilistic generative model in classification. But the details of how to estimate the parameters in the model will not be introduced.\nFrom Bayesian Formular to Logistic Sigmoid Function To build \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\), we can start from Bayesian formula. To the class \\(\\mathcal{C}_1\\) of a two-classes problem, the posterior probability:\n\\[ \\begin{aligned} \\Pr(\\mathcal{C}_1|\\mathbf{x})\u0026amp;=\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)}{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)+\\Pr(\\mathbf{x}|\\mathcal{C}_2)\\Pr(\\mathcal{C}_2)}\\\\ \u0026amp;=\\frac{1}{1+\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_2)\\Pr(\\mathcal{C}_2)}{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)}} \\end{aligned}\\tag{1} \\]\nrepresents a new function:\n\\[ \\begin{aligned} \\Pr(\\mathcal{C}_1|\\mathbf{x})\u0026amp;=\\delta(a)\\\\ \u0026amp;=\\frac{1}{1+e^{-a}} \\end{aligned}\\tag{2} \\]\nwhere:\n\\[ a=\\ln\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)}{\\Pr(\\mathbf{x}|\\mathcal{C}_2)\\Pr(\\mathcal{C}_2)}\\tag{3} \\]\nAn usual question is why we set \\(a=\\ln\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)}{\\Pr(\\mathbf{x}|\\mathcal{C}_2)\\Pr(\\mathcal{C}_2)}\\) but not \\(a=\\ln\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_2)\\Pr(\\mathcal{C}_2)}{\\Pr(\\mathbf{x}|\\mathcal{C}_1)\\Pr(\\mathcal{C}_1)}\\). In my opinion, this \\(a\\) just determine the graph of function \\(\\delta(a)\\). However, we perfer monotone-increasing function and \\(\\frac{1}{1+e^{-a}}\\) is just a monotone-increasing function but \\(\\frac{1}{1+e^{a}}\\) is not.\n\\(\\delta(\\cdot)\\) is called logistic sigmoid function or squashing function, because it maps any number into interval \\((0,1)\\). The range of the function is just within the range of probability. So it is a good way to represent some kinds of probability, such as the \\(\\Pr(\\mathcal{C}_1|\\mathbf{x})\\). the shape of the logistic sigmoid function is:\nSome Properties of Logistic Sigmoid For the logistic sigmoid function is symmetrical, then:\n\\[ 1-\\delta(a)=\\frac{e^{-a}}{1+e^{-a}}=\\frac{1}{e^a+1}\\tag{4} \\]\nand:\n\\[ \\delta(-a)=\\frac{1}{1+e^a}\\tag{5} \\]\nSo, we have an important equation:\n\\[ 1-\\delta(a)=\\delta(-a)\\tag{6} \\]\nThe inverse function of \\(y=\\delta(a)\\) is: \\[ \\begin{aligned} y\u0026amp;=\\frac{1}{1+e^{-a}}\\\\ e^{-a}\u0026amp;=\\frac{1}{y}-1\\\\ a\u0026amp;=-\\ln(\\frac{1-y}{y})\\\\ a\u0026amp;=\\ln(\\frac{y}{1-y}) \\end{aligned}\\tag{7} \\]\nThe derivative of the logistic sigmoid function is: \\[ \\frac{d\\delta(a)}{d a}=\\frac{e^{-a}}{(1+e^{-a})^2}=(1-\\delta(a))\\delta(a)\\tag{8} \\]\nMultiple Classes Problems We, now, extend the logistic sigmoid function into multiple classes condition. And we also start from the Bayesian formula:\n\\[ \\Pr(\\mathcal{C}_k|\\mathbf{x})=\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}{\\sum_i\\Pr(\\mathbf{x}|\\mathcal{C}_i)\\Pr(\\mathcal{C}_i)}\\tag{9} \\]\nIn this condition,if we set \\(a_i=\\ln\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}{\\Pr(\\mathbf{x}|\\mathcal{C}_i)\\Pr(\\mathcal{C}_i)}\\), the whole fomular will be too complecated. To simplify the equation, we just set:\n\\[ a_i=\\ln \\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)\\tag{10} \\]\nand we get a function of posterior probability: \\[ \\Pr(\\mathcal{C}_k|\\mathbf{x})=\\frac{e^{a_k}}{\\sum_i e^{a_i}}\\tag{11} \\] And according to the property of probability, we get the value of function: \\[ y(a)=\\frac{e^{a_k}}{\\sum_i e^{a_i}}\\tag{12} \\] belongs to interval \\([0,1]\\). And it is called the softmax function. Although according to equation (10), the domain of the definition of softmax function is \\((-\\infty,0]\\), \\(a\\) can be any real number. It’s called softmax because it is a smooth version of the max function.\nWhen \\(a_k\\gg a_j\\) for \\(k\\neq j\\), we have:\n\\[ \\begin{aligned} \\Pr(\\mathbf{x}|\\mathcal{C}_k)\u0026amp;\\simeq1\\\\ \\Pr(\\mathbf{x}|\\mathcal{C}_j)\u0026amp;\\simeq0 \\end{aligned} \\]\nSo both the logistic sigmoid function and softmax function can be used to form generative classifiers, which gives a value to the decision step.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Probabilistic-Generative-Models/","summary":"Preliminaries Probability   Bayesian Formular  Calculus  Probabilistic Generative Models1 The generative model used for making decisions contains an inference step and a decision step:\nInference step is to calculate \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) which means the probability of \\(\\mathbf{x}\\) belonging to the class \\(\\mathcal{C}_k\\) given \\(\\mathbf{x}\\) Decision step is to make a decision based on \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) which was calculated in step 1  In this post, we just give an introduction and a framework for the probabilistic generative model in classification.","title":"An Introduction to Probabilistic Generative Models"},{"content":"Preliminaries linear algebra   inner multiplication projection  Idea of Fisher linear discriminant1 ‘Least-square method’ in classification can only deal with a small set of tasks. That is because it was designed for the regression task. Then we come to the famous Fisher linear discriminant. This method is also discriminative for it gives directly the class to which the input \\(\\mathbf{x}\\) belongs. Assuming that the linear function\n\\[ y=\\mathbf{w}^T\\mathbf{x}+w_0\\tag{1} \\]\nis employed as before. Then the threshold function \\(f(\\mathbf{y})=\\begin{cases}1 \u0026amp;\\text{ if } y\\leq 0\\\\0 \u0026amp;\\text{ otherwise }\\end{cases}\\) was employed. If \\(y\u0026lt;0\\) or equivalenttly \\(\\mathbf{w}^T\\mathbf{x}\\leq -w_0\\) , \\(\\mathbf{x}\\) belongs to \\(\\mathcal{C}_1\\), or it belongs to \\(\\mathcal{C}_2\\).\nThis is an intuitive classification framework, but if such kinds of parameters exist and how to find them out is still a hard problem.\nFrom the linear algebra view, when we set \\(w_0=0\\) the equation (1) can be viewed as the vector \\(\\mathbf{x}\\) projecting on vector \\(\\mathbf{w}\\).\nAnd a good parameter vector direction and a threshold may solve this problem. And the measurement of how good the parameter vector direction is has some different candidates.\nDistance Between Class Center The first strategy that comes to us is to maximize the distance between the projections of the centers of different classes.\nThe first step is to get the center of a class \\(\\mathcal{C}_k\\) whose size is \\(N_k\\) by:\n\\[ \\mathbf{m}_k=\\frac{1}{N_k}\\sum_{x_i\\in \\mathcal{C}_k}\\mathbf{x}_i\\tag{2} \\]\nSo the distance between the projections \\(m_1\\), \\(m_2\\) of centers of the two classes, \\(\\mathbf{m}_1\\), \\(\\mathbf{m}_2\\) is:\n\\[ m_1-m_2=\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2)\\tag{3} \\]\nAnd for the \\(\\mathbf{w}\\) here is referred to as the direction vector, its margin is \\(1\\):\n\\[ ||\\mathbf{w}||=1\\tag{4} \\]\nWhen \\(\\mathbf{w}\\) has the same direction with \\(\\mathbf{m}_1-\\mathbf{m}_2\\), equation(3) get its maximum value.\nThen the result looks like this:\nThe blue star and blue circle are the center of red stars and green circles, respectively. and the blue arrow is the direction we want in which the projections of center points have the longest distance.\nWith our observation, this line does not give the optimum solution. Because, although the projections of centers have the longest distance on this line, the projections of all sample points scatter into a relatively large region and some of them from different classes have been mixed up.\nThis phenomenon exists in our daily life. For example, when two seats are closed, the people sitting on them do not have enough space(this is the original condition). Then we move the seats and make them a little farther from each other(make the projections of centers far from each other). But this time, two big guys come to sit on them(the projection has a big variance), and the space is still not enough.\nIn our problem, the projections of the points of a class are the big guys. We need to make the projection of centers far away from each other and make the projections of points in one class slender (which means a lower variance) at the same time.\nThe variance of the projected points of class \\(k\\) can be calculated by:\n\\[ s^2_k=\\sum_{n\\in \\mathcal{C}_k}(y_n - m_k)^2\\tag{5} \\]\nand it is also called within-class variance.\nTo make the seat comfortable for people who sit on them, we need to make the seat as far as possible from each other(maximize \\((m_2-m_1)^2\\)) and only allow children to sit(minimize the sum of within-class variance).\nFisher criterion satisfies this requirement:\n\\[ J(\\mathbf{w})=\\frac{(m_2-m_1)^2}{s_1^2+s_2^2}\\tag{6} \\]\nAnd \\(J(\\mathbf{w})\\) in details is:\n\\[ \\begin{aligned} J(\\mathbf{w})\u0026amp;=\\frac{(m_2-m_1)^2}{s_1^2+s_2^2}\\\\ \u0026amp;=\\frac{(\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2))^T(\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2))}{\\sum_{n\\in \\mathcal{C}_1}(y_n - m_1)^2+\\sum_{n\\in \\mathcal{C}_2}(y_n - m_2)^2}\\\\ \u0026amp;=\\frac{(\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2))^T(\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2))} {\\sum_{n\\in \\mathcal{C}_1}(\\mathbf{w}^T\\mathbf{x}_n - \\mathbf{w}^T\\mathbf{m}_1)^2+\\sum_{n\\in \\mathcal{C}_2}(\\mathbf{w}^T\\mathbf{x}_n - \\mathbf{w}^T\\mathbf{m}_2)^2}\\\\ \u0026amp;=\\frac{\\mathbf{w}^T(\\mathbf{m}_1-\\mathbf{m}_2)(\\mathbf{m}_1-\\mathbf{m}_2)^T\\mathbf{w}} {\\mathbf{w}^T(\\sum_{n\\in \\mathcal{C}_1}(\\mathbf{x}_n - \\mathbf{m}_1)(\\mathbf{x}_n - \\mathbf{m}_1)^T+\\sum_{n\\in \\mathcal{C}_2}(\\mathbf{x}_n - \\mathbf{m}_2)(\\mathbf{x}_n - \\mathbf{m}_2)^T)\\mathbf{w}} \\end{aligned}\\tag{7} \\]\nAnd we can set:\n\\[ \\begin{aligned} S_B \u0026amp;= (\\mathbf{m}_1-\\mathbf{m}_2)(\\mathbf{m}_1-\\mathbf{m}_2)^T\\\\ S_W \u0026amp;= \\sum_{n\\in \\mathcal{C}_1}(\\mathbf{x}_n - \\mathbf{m}_1)(\\mathbf{x}_n - \\mathbf{m}_1)^T+\\sum_{n\\in \\mathcal{C}_2}(\\mathbf{x}_n - \\mathbf{m}_2)(\\mathbf{x}_n - \\mathbf{m}_2)^T \\end{aligned}\\tag{8} \\]\nwhere \\(S_B\\) represents the covariance matrix Between classes, and \\(S_W\\) represents the Within classes covariance. Then the equation(8) becomes:\n\\[ \\begin{aligned} J(\\mathbf{w})\u0026amp;=\\frac{\\mathbf{w}^TS_B\\mathbf{w}}{\\mathbf{w}^TS_W\\mathbf{w}} \\end{aligned}\\tag{9} \\]\nTo maximise the \\(J(\\mathbf{w})\\), we should differentiat equation (9) with respect to \\(\\mathbf{w}\\) firstly:\n\\[ \\begin{aligned} \\frac{\\partial }{\\partial \\mathbf{w}}J(\\mathbf{w})\u0026amp;=\\frac{\\partial }{\\partial \\mathbf{w}}\\frac{\\mathbf{w}^TS_B\\mathbf{w}}{\\mathbf{w}^TS_W\\mathbf{w}}\\\\ \u0026amp;=\\frac{(S_B+S_B^T)\\mathbf{w}(\\mathbf{w}^TS_W\\mathbf{w})-(\\mathbf{w}^TS_B\\mathbf{w})(S_W+S_W^T)\\mathbf{w}}{(\\mathbf{w}^TS_W\\mathbf{w})^T(\\mathbf{w}^TS_W\\mathbf{w})} \\end{aligned}\\tag{10} \\]\nand set it to zero:\n\\[ \\frac{(S_B+S_B^T)\\mathbf{w}(\\mathbf{w}^TS_W\\mathbf{w})-(\\mathbf{w}^TS_B\\mathbf{w})(S_W+S_W^T)\\mathbf{w}}{(\\mathbf{w}^TS_W\\mathbf{w})^T(\\mathbf{w}^TS_W\\mathbf{w})}=0\\tag{11} \\]\nand then:\n\\[ \\begin{aligned} (S_B+S_B^T)\\mathbf{w}(\\mathbf{w}^TS_W\\mathbf{w})\u0026amp;=(\\mathbf{w}^TS_B\\mathbf{w})(S_W+S_W^T)\\mathbf{w}\\\\ (\\mathbf{w}^TS_W\\mathbf{w})S_B\\mathbf{w}\u0026amp;=(\\mathbf{w}^TS_B\\mathbf{w})S_W\\mathbf{w} \\end{aligned}\\tag{12} \\]\nBecause \\((\\mathbf{w}^TS_W\\mathbf{w})\\) and \\((\\mathbf{w}^TS_B\\mathbf{w})\\) are scalars and according equation (8) when we multiply both sides by \\(\\mathbf{w}\\) and we have\n\\[ S_B \\mathbf{w}= (\\mathbf{m}_1-\\mathbf{m}_2)((\\mathbf{m}_1-\\mathbf{m}_2)^T\\mathbf{w})\\tag{13} \\]\nand \\((\\mathbf{m}_1-\\mathbf{m}_2)^T\\mathbf{w}\\) is a scalar and \\(S_B\\mathbf{w}\\) have the same direction with \\((\\mathbf{m}_1-\\mathbf{m}_2)\\)\nso the equation (12) can be written as:\n\\[ \\begin{aligned} \\mathbf{w}\u0026amp;\\propto S^{-1}_WS_B\\mathbf{w}\\\\ \\mathbf{w}\u0026amp;\\propto S^{-1}_W(\\mathbf{m}_1-\\mathbf{m}_2) \\end{aligned}\\tag{14} \\]\nSo, up to now, we have had a result parameter vector \\(\\mathbf{w}\\) based on maximizing Fisher Criterion.\nCode The code of the process is relatively easy:\nclass LinearClassifier():   def fisher(self, x, y):  x = np.array(x)  x_dim = x.shape[1]  m_1 = np.zeros(x_dim)  m_1_size = 0  m_2 = np.zeros(x_dim)  m_2_size = 0  for i in range(len(y)):  if y[i] == 0:  m_1 = m_1 + x[i]  m_1_size += 1  else:  m_2 = m_2 + x[i]  m_2_size += 1  if m_1_size != 0 and m_2_size != 0:  m_1 = (m_1/m_1_size).reshape(-1, 1)  m_2 = (m_2/m_2_size).reshape(-1, 1)  s_c_1 = np.zeros([x_dim, x_dim])  s_c_2 = np.zeros([x_dim, x_dim])  for i in range(len(y)):  if y[i] == 0:  s_c_1 += (x[i] - m_1).dot((x[i] - m_1).transpose())  else:  s_c_2 += (x[i] - m_2).dot((x[i] - m_2).transpose())  s_w = s_c_1 + s_c_2   return np.linalg.inv(s_w).dot(m_2-m_1) The entire project can be found https://github.com/Tony-Tan/ML and please star me.\nThe results of the code(where the line is the one to which the points are projected):\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006. And what we should do is estimate the parameters of the model.↩︎\n   ","permalink":"https://anthony-tan.com/Fisher-Linear-Discriminant/","summary":"Preliminaries linear algebra   inner multiplication projection  Idea of Fisher linear discriminant1 ‘Least-square method’ in classification can only deal with a small set of tasks. That is because it was designed for the regression task. Then we come to the famous Fisher linear discriminant. This method is also discriminative for it gives directly the class to which the input \\(\\mathbf{x}\\) belongs. Assuming that the linear function\n\\[ y=\\mathbf{w}^T\\mathbf{x}+w_0\\tag{1} \\]","title":"Fisher Linear Discriminant(LDA)"},{"content":"Preliminaries convex definition linear algebra   vector length vector direction  Discriminant Function in Classification The discriminant function or discriminant model is on the other side of the generative model. And we, here, have a look at the behavior of the discriminant function in linear classification.1\nIn the post ‘Least Squares Classification’, we have seen, in a linear classification task, the decision boundary is a line or hyperplane by which we separate two classes. And if our model is based on the decision boundary or, in other words, we separate inputs by a function and a threshold, the model is a discriminant model and the decision boundary is formed by the function and a threshold.\nNow, we are going to talk about what the decision boundaries look like in the \\(K\\)-classes problem when \\(K=2\\) and \\(K\u0026gt;2\\). To illustrate the boundaries, we only consider the 2D(two dimensional) input vector \\(\\mathbf{x}\\) who has only two components.\nTwo classes The easiest decision boundary comes from 2-dimensional input space which is separated into 2 regions:\nwhose decision boundary is:\n\\[ \\mathbf{w}^T\\mathbf{x}+w_0=\\text{ constant }\\tag{1} \\]\nThis equation is equal to \\(\\mathbf{w}^T\\mathbf{x}+w_0=0\\) because \\(w_0\\) is also a constant, so it can be merged with the r.h.s. constant. Of course, the 1-dimensional input space is easier than 2-dimensional, and its decision boundary is a point.\nLet’s go back to the line, and it has the following properties:\nThe vector \\(\\mathbf{w}\\) always points to a certain region and is perpendicular to the line. \\(w_0\\) decides the location of the boundary relative to the origin. The perpendicular distance \\(r\\) to the line of a point \\(\\mathbf{x}\\) can be calculated by \\(r=\\frac{y(\\mathbf{x})}{||\\mathbf{w}||}\\) where \\(y(\\mathbf{x})=\\mathbf{w}^T\\mathbf{x}+w_0\\)  Because these three properties are all basic concepts of a line, we just prove the third point roughly:\nproof: We set \\(\\mathbf{x}_{\\perp}\\) is the projection of \\(\\mathbf{x}\\) on the line.\nWe using the first point that \\(\\mathbf{w}\\) is perpendicular to the line and \\(\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\) is the union vector:\n\\[ \\mathbf{x}=\\mathbf{x}_{\\perp}+r\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\tag{2} \\]\nand we substitute equation (2) to the line function \\(y(\\mathbf{x})=\\mathbf{w}^T\\mathbf{x}+w_0\\) :\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;=\\mathbf{w}^T(\\mathbf{x}_{\\perp}+r\\frac{\\mathbf{w}}{||\\mathbf{w}||})+w_0\\\\ \u0026amp;=\\mathbf{w}^T\\mathbf{x}_{\\perp}+\\mathbf{w}^Tr\\frac{\\mathbf{w}}{||\\mathbf{w}||}+w_0\\\\ \u0026amp;=\\mathbf{w}^Tr\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\\\ \u0026amp;=r\\frac{||\\mathbf{w}||^2}{||\\mathbf{w}||}\\\\ \\end{aligned}\\tag{3} \\]\nSo we have\n\\[ r=\\frac{y(\\mathbf{x})}{||\\mathbf{w}||}\\tag{4} \\]\nQ.E.D.\nHowever, augmented vectors \\(\\mathbf{w}= \\begin{bmatrix}w_0\u0026amp;w_1\u0026amp; \\cdots\u0026amp;w_d\\end{bmatrix}^T\\) and \\(\\mathbf{x}= \\begin{bmatrix}1\u0026amp;x_1\u0026amp; \\cdots\u0026amp;x_d\\end{bmatrix}^T\\) can cancel \\(w_0\\) of the original boundary equation. So a \\(d+1\\)-dimensional hyperplane that went through the origin could be instea replaced by an \\(d\\)-dimensional hyperplane.\nMultiple Classes Things changed when we consider more than 2 classes. Their boundaries become more complicated, and we have 3 different strategies for this problem intuitively:\n1-versus-the-rest Classifier This strategy needs at least \\(K-1\\) classifiers(boundaries). Each classifier \\(k\\) just decides which side belongs to class \\(k\\) and the other side does not belong to \\(k\\). So when we have two boundaries, like:\nwhere the region \\(R_4\\) is embarrassed, based on the properties of the decision boundary, and the definition of classification in the post‘From Linear Regression to Linear Classification’, region \\(R_4\\) can not belong to \\(\\mathcal{C}_1\\) and \\(\\mathbb{C}_2\\) simultaneously.\nSo the first strategy can work for some regions, but there are some black whole regions where the input \\(\\mathbf{x}\\) belongs to more than one class and some white whole regions where the input \\(\\mathbf{x}\\) belongs to no classes(region \\(R_3\\) could be such a region)\n1-versus-1 classifier Another kind of multiple class boundary is the combination of several 1-versus-1 linear decision boundaries. Both sides of a decision boundary belong to a certain class, not like the 1-versus-rest classifier. And to a \\(K\\) class task, it needs \\(K(K-1)/2\\) binary discriminant functions.\nHowever, the contradiction still exists. Region \\(R_4\\) belongs to class \\(\\mathcal{C}_1\\), \\(\\mathcal{C}_2\\), and \\(\\mathcal{C}_3\\) simultaneously.\nSo this is also not good for all situations.\n\\(K\\) Linear functions We use a set of \\(K\\) linear functions: \\[ \\begin{aligned} y_1(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_1\\mathbf{x}+w_{10}\\\\ y_2(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_2\\mathbf{x}+w_{20}\\\\ \u0026amp;\\vdots \\\\ y_K(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_K\\mathbf{x}+w_{K0}\\\\ \\end{aligned}\\tag{5} \\]\nand an input belongs to \\(k\\) when \\(y_k(\\mathbf{x})\u0026gt;y_j(\\mathbf{x})\\) where \\(j\\in \\{1,2,\\cdots,K\\}\\) that \\(j\\neq k\\). According to this definition, the decision boundary between class \\(k\\) and class \\(j\\) is \\(y_k(\\mathbf{x})=y_j(\\mathbf{x})\\) where \\(k,j\\in\\{1,2,\\cdots,K\\}\\) and \\(j\\neq k\\). Then a decision hyperplane is defined as:\n\\[ (\\mathbf{w}_k-\\mathbf{w}_j)^T\\mathbf{x}+(w_{k0}-w_{j0})=0\\tag{6} \\]\nThese decision boundaries separate the input spaces into \\(K\\) single connect, convex regions.\nproof: choose two points in the region \\(k\\) that \\(k\\in \\{1,2,\\cdots,K\\}\\). \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) are two points in the region. An arbitrary point on the line between \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) can be written as \\(\\mathbf{x}\u0026#39;=\\lambda \\mathbf{x}_A + (1-\\lambda)\\mathbf{x}_B\\) where \\(0\\leq\\lambda\\leq1\\). For the linearity of \\(y_k(\\mathbf{x})\\) we have:\n\\[ y_k(\\mathbf{x}\u0026#39;)=\\lambda y_k(\\mathbf{x}_A) + (1-\\lambda)y_k(\\mathbf{x}_B)\\tag{7} \\]\nBecause \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) belong to class \\(k\\), \\(y_k(\\mathbf{x}_A)\u0026gt;y_j(\\mathbf{x}_A)\\) and \\(y_k(\\mathbf{x}_B)\u0026gt;y_j(\\mathbf{x}_B)\\) where \\(j\\neq k\\). Then \\(y_k(\\mathbf{x}\u0026#39;)\u0026gt;y_j(\\mathbf{x}\u0026#39;)\\) and the region of class \\(k\\) is convex.\nQ.E.D\nThe last strategy seems good. And what we should do is estimate the parameters of the model. The most famous approaches that will study are: 1. Least square 2. Fisher’s linear discriminant 3. Perceptron algorithm\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Discriminant-Functions-and-Decision-Boundary/","summary":"Preliminaries convex definition linear algebra   vector length vector direction  Discriminant Function in Classification The discriminant function or discriminant model is on the other side of the generative model. And we, here, have a look at the behavior of the discriminant function in linear classification.1\nIn the post ‘Least Squares Classification’, we have seen, in a linear classification task, the decision boundary is a line or hyperplane by which we separate two classes.","title":"Discriminant Functions and Decision Boundary"},{"content":"Preliminaries A Simple Linear Regression Least Squares Estimation From Linear Regression to Linear Classification pseudo-inverse  Least Squares for Classification1 Least-squares for linear regression had been talked about in ‘Simple Linear Regression’. And in this post, we want to find out whether this powerful algorithm can be used in classification.\nRecalling the distinction between the properties of classification and regression, two points need to be emphasized again(‘From Linear Regression to Linear Classification’):\nthe targets of regression are continuous but the targets of classification are discrete. the output of the classification hypothesis could be \\(\\mathbb{P}(\\mathcal{C}_k|\\mathbf{x})\\) generatively or the output is just a class label \\(\\mathcal{C}_k\\) discriminatively.  The generative model will be talked about in other posts. And we focus on discriminative models in these posts which means our hypothesis directly gives which class the input belongs to.\nWe want to use least-squares methods which had been designed and proved for linear regression. And what we could do to extend the least-squares method to classification are:\nmodifying the type of output and designing a discriminative model  Modifying the type of output is to convert the class label into a number, like ‘apple’ to \\(1\\), ‘orange’ to 0. And when we use the 1-of-K label scheme(https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/), we could build the model with \\(K\\) linear functions:\n\\[ \\begin{aligned} y_1(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_1\\mathbf{x}\\\\ y_2(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_2\\mathbf{x}\\\\ \\vdots\u0026amp;\\\\ y_K(\\mathbf{x})\u0026amp;=\\mathbf{w}^T_K\\mathbf{x}\\\\ \\end{aligned}\\tag{1} \\]\nwhere \\(\\mathbf{x}=\\begin{bmatrix}1\u0026amp;x_1\u0026amp;x_2\u0026amp;\\cdots\u0026amp;x_n\\end{bmatrix}^T\\) and \\(\\mathbf{w}_i=\\begin{bmatrix}w_0\u0026amp;w_1\u0026amp;w_2\u0026amp;\\cdots\u0026amp;w_n\\end{bmatrix}^T\\) for \\(i=1,2,\\cdots,K\\). And \\(y_i\\) is the \\(i\\) th component of 1-of-K output for \\(i=1,2,\\cdots,K\\). Clearly, the output of each \\(y_i(\\mathbf{x})\\) is continuous and could not be just \\(0\\) or \\(1\\). So we set the largest value to be 1 and others 0.\nWe had discussed the linear regression with the least squares in a ‘single-target’ regression problem. And that idea can also be employed in the multiple targets regression. And these \\(K\\) parameter vectors \\(\\mathbf{w}_i\\) can be calculated simultaneously. We can rewrite the equation (1) into the matrix form:\n\\[ \\mathbf{y}(\\mathbf{x})=W^T\\mathbf{x}\\tag{2} \\]\nwhere the \\(i\\)th column of \\(W\\) is \\(\\mathbf{w}_i\\)\nThen we employ the least square method for a sample:\n\\[ \\{(\\mathbf{x}_1,\\mathbf{t}_1),(\\mathbf{x}_2,\\mathbf{t}_2),\\cdots,(\\mathbf{x}_m,\\mathbf{t}_m)\\} \\tag{3} \\]\nwhere \\(\\mathbf{t}\\) is a \\(K\\)-dimensional target consisting of \\(k-1\\) 0’s and one ‘1’. And each diminsion of output \\(\\mathbf{y}(\\mathbf{x})_i\\) is the regression result of the corresponding dimension of target \\(t_i\\). And we build up the input matrix \\(X\\) of all \\(m\\) input consisting of \\(\\mathbf{x}^T\\) as rows:\n\\[ X=\\begin{bmatrix} -\u0026amp;\\mathbf{x}^T_1\u0026amp;-\\\\ -\u0026amp;\\mathbf{x}^T_2\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\mathbf{x}^T_K\u0026amp;- \\end{bmatrix}\\tag{4} \\]\nThe sum of square errors is:\n\\[ E(W)=\\frac{1}{2}\\mathrm{Tr}\\{(XW-T)^T(XW-T)\\} \\tag{5} \\]\nwhere the matrix \\(T\\) is the target matrix whose \\(i\\) th row in target vevtor \\(\\mathbf{t}^T_i\\). The trace operation is employed because the only the value \\((W\\mathbf{x}^T_i-\\mathbf{t}_i)^T(W\\mathbf{x}_i^T-\\mathbf{t}_i)\\) for \\(i=1,2,\\cdots,m\\) is meaningful, but \\((W\\mathbf{x}^T_i-\\mathbf{t}_i)^T(W\\mathbf{x}_j^T-\\mathbf{t}_j)\\) where \\(i\\neq j\\) and \\(i,j = 1,2,\\cdots,m\\) is useless.\nTo minimize the linear equation in equation(5), we can get its derivative\n\\[ \\begin{aligned} \\frac{dE(W)}{dW}\u0026amp;=\\frac{d}{dW}(\\frac{1}{2}\\mathrm{Tr}\\{(XW-T)^T(XW-T)\\})\\\\ \u0026amp;=\\frac{1}{2}\\frac{d}{dW}(\\mathrm{Tr}\\{W^TX^TXW-T^TXW-W^TX^TT+T^TT\\})\\\\ \u0026amp;=\\frac{1}{2}\\frac{d}{dW}(\\mathrm{Tr}\\{W^TX^TXW\\}-\\mathrm{Tr}\\{T^TXW\\}\\\\ \u0026amp;-\\mathrm{Tr}\\{W^TX^TT\\}+\\mathrm{Tr}\\{T^TT\\})\\\\ \u0026amp;=\\frac{1}{2}\\frac{d}{dW}(\\mathrm{Tr}\\{W^TX^TXW\\}-2\\mathrm{Tr}\\{T^TXW\\}+\\mathrm{Tr}\\{T^TT\\})\\\\ \u0026amp;=\\frac{1}{2}(X^TXW-X^TT) \\end{aligned}\\tag{6} \\]\nand set it equal to \\(\\mathbf{0}\\):\n\\[ \\begin{aligned} \\frac{1}{2}(X^TXW-X^TT )\u0026amp;= \\mathbf{0}\\\\ W\u0026amp;=(X^TX)^{-1}X^TT \\end{aligned}\\tag{7} \\]\nwhere we assume \\(X^TX\\) can be inverted. The component \\((X^TX)^{-1}X^T\\) is also called pseudo-inverse of the matrix \\(X\\) and it is always denoted as \\(X^{\\dagger}\\).\nCode and Result The code of this algorithm is relatively simple because we have programmed the linear regression before which has the same form of equation (7).\nWhat we should care about is the formation of these matrices \\(W\\), \\(X\\), and \\(T\\).\nwe should first convert the target value into the 1-of-K form:\ndef label_convert(y, method =\u0026#39;1-of-K\u0026#39;):  if method == \u0026#39;1-of-K\u0026#39;:  label_dict = {}  number_of_label = 0  for i in y:  if i not in label_dict:  label_dict[i] = number_of_label  number_of_label += 1  y_ = np.zeros([len(y),number_of_label])  for i in range(len(y)):  y_[i][label_dict[y[i]]] = 1  return y_,number_of_label what we do is count the total number of labels(\\(K\\))and we set the \\(i\\) th component of the 1-of-K target to 1 and other components to 0.\nThe kernel of the algorithm is:\nclass LinearClassifier():  def least_square(self, x, y):  x = np.array(x)  x_dim = x.shape[0]  x = np.c_[np.ones(x_dim), x]  w = np.linalg.pinv(x.transpose().dot(x)).dot(x.transpose()).dot(y)  return w.transpose() the line x = np.c_[np.ones(x_dim), x] is to augment the input vector \\(\\mathbf{x}\\) with a dummy value \\(1\\). And the transpose of the result is to make each row represent a weight vector of eqation (2). The entire project can be found The entire project can be found https://github.com/Tony-Tan/ML and please star me 🥰.\nI have tested the algorithm in several training sets, and the result is like the following figures:\nProblems of Least Squares Lack of robustness if outliers (Figure 2 illustrates this problem) Sum of squares error penalizes the predictions that are too correct(the decision boundary will be tracked to the outlinear as the points at right bottom corner in figure 2) Least-squares workes for regression when we assume the target data has a Gaussian distribution and then the least-squares method maximizes the likelihood function. The distribution of targets in these classification tasks is not Gaussian.  References  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Least-Squares-in-Classification/","summary":"Preliminaries A Simple Linear Regression Least Squares Estimation From Linear Regression to Linear Classification pseudo-inverse  Least Squares for Classification1 Least-squares for linear regression had been talked about in ‘Simple Linear Regression’. And in this post, we want to find out whether this powerful algorithm can be used in classification.\nRecalling the distinction between the properties of classification and regression, two points need to be emphasized again(‘From Linear Regression to Linear Classification’):","title":"Least Squares in Classification"},{"content":"Preliminaries An Introduction to Linear Regression A Simple Linear Regression Bayesian theorem Feature extraction  Recall Linear Regression The goal of a regression problem is to find out a function or hypothesis that given an input \\(\\mathbf{x}\\), it can make a prediction \\(\\hat{y}\\) to estimate the target. Both the target \\(y\\) and prediction \\(\\hat{y}\\) here are continuous. They have the properties of numbers1:\n Consider 3 inputs \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) and \\(\\mathbf{x}_3\\) and their coresponding targets are \\(y_1=0\\), \\(y_2=1\\) and \\(y_3=2\\). Then a good predictor should give the predictions \\(\\hat{y}_1\\), \\(\\hat{y}_2\\) and \\(\\hat{y}_3\\) where the distance between \\(\\hat{y}_1\\) and \\(\\hat{y}_2\\) is larger than the one between \\(\\hat{y}_1\\) and \\(\\hat{y}_3\\)\n Some properties of regression tasks we should pay attention to are:\nThe goal of regression is to produce a hypothesis that can give a prediction as close to the target as possible The output of the hypothesis and target are continuous numbers and have numerical meanings, like distance, velocity, weights, and so on.  General Classification On the other side, we met more classification tasks in our life than regression. Such as in the supermarket we can tell the apple and the orange apart easily. And we can even verify whether this apple is tasty or not.\nThen the goal of classification is clear:\n Assign input \\(\\mathbf{x}\\) to a certain class of \\(K\\) available classes. And \\(\\mathbf{x}\\) must belong to one and only one class.\n The input \\(\\mathbf{x}\\), like the input of regression, can be a feature or basis function and can be continuous or discrete. However, its output is discrete. Let’s go back to the example that we can tell apple, orange, and pineapple apart. The difference between apple and orange and the difference between apple and pineapple can not be compared, because the distance(it is the mathematical name of difference) itself had no means.\nA Binary Code Scheme we can not calculate apple and orange directly. So a usual first step in the classification task is mapping the target or labels of an example into a number, like \\(1\\) for the apple and \\(2\\) for the orange.\nA binary code scheme is another way to code targets.\nFor a two classes mission, the numerical labels can be:\n\\[ \\mathcal{C}_1=0 \\text{ and }\\mathcal{C}_2=1\\tag{1} \\]\nIt’s equal to:\n\\[ \\mathcal{C}_1=1 \\text{ and }\\mathcal{C}_2=0\\tag{2} \\]\nAnd to a \\(K\\) classes target, the binary code scheme is:\n\\[ \\begin{aligned} \\mathcal{C}_1 \u0026amp;= \\{1,0,\\cdots,0\\}\\\\ \\mathcal{C}_2 \u0026amp;= \\{0,1,\\cdots,0\\}\\\\ \\vdots \u0026amp; \\\\ \\mathcal{C}_K \u0026amp;= \\{0,0,\\cdots,1\\}\\\\ \\end{aligned}\\tag{3} \\]\nThe \\(n\\)-dimensional input \\(\\mathbf{x}\\in \\mathbb{R}^n\\) and \\(\\mathbb{R}^n\\) is called the input space. In the classification task, the input points can be separated by the targets, and these parts of space are called decision regions and the boundaries between decision regions are called decision boundaries or decision surfaces. When the decision boundary is linear, the task is called linear classification.\nThere are roughly two kinds of procedures for classification:\nDiscriminant Function: assign input \\(\\mathbf{x}\\) to a certain class directly. We infer \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) firstly and then make a decision based on the posterior probability. Inference of \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) was calculated firstly \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})\\) can also be calculated by Bayesian Theorem \\(\\Pr(\\mathcal{C}_k|\\mathbf{x})=\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}{\\Pr(\\mathbf{x})}=\\frac{\\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}{\\sum_k \\Pr(\\mathbf{x}|\\mathcal{C}_k)\\Pr(\\mathcal{C}_k)}\\)   They are the discriminate model and generative model, respectively.\nLinear Classification In the regression problem, the output of the linear function:\n\\[ \\mathbf{w}^T\\mathbf{x}+b\\tag{4} \\]\nis approximate of the target. But in the classification task, we want the output to be the class to which the input \\(\\mathbf{x}\\) belongs. However, the output of the linear function is always continuous. This output is more like the posterior probability, say \\(\\Pr({\\mathcal{C}_i|\\mathbf{x}})\\) rather than the discrete class label. To generate a class label output, function \\(f(\\cdot)\\) which is called ‘action function’ in machine learning was employed. For example, we can choose a threshold function as the active function:\n\\[ y(\\mathbf{x})=f(\\mathbf{w}^T\\mathbf{x}+b)\\tag{5} \\]\nwhere \\(f(\\cdot)\\) is the threshold function:\n\\[ f(x) = \\begin{cases}1\u0026amp;x\\geq c\\\\0\u0026amp;\\text{otherwise}\\end{cases}\\tag{6} \\] where \\(c\\) is a constant.\nIn this case, the boundary is \\(\\mathbf{w}^T\\mathbf{x}+b = c\\), and it is a line. So we call this kind of model ‘linear classification’. The input \\(\\mathbf{x}\\) can be replaced by a basis function \\(\\phi(\\mathbf{x})\\) as mentioned in the polynomial regression.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/From-Linear-Regression-to-Linear-Classification/","summary":"Preliminaries An Introduction to Linear Regression A Simple Linear Regression Bayesian theorem Feature extraction  Recall Linear Regression The goal of a regression problem is to find out a function or hypothesis that given an input \\(\\mathbf{x}\\), it can make a prediction \\(\\hat{y}\\) to estimate the target. Both the target \\(y\\) and prediction \\(\\hat{y}\\) here are continuous. They have the properties of numbers1:\n Consider 3 inputs \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) and \\(\\mathbf{x}_3\\) and their coresponding targets are \\(y_1=0\\), \\(y_2=1\\) and \\(y_3=2\\).","title":"From Linear Regression to Linear Classification"},{"content":"Priliminaries A Simple Linear Regression Least Squares Estimation  Extending Linear Regression with Features1 The original linear regression is in the form:\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;= b + \\mathbf{w}^T \\mathbf{x}\\\\ \u0026amp;=w_01 + w_1x_1+ w_2x_2+\\cdots + w_{m+1}x_{m+1} \\end{aligned}\\tag{1} \\]\nwhere the input vector \\(\\mathbf{x}\\) and parameter \\(\\mathbf{w}\\) are \\(m\\)-dimension vectors whose first components are \\(1\\) and bias \\(w_0=b\\) respectively. This equation is linear for both the input vector and parameter vector. Then an idea come to us, if we set \\(x_i=\\phi_i(\\mathbf{x})\\) then equation (1) convert to:\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;= b + \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x})\\\\ \u0026amp;=w_01 + w_1\\phi_1(\\mathbf{x})+\\cdots + w_{m+1}\\phi_{m+1}(\\mathbf{x}) \\end{aligned}\\tag{2} \\]\nwhere \\(\\phi(\\mathbf{x})=\\begin{bmatrix}\\phi_1(\\mathbf{x})\\\\\\phi_2(\\mathbf{x})\\\\ \\vdots\\\\\\phi_m(\\mathbf{x})\\end{bmatrix}\\) and \\(\\mathbf{w}=\\begin{bmatrix}w_1\\\\w_2\\\\ \\vdots\\\\ w_m\\end{bmatrix}\\) the function with input \\(\\mathbf{x}\\), \\(\\mathbf{\\phi}(\\cdot)\\) is called feature.\nThis feature function was used widely, especially in reducing dimensions of original input(such as in image processing) and increasing the flexibility of the predictor(such as in extending linear regression to polynomial regression).\nPolynomial Regression When we set the feature as:\n\\[ \\phi(x) = \\begin{bmatrix}x\\\\x^2\\end{bmatrix}\\tag{3} \\]\nthe linear regression converts to:\n\\[ y(\\mathbf{x})=b+ w_1x+w_2x^2\\tag{4} \\]\nHowever, the estimation of the parameter \\(\\mathbf{w}\\) is not changed by the extension of the feature function. Because in the least-squares or other optimization algorithms the parameters or random variables are \\(\\mathbf{w}\\), and we do not care about the change of input space. And when we use the algorithm described in ‘least squares estimation’:\n\\[ \\mathbf{w}=(X^TX)^{-1}X^T\\mathbf{y}\\tag{5} \\]\nto estimate the parameter, we got:\n\\[ \\mathbf{w}=(\\Phi^T\\Phi)^{-1}\\Phi^T\\mathbf{y}\\tag{6} \\]\nwhere \\[ \\Phi=\\begin{bmatrix} -\u0026amp;\\phi(\\mathbf{x_1})^T\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\phi(\\mathbf{x_m})^T\u0026amp;-\\end{bmatrix}\\tag{7} \\]\nCode for polynomial regression To the same task in the ‘least squares estimation’, regression of the weights of the newborn baby with days is like:\nThe linear regression result of a male baby is :\nAnd code of the least square polynomial regression with power \\(d\\) is\ndef fit_polynomial(self, x, y, d):  x_org = np.array(x).reshape(-1, 1)  # add a column which is all 1s to calculate bias  x = np.c_[np.ones(x.size).reshape(-1, 1), x_org]  x_org_d = x_org  # building polynomial with highest power d  for i in range(1, d):  x_org_d = x_org_d * x_org  x = np.c_[x, x_org_d]  y = np.array(y).reshape(-1, 1)  w = np.linalg.inv(x.transpose().dot(x)).dot(x.transpose()).dot(y)  return w The entire project can be found The entire project can be found https://github.com/Tony-Tan/ML and please star me.\nAnd the result of the regression is:\nThe blue regression line looks pretty well compared to the right line.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Polynomial-Regression-and-Features-Extension-of-Linear-Regression/","summary":"Priliminaries A Simple Linear Regression Least Squares Estimation  Extending Linear Regression with Features1 The original linear regression is in the form:\n\\[ \\begin{aligned} y(\\mathbf{x})\u0026amp;= b + \\mathbf{w}^T \\mathbf{x}\\\\ \u0026amp;=w_01 + w_1x_1+ w_2x_2+\\cdots + w_{m+1}x_{m+1} \\end{aligned}\\tag{1} \\]\nwhere the input vector \\(\\mathbf{x}\\) and parameter \\(\\mathbf{w}\\) are \\(m\\)-dimension vectors whose first components are \\(1\\) and bias \\(w_0=b\\) respectively. This equation is linear for both the input vector and parameter vector. Then an idea come to us, if we set \\(x_i=\\phi_i(\\mathbf{x})\\) then equation (1) convert to:","title":"Polynomial Regression and Features-Extension of Linear Regression"},{"content":"Priliminaries A Simple Linear Regression Least Squares Estimation linear algebra  Square Loss Function for Regression1 For any input \\(\\mathbf{x}\\), our goal in a regression task is to give a prediction \\(\\hat{y}=f(\\mathbf{x})\\) to approximate target \\(t\\) where the function \\(f(\\cdot)\\) is the chosen hypothesis or model as mentioned in the post https://anthony-tan.com/A-Simple-Linear-Regression/.\nThe difference between \\(t\\) and \\(\\hat{y}\\) can be called ‘error’ or more precisely ‘loss’. Because in an approximation task, ‘error’ occurs by chance and always exists, and ‘loss’ is a good word to represent the difference. The loss can be written generally as function \\(\\ell(f(\\mathbf{x}),t)\\). Intuitively, the smaller the loss, the better the approximation.\nSo the expectation of loss:\n\\[ \\mathbb E[\\ell]=\\int\\int \\ell(f(\\mathbf{x}),t)p(\\mathbf{x},t)d \\mathbf{x}dt\\tag{1} \\]\nshould be as small as possible.\nIn probability viewpoint, the input vector \\(\\mathbf{x}\\), target \\(t\\) and parameters in function(model) \\(f(\\cdot)\\) are all random variables. Then the expectation of loss function may exist.\nConsidering the square error loss function \\(e=(f(\\mathbf{x})-t)^2\\), it is a usual measure of the difference between the prediction and the target. And substitute the loss function into equation (1), we have:\n\\[ \\mathbb E[\\ell]=\\int\\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)d \\mathbf{x}dt\\tag{2} \\]\nTo minimize this function, we could use Euler-Lagrange equation, Fundamental theorem of calculus and Fubini’s theorem:\nFubini’s theorem told us that we can change the order of integration: \\[ \\begin{aligned} \\mathbb E[\\ell]\u0026amp;=\\int\\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)d \\mathbf{x}dt\\\\ \u0026amp;=\\int\\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)dtd \\mathbf{x} \\end{aligned}\\tag{3} \\]\nAccording to the Euler-Lagrange equation, we first create a new function \\(G(x,f,f\u0026#39;)\\): \\[ G(x,f,f\u0026#39;)= \\int (f(\\mathbf{x})-t)^2p(\\mathbf{x},t)dt\\tag{4} \\]\nThe Euler-Lagrange equation is used to minimize the equation (2): \\[ \\frac{\\partial G}{\\partial f}-\\frac{d}{dx}\\frac{\\partial G}{\\partial f\u0026#39;}=0\\tag{5} \\]\nBecause there is no \\(y\u0026#39;\\) component in function \\(G()\\). Then the equation: \\[ \\frac{\\partial G}{\\partial f}=0\\tag{6} \\] becomes the necessary condition to minimize the equation (2):\n\\[ 2\\int (f(\\mathbf{x})-t)p(\\mathbf{x},t)dt=0 \\tag{7} \\]\nRearrange the equation (7), and we get a good predictor that can minimize the square loss function :\n\\[ \\begin{aligned} \\int (f(\\mathbf{x})-t)p(\\mathbf{x},t)dt\u0026amp;=0\\\\ \\int f(\\mathbf{x})p(\\mathbf{x},t)dt-\\int tp(\\mathbf{x},t)dt\u0026amp;=0\\\\ f(\\mathbf{x})\\int p(\\mathbf{x},t)dt\u0026amp;=\\int tp(\\mathbf{x},t)dt\\\\ f(\\mathbf{x})\u0026amp;=\\frac{\\int tp(\\mathbf{x},t)dt}{\\int p(\\mathbf{x},t)dt}\\\\ f(\\mathbf{x})\u0026amp;=\\frac{\\int tp(\\mathbf{x},t)dt}{p(\\mathbf{x})}\\\\ f(\\mathbf{x})\u0026amp;=\\int tp(t|\\mathbf{x})dt\\\\ f(\\mathbf{x})\u0026amp;= \\mathbb{E}_t[t|\\mathbf{x}] \\end{aligned}\\tag{8} \\]\nWe finally find the expectation of \\(t\\) given \\(\\mathbf{x}\\) is the optimum solution. The expectation of \\(t\\) given \\(\\mathbf{x}\\) is also called the regression function.\nA small summary: \\(\\mathbb{E}[t| \\mathbf{x}]\\) is a good estimate of \\(f(\\mathbf{x})\\)\nMaximum Likelihood Estimation Generally, we assume that there is a generator behind the data:\n\\[ t=g(\\mathbf{x},\\mathbf{w})+\\epsilon\\tag{9} \\]\nwhere the function \\(g(\\mathbf{x},\\mathbf{w})\\) is a deterministic function, \\(t\\) is the target variable and \\(\\epsilon\\) is zero mean Gaussian random variable with percision \\(\\beta\\) which is the inverse variance. Because of the property of Gaussian distribution, \\(t\\) has a Gaussian distribution, with mean(expectation) \\(g(\\mathbf{x},\\mathbf{w})\\) and percesion \\(\\beta\\). And recalling the standard form of Gaussian distribution:\n\\[ \\begin{aligned} \\Pr(t|\\mathbf{x},\\mathbf{w},\\beta)\u0026amp;=\\mathcal{N}(t|g(\\mathbf{x},\\mathbf{w}),\\beta^{-1})\\\\ \u0026amp;=\\frac{\\beta}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{1}{2}(\\beta(x-\\mu)^2)} \\end{aligned}\\tag{10} \\]\nOur task here is to approximate the generator in equation (9) with a linear function. Somehow, when we use the square loss function, the optimum solution for this task is \\(\\mathbb{E}[t|\\mathbf{x}]\\) to equation (8).\nthe solution to equation (10) is:\n\\[ \\mathbb{E}[t|\\mathbf{x}]=g(\\mathbf{x},\\mathbf{w})\\tag{11} \\]\nWe set the linear model as: \\[ f(x)=\\mathbf{w}^T\\mathbf{x}+b\\tag{12} \\]\nand this can be converted to:\n\\[ f(x)= \\begin{bmatrix} b\u0026amp;\\mathbf{w}^T \\end{bmatrix} \\begin{bmatrix} 1\\\\ \\mathbf{x} \\end{bmatrix}=\\mathbf{w}_a^T\\mathbf{x}_a \\tag{13} \\]\nfor short, we just write the \\(\\mathbf{w}_a\\) and \\(\\mathbf{x}_a\\) as \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\). Then the linear model becomes:\n\\[ f(x)=\\mathbf{w}^T\\mathbf{x}\\tag{14} \\]\nAs we mentioned above we consider all the parameter as a random variable, then the conditioned distribution of \\(\\mathbf{w}\\) is \\(\\Pr(\\mathbf{w}|\\mathbf{t},\\beta)\\). \\(X\\) or \\(\\mathbf{x}\\) was omitted in the condition because it does not affect the result at all. And the Bayesian theorem told us:\n\\[ \\Pr(\\mathbf{w}|\\mathbf{t},\\beta)=\\frac{\\Pr( \\mathbf{t}|\\mathbf{w},\\beta) \\Pr(\\mathbf{w})} {\\Pr(\\mathbf{t})}=\\frac{\\text{Likelihood}\\times \\text{Prior}}{\\text{Evidence}}\\tag{15} \\]\nWe want to find the \\(\\mathbf{w}^{\\star}\\) that maximise the posterior probability \\(\\Pr(\\mathbf{w}|\\mathbf{t},\\beta)\\). Because \\(\\Pr(\\mathbf{t})\\) and \\(\\Pr(\\mathbf{w})\\) are constant. Then the maximum of likelihood \\(\\Pr(\\mathbf{t}|\\mathbf{w},\\beta)\\) maximise the posterior probability.\n\\[ \\begin{aligned} \\Pr(\\mathbf{t}|\\mathbf{w},\\beta)\u0026amp;=\\Pi_{i=0}^{N}\\mathcal{N}(t_i|\\mathbf{w}^T\\mathbf{x}_i,\\beta^{-1})\\\\ \\ln \\Pr(\\mathbf{t}|\\mathbf{w},\\beta)\u0026amp;=\\sum_{i=0}^{N}\\ln \\mathcal{N}(t_i|\\mathbf{w}^T\\mathbf{x}_i,\\beta^{-1})\\\\ \u0026amp;=\\sum_{i=0}^{N}\\ln \\frac{\\beta}{\\sqrt{2\\pi}}\\mathrm{e}^{-\\frac{1}{2}(\\beta(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2)}\\\\ \u0026amp;=\\sum_{i=0}^{N} \\ln \\beta - \\sum_{i=0}^{N} \\ln \\sqrt{2\\pi} - \\frac{1}{2}\\beta\\sum_{i=0}^{N}(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2 \\end{aligned}\\tag{16} \\]\nThis gives us a wonderful result.\nWe can only control the component \\(\\frac{1}{2}\\beta\\sum_{i=0}^{N}(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2\\) of the last line of equation(16), because \\(\\sum_{i=0}^{N} \\ln \\beta\\) and \\(- \\sum_{i=0}^{N} \\ln \\sqrt{2\\pi}\\) were decided by the assumptions. In other words, to maximise the likelihood, we just need to minimise:\n\\[ \\sum_{i=0}^{N}(t_i-\\mathbf{w}^T\\mathbf{x}_i)^2\\tag{17} \\]\nThis was just to minimize the sum of squares. Then this optimization problem went back to the least square problem.\nLeast Square Estimation and Maximum Likelihood Estimation When we assume there is a generator:\n\\[ t=g(\\mathbf{x},\\mathbf{w})+\\epsilon\\tag{18} \\]\nbehind the data, and \\(\\epsilon\\) has a zero-mean Gaussian distribution with any precision \\(\\beta\\), the maximum likelihood estimation finally converts to the least square estimation. This is not only worked for linear regression because we did not assume what \\(g(\\mathbf{x},\\mathbf{w})\\) is.\nHowever, when the \\(\\epsilon\\) has a different distribution but not Gaussian distribution, the least square estimation will not be the optimum solution for maximum likelihood estimation.\nReferences  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Maximum-Likelihood-Estimation/","summary":"Priliminaries A Simple Linear Regression Least Squares Estimation linear algebra  Square Loss Function for Regression1 For any input \\(\\mathbf{x}\\), our goal in a regression task is to give a prediction \\(\\hat{y}=f(\\mathbf{x})\\) to approximate target \\(t\\) where the function \\(f(\\cdot)\\) is the chosen hypothesis or model as mentioned in the post https://anthony-tan.com/A-Simple-Linear-Regression/.\nThe difference between \\(t\\) and \\(\\hat{y}\\) can be called ‘error’ or more precisely ‘loss’. Because in an approximation task, ‘error’ occurs by chance and always exists, and ‘loss’ is a good word to represent the difference.","title":"Maximum Likelihood Estimation"},{"content":"Priliminaries A Simple Linear Regression the column space  Another Example of Linear Regression 1 In the blog A Simple Linear Regression, squares of the difference between the output of a predictor and the target were used as a loss function in a regression problem. And it could be also written as:\n\\[ \\ell(\\hat{\\mathbf{y}}_i,\\mathbf{y}_i)=(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i)^T(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i) \\tag{1} \\]\nThe linear regression model in a matrix form is:\n\\[ y=\\mathbf{w}^T\\mathbf{x}+\\mathbf{b}\\tag{2} \\]\nWhat we do in this post is analyze the least-squares methods from two different viewpoints\nConsider a new training set, newborn weights, and time from the WHO:\n  Day Male(kg) Female(kg)    0 3.5 3.4  15 4.0 3.8  45 4.9 4.5  75 5.7 5.2  105 6.4 5.9  135 7.0 6.4  165 7.6 7.0  195 8.2 7.5  225 8.6 7.9  255 9.1 8.3  285 9.5 8.7  315 9.8 9.0  345 10.2 9.4  375 10.5 9.7    View of algebra This is just what the post A Simple Linear Regression did. The core idea of this view is that the loss function is quadratic so its stationary point is the minimum or maximum. Then what to do is just find the stationary point.\nAnd its result is: View of Geometric Such a simple example with just two parameters above had almost messed us up in calculation. However, the practical task may have more parameters, say hundreds or thousands of parameters. It is impossible for us to solve that in a calculus way.\nNow let’s review the linear relation in equation (2) and when we have a training set of \\(m\\) points : \\[ \\{(\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),\\dots,(\\mathbf{x}_m,y_m)\\}\\tag{3} \\]\nBecause they are sampled from an identity “machine”. They can be stacked together in a matrix form as:\n\\[ \\begin{bmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_m \\end{bmatrix}=\\begin{bmatrix} -\u0026amp;\\mathbf{x}_1^T\u0026amp;-\\\\ -\u0026amp;\\mathbf{x}_2^T\u0026amp;-\\\\ \u0026amp;\\vdots\u0026amp;\\\\ -\u0026amp;\\mathbf{x}_m^T\u0026amp;- \\end{bmatrix}\\mathbf{w}+I_m\\mathbf{b}\\tag{4} \\]\nwhere \\(I_m\\) is an identical matrix whose column and row is \\(m\\) and \\(\\mathbf{b}\\) is \\(b\\) repeating \\(m\\) times. To make the equation shorter and easier to calculate, we can put \\(b\\) into the vector \\(\\mathbf{w}\\) like:\n\\[ \\begin{bmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_m \\end{bmatrix}=\\begin{bmatrix} 1\u0026amp;-\u0026amp;\\mathbf{x}_1^T\u0026amp;-\\\\ 1\u0026amp;-\u0026amp;\\mathbf{x}_2^T\u0026amp;-\\\\ 1\u0026amp;\u0026amp;\\vdots\u0026amp;\\\\ 1\u0026amp;-\u0026amp;\\mathbf{x}_m^T\u0026amp;- \\end{bmatrix} \\begin{bmatrix} b\\\\ \\mathbf{w} \\end{bmatrix} \\tag{5} \\]\nWe use a simplified equation to represent the relation in equation(5): \\[ \\mathbf{y} = X\\mathbf{w}\\tag{6} \\]\nFrom the linear algebra points, equation(6) represents that \\(\\mathbf{y}\\) is in the column space of \\(X\\). However, when \\(\\mathbf{y}\\) isn’t, the equation (6) does not hold anymore. And what we need to do next is to find a vector \\(\\mathbf{\\hat{y}}\\) in the column space which is the closest one to the vector \\(\\mathbf{y}\\):\n\\[ \\arg\\min_{\\mathbf{\\hat{y}}=X\\mathbf{w}} ||\\mathbf{y}-\\mathbf{\\hat{y}}||\\tag{7} \\]\nAnd as we have known, the projection of \\(\\mathbf{y}\\) to the column space of \\(X\\) has the shortest distance to \\(\\mathbf{y}\\)\nAccording to linear algebra, the closest vector in a subspace to a vector is its projection in that subspace. Then our mission now is to find \\(\\mathbf{w}\\) to make:\n\\[ \\mathbf{\\hat{y}} = X\\mathbf{w}\\tag{8} \\]\nwhere \\(\\mathbf{\\hat{y}}\\) is the projection of \\(\\mathbf{y}\\) in the column space of \\(X\\).\nAccording to the projection equation in linear algebra:\n\\[ \\mathbf{\\hat{y}}=X(X^TX)^{-1}X^T\\mathbf{y}\\tag{22} \\]\nThen substitute equation (8) into equation (9) and assuming \\((X^TX)^{-1}\\) exists:\n\\[ \\begin{aligned} X\\mathbf{w}\u0026amp;=X(X^TX)^{-1}X^T\\mathbf{y}\\\\ X^TX\\mathbf{w}\u0026amp;=X^TX(X^TX)^{-1}X^T\\mathbf{y}\\\\ X^TX\\mathbf{w}\u0026amp;=X^T\\mathbf{y}\\\\ \\mathbf{w}\u0026amp;=(X^TX)^{-1}X^T\\mathbf{y} \\end{aligned}\\tag{10} \\]\nTo a thin and tall matrix, \\(X\\), which means that the number of sample points in the training set is far more than the dimension of a sample point, \\((X^TX)^{-1}\\) exists usually.\nCode of Linear Regression(Matrix Form) import pandas as pds import numpy as np import matplotlib.pyplot as plt   class LeastSquaresEstimation():  def __init__(self, method=\u0026#39;OLS\u0026#39;):  self.method = method   def fit(self, x, y):  x = np.array(x).reshape(-1, 1)  # add a column which is all 1s to calculate bias of linear function  x = np.c_[np.ones(x.size).reshape(-1, 1), x]  y = np.array(y).reshape(-1, 1)  if self.method == \u0026#39;OLS\u0026#39;:  w = np.linalg.inv(x.transpose().dot(x)).dot(x.transpose()).dot(y)  b = w[0][0]  w = w[1][0]  return w, b   if __name__ == \u0026#39;__main__\u0026#39;:  data_file = pds.read_csv(\u0026#39;./data/babys_weights_by_months.csv\u0026#39;)  lse = LeastSquaresEstimation()  weight_male, bias_male = lse.fit(data_file[\u0026#39;day\u0026#39;],data_file[\u0026#39;male\u0026#39;])  day_0 = data_file[\u0026#39;day\u0026#39;][0]  day_end = list(data_file[\u0026#39;day\u0026#39;])[-1]  days = np.array([day_0,day_end])  plt.scatter(data_file[\u0026#39;day\u0026#39;], data_file[\u0026#39;male\u0026#39;], c=\u0026#39;r\u0026#39;, label=\u0026#39;male\u0026#39;, alpha=0.5)  plt.scatter(data_file[\u0026#39;day\u0026#39;], data_file[\u0026#39;female\u0026#39;], c=\u0026#39;b\u0026#39;, label=\u0026#39;female\u0026#39;, alpha=0.5)  plt.xlabel(\u0026#39;days\u0026#39;)  plt.ylabel(\u0026#39;weight(kg)\u0026#39;)  plt.legend()  plt.show() the entire project can be found at https://github.com/Tony-Tan/ML and please star me 😀.\nIts output is also like:\nReference  Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.↩︎\n   ","permalink":"https://anthony-tan.com/Least-Squares-Estimation/","summary":"Priliminaries A Simple Linear Regression the column space  Another Example of Linear Regression 1 In the blog A Simple Linear Regression, squares of the difference between the output of a predictor and the target were used as a loss function in a regression problem. And it could be also written as:\n\\[ \\ell(\\hat{\\mathbf{y}}_i,\\mathbf{y}_i)=(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i)^T(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i) \\tag{1} \\]\nThe linear regression model in a matrix form is:\n\\[ y=\\mathbf{w}^T\\mathbf{x}+\\mathbf{b}\\tag{2} \\]\nWhat we do in this post is analyze the least-squares methods from two different viewpoints","title":"Least Squares Estimation"},{"content":"Preliminaries Linear Algebra(the concepts of space, vector) Calculus An Introduction to Linear Regression  Notations of Linear Regression1 We have already created a simple linear model in the post “An Introduction to Linear Regression”. According to the definition of linearity, we can develop the simplest linear regression model:\n\\[ Y\\sim w_1X+w_0\\tag{1} \\]\nwhere the symbol \\(\\sim\\) is read as “is approximately modeled as”. Equation (1) can also be described as “regressing \\(Y\\) on \\(X\\)(or \\(Y\\) onto \\(X\\))”.\nGo back to the example that was given in “An Introduction to Linear Regression”. Combining with the equation (1), we get a model of the budget for TV advertisement and sales:\n\\[ \\text{Sales}=w_1\\times \\text{TV}+ w_0\\tag{2} \\]\nAssuming we have a machine here, which can turn grain into flour, the input is the grain, \\(X\\) in equation (1), and the output is flour, \\(Y\\). Accordingly, \\(\\mathbf{w}\\) is the gears in the machine.\nThen the mathematically model is:\n\\[ y=\\hat{w_1}x+\\hat{w_0}\\tag{3} \\]\nThe hat symbol “\\(\\;\\hat{}\\;\\)” is used to present that this variable is a prediction, which means it is not the true value of the variable but a conjecture through certain mathematical strategies or methods else.\nThen, a new input \\(x_i\\) has its prediction:\n\\[ \\hat{y}_i=\\hat{w_1}x_i+\\hat{w_0}\\tag{4} \\]\nStatistical learning mainly studies \\(\\begin{bmatrix}\\hat{w_0}\\\\\\hat{w_1}\\end{bmatrix}\\) but machine learning concerns more about \\(\\hat{y}\\) All of them were based on the observed data.\nOnce we got this model, what we do next is estimating the parameters\nEstimating the Parameters For the advertisement task, what we have are a linear regression model equation(2) and a set of observations:\n\\[ \\{(x_1,y_1),(x_2,y_2),(x_3,y_3),\\dots,(x_n,y_n)\\}\\tag{5} \\]\nwhich is also known as training set. By the way, \\(x_i\\) in equation (5) is a sample of \\(X\\) and so is \\(y_i\\) of \\(Y\\). \\(n\\) is the size of the training set, the number of observations pairs.\nThe method we employed here is based on a measure of the “closeness” of the outputs of the model to the observed target (\\(y\\)s in set (5)). By far, the most used method is the “least squares criterion”.\nThe outputs \\(\\hat{y}_i\\) of current model(parameters) to every input \\(x_i\\) are:\n\\[ \\{(x_1,\\hat{y}_1),(x_2,\\hat{y}_2),(x_3,\\hat{y}_3),\\dots,(x_n,\\hat{y}_n)\\}\\tag{6} \\]\nand the difference between \\(\\hat{y}_i\\) and \\(y_i\\) is called residual and written as \\(e_i\\):\n\\[ e_i=y_i-\\hat{y}_i\\tag{7} \\]\n\\(y_i\\) is the target, which is the value our model is trying to achieve. So, the smaller the \\(|e_i|\\) is, the better the model is. Because the absolute operation is not a good analytic operation, we replace it with the quadratic operation:\n\\[ \\mathcal{L}_\\text{RSS}=e_1^2+e_2^2+\\dots+e_n^2\\tag{8} \\]\n\\(\\mathcal{L}_\\text{RSS}\\) means “Residual Sum of Squares”, the sum of total square residual. And to find a better model, we need to minimize the sum of the total residual. In machine learning, this is called loss function.\nNow we take equations (4),(7) into (8):\n\\[ \\begin{aligned} \\mathcal{L}_\\text{RSS}=\u0026amp;(y_1-\\hat{w_1}x_1-\\hat{w_0})^2+(y_2-\\hat{w_1}x_2-\\hat{w_0})^2+\\\\ \u0026amp;\\dots+(y_n-\\hat{w_1}x_n-\\hat{w_0})^2\\\\ =\u0026amp;\\sum_{i=1}^n(y_i-\\hat{w_1}x_i-\\hat{w_0})^2 \\end{aligned}\\tag{9} \\]\nTo minimize the function “\\(\\mathcal{L}_\\text{RSS}\\)”, the calculus told us the possible minimum(maximum) points always stay at stationary points. And the stationary points are the points where the derivative of the function is zero. Remember that the minimum(maximum) points must be stationary points, but the stationary point is not necessary to be a minimum(maximum) point. For more information, ‘Numerical Optimization’ is a good book.\nSince the ‘\\(\\mathcal{L}_\\text{RSS}\\)’ is a function of a vector \\(\\begin{bmatrix}w_0\u0026amp;w_1\\end{bmatrix}^T\\), the derivative is replaced by partial derivative. As the ‘\\(\\mathcal{L}_\\text{RSS}\\)’ is just a simple quadric surface, the minimum or maximum exists, and there is one and only one stationary point.\nThen our mission to find the best parameters for the regression has been converted to calculus the solution of the function system that the derivative(partial derivative) is set to zero.\nThe partial derivative of \\(\\hat{w_1}\\) is\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}=\u0026amp;-2\\sum_{i=1}^nx_i(y_i-\\hat{w_1}x_i-\\hat{w_0})\\\\ =\u0026amp;-2(\\sum_{i=1}^nx_iy_i-\\hat{w_1}\\sum_{i=1}^nx_i^2-\\hat{w_0}\\sum_{i=1}^nx_i) \\end{aligned}\\tag{10} \\]\nand derivative of \\(\\hat{w_0}\\) is:\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_0}}}=\u0026amp;-2\\sum_{i=1}^n(y_i-\\hat{w_1}x_i-\\hat{w_0})\\\\ =\u0026amp;-2(\\sum_{i=1}^ny_i-\\hat{w_1}\\sum_{i=1}^nx_i-\\sum_{i=1}^n\\hat{w_0}) \\end{aligned}\\tag{11} \\]\nSet both of them to zero and we can get:\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_0}}}\u0026amp;=0\\\\ \\hat{w_0} \u0026amp;=\\frac{\\sum_{i=1}^ny_i-\\hat{w_1}\\sum_{i=1}^nx_i}{n}\\\\ \u0026amp;=\\bar{y}-\\hat{w_1}\\bar{x} \\end{aligned}\\tag{12} \\]\nand\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}\u0026amp;=0\\\\ \\hat{w_1}\u0026amp;=\\frac{\\sum_{i=1}^nx_iy_i-\\hat{w_0}\\sum_{i=1}^nx_i}{\\sum_{i=1}^nx_i^2} \\end{aligned}\\tag{13} \\]\nTo get a equation of \\(\\hat{w_1}\\) independently, we take equation(13) to equation(12):\n\\[ \\begin{aligned} \\frac{\\partial{\\mathcal{L}_\\text{RSS}}}{\\partial{\\hat{w_1}}}\u0026amp;=0\\\\ \\hat{w_1}\u0026amp;=\\frac{\\sum_{i=1}^nx_i(y_i-\\bar{y})}{\\sum_{i=1}^nx_i(x_i-\\bar{x})} \\end{aligned}\\tag{14} \\]\nwhere \\(\\bar{x}=\\frac{\\sum_{i=1}^nx_i}{n}\\) and \\(\\bar{y}=\\frac{\\sum_{i=1}^ny_i}{n}\\)\nBy the way, equation (14) has another form:\n\\[ \\hat{w_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})(x_i-\\bar{x})}\\tag{15} \\]\nand they are equal.\nDiagrams and Code Using python to demonstrate our result Equ. (12)(14) is correct:\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt  # load data from csv file by pandas AdvertisingFilepath=\u0026#39;./data/Advertising.csv\u0026#39; data=pd.read_csv(AdvertisingFilepath)  # convert original data to numpy array data_TV=np.array(data[\u0026#39;TV\u0026#39;]) data_sale=np.array(data[\u0026#39;sales\u0026#39;])  # calculate mean of x and y y_sum=0 y_mean=0 x_sum=0 x_mean=0 for x,y in zip(data_TV,data_sale):  y_sum+=y  x_sum+=x if len(data_sale)!=0:  y_mean=y_sum/len(data_sale) if len(data_TV)!=0:  x_mean=x_sum/len(data_TV)  # calculate w_1 w_1=0 a=0 b=0 for x,y in zip(data_TV,data_sale):  a += x*(y-y_mean)  b += x*(x-x_mean) if b!=0:  w_1=a/b  # calculate w_0 w_0=y_mean-w_1*x_mean  # draw a picture plt.xlabel(\u0026#39;TV\u0026#39;) plt.ylabel(\u0026#39;Sales\u0026#39;) plt.title(\u0026#39;TV and Sales\u0026#39;) plt.scatter(data_TV,data_sale,s=8,c=\u0026#39;g\u0026#39;, alpha=0.5) x=np.arange(-10,350,0.1) plt.plot(x,w_1*x+w_0,\u0026#39;r-\u0026#39;) plt.show() After running the code, we got:\nReference  James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.↩︎\n   ","permalink":"https://anthony-tan.com/A-Simple-Linear-Regression/","summary":"Preliminaries Linear Algebra(the concepts of space, vector) Calculus An Introduction to Linear Regression  Notations of Linear Regression1 We have already created a simple linear model in the post “An Introduction to Linear Regression”. According to the definition of linearity, we can develop the simplest linear regression model:\n\\[ Y\\sim w_1X+w_0\\tag{1} \\]\nwhere the symbol \\(\\sim\\) is read as “is approximately modeled as”. Equation (1) can also be described as “regressing \\(Y\\) on \\(X\\)(or \\(Y\\) onto \\(X\\))”.","title":"A Simple Linear Regression"},{"content":"Preliminariess Linear Algebra(the concepts of space, vector) Calculus  What is Linear Regression Linear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors).\nMachine Learning and Statistical Learning Machine learning and statistical learning are similar but have some distinctions. In machine learning, models, regression models, or classification models, are used to predict the outputs of the new incoming inputs.\nIn contrast, in statistical learning, regression and classification are employed to model the data to find out the hidden relations among the inputs. In other words, the models of data, no matter they are regression or classification, or something else. They are used to analyze the mechanism behind the data.\n“Linear” Linear is a property of the operation \\(f(\\cdot)\\), which has the following two properties:\n\\(f(a\\mathbf{x})=af(\\mathbf{x})\\) \\(f(\\mathbf{x}+\\mathbf{y})=f(\\mathbf{x})+f(\\mathbf{y})\\)  where \\(a\\) is a scalar. Then we say \\(f(\\cdot)\\) is linear or \\(f(\\cdot)\\) has a linearity property.\nThe linear operation can be represented as a matrix. And when a 2-dimensional linear operation was drawn on the paper, it is a line. Maybe that is why it is named linear, I guess.\n“Regression” In statistical or machine learning, regression is a crucial part of the whole field. And, the other part is the well-known classification. If we have a close look at the outputs data type, one distinction between them is that the output of regression is continuous but the output of classification is discrete.\nWhat is linear regression Linear regression is a regression model. All parameters in the model are linear, like:\n\\[ f(\\mathbf{x})=w_1x_1+w_2x_2+w_3x_3\\tag{1} \\]\nwhere the \\(w_n\\) where \\(n=1,2,3\\) are the parameters of the model, the output \\(f(\\mathbf{x})\\) can be written as \\(t\\) for 1-deminsional outputs (or \\(\\mathbf{t}\\) for multi-deminsional outputs).\n\\(f(\\mathbf{w})\\) is linear:\n\\[ \\begin{aligned} f(a\\cdot\\mathbf{w})\u0026amp;=aw_1x_1+aw_2x_2+aw_3x_3=a\\cdot f(\\mathbf{w}) \\\\ f(\\mathbf{w}+\\mathbf{v})\u0026amp;=(w_1+v_1)x_1+(w_2+v_2)x_2+(w_3+v_3)x_3\\\\ \u0026amp;=w_1x_1+v_1x_1+w_2x_2+v_2x_2+w_3x_3+v_3x_3\\\\ \u0026amp;=f(\\mathbf{w})+f(\\mathbf{v}) \\end{aligned}\\tag{2} \\]\nwhere \\(a\\) is a scalar, and \\(\\mathbf{v}\\) is in the same space with \\(\\mathbf{w}\\)\nQ.E.D\nThere is also another view that the linear property of models is also for \\(\\mathbf{x}\\), the input.\nBut the following model\n\\[ t=f(\\mathbf{x})=w_1\\log(x_1)+w_2\\sin(x_2)\\tag{3} \\]\nis a case of linear regression problem in our definition. But from the second point of view, it is not linear for inputs \\(\\mathbf{x}\\). However, this is not an unsolvable contradiction. If we use:\n\\[ y_1= \\log(x_1)\\\\ y_2= \\sin(x_2)\\tag{4} \\]\nto replace the \\(\\log\\) and \\(\\sin\\) in equation (3), we get again\n\\[ t=f(\\mathbf{y})=w_1y_1+w_2y_2\\tag{5} \\]\na linear operation for both input \\(\\mathbf{y}=\\begin{bmatrix}y_1\\;y_2\\end{bmatrix}^T\\) and parameters \\(\\mathbf{z}\\) .\nThe tranformation, equation(4), is called feature extraction. \\(\\mathbf{y}\\) is called features, and \\(\\log\\) and \\(\\sin\\) are called basis functions\nAn Example This example is taken from (James20131), It is about the sale between different kinds of advertisements. I downloaded the data set from http://faculty.marshall.usc.edu/gareth-james/ISL/data.html. It’s a CSV file, including 200 rows. Here I draw 3 pictures using ‘matplotlib’ to make the data more visible. They are advertisements for ‘TV’, ‘Radio’, ‘Newspaper’ to ‘Sales’ respectively.\nFrom these figures, we can find TV ads and Sales looks like having a stronger relationship than radio ads and sales. However, the Newspaper ads and Sales look independent.\nFor statistical learning, we should take statistical methods to investigate the relation in the data. And in machine learning, to a certain input, predicting an output is what we are concerned.\nWhy Linear Regression Linear regression has been used for more than 200 years, and it’s always been our first class of statistical learning or machine learning. Here we list 3 practical elements of linear regression, which are essential for the whole subject:\nIt is still working in some areas. Although more complicated models have been built, they could not be replaced totally. It is a good jump-off point to the other more feasible and adorable models, which may be an extension or generation of naive linear regression Linear regression is easy, so it is possible to be analyzed mathematically.  This is why linear regression is always our first step to learn machine learning and statistical learning. And by now, this works pretty well.\nA Probabilistic View Machine learning or statistical learning can be described from two different views - Bayesian and Frequentist. They both worked well for some different instances, but they also have their limitations. The Bayesian view of the linear regression will be talked about as well later.\nBayesian statisticians thought the input \\(\\mathbf{x}\\), the output \\(t\\), and the parameter \\(\\mathbf{w}\\) are all random variables, while the frequentist does not think so. Bayesian statisticians predict the unknown input \\(\\mathbf{x}_0\\) by forming the distribution \\(\\mathbb{P}(t_0|\\mathbf{x}_0)\\) and then sampling from it. To achieve this goal, we must build the \\(\\mathbb{P}(t|\\mathbf{x})\\) firstly. This is the modeling progress, or we can call it learning progress.\nReferences  James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.↩︎\n   ","permalink":"https://anthony-tan.com/An-Introduction-to-Linear-Regression/","summary":"Preliminariess Linear Algebra(the concepts of space, vector) Calculus  What is Linear Regression Linear regression is a basic idea in statistical and machine learning based on the linear combination. And it was usually used to predict some responses to some inputs(predictors).\nMachine Learning and Statistical Learning Machine learning and statistical learning are similar but have some distinctions. In machine learning, models, regression models, or classification models, are used to predict the outputs of the new incoming inputs.","title":"An Introduction to Linear Regression"},{"content":"About Me Hi, I\u0026rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years. So far, I\u0026rsquo;ve learned and reviewed some papers that are necessary for reinforcement learning research, and some mathematics, like calculus, linear algebra, probability, and so on.\nHowever the bigger one in the picture is me, Tony, and the smaller one is my dog, Potato(He was too young to take a shower when we take the picture, and now he is no more a dirty puppy😀)\nWhy the blogs These blogs here are used to simply explain what I have learned, according to the Feyman Technique. And blogging what I\u0026rsquo;ve just learned is the most important part of learning. The whole process is:\n Choosing a concept or theory I would like to know(collecting necessary materials )  Outlining what prior knowledge of this concept or theory Taking note of this prior knowledge   Try to explain the new theory to readers of the website without any new words and concepts in the theory (draft) Go back to the source and fill in the gap in understanding Simplify the explaining(rewrite and post)  What in blogs These blogs contain:\n Mathematics Neuroscience Algorithms  Deep Learning Reinforcement Learning    And some of these posts might also be represented by videos on my YouTube channel.\n","permalink":"https://anthony-tan.com/about/","summary":"About Me Hi, I\u0026rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years.","title":""}]