[{"content":"Keywords: least squares estimation\nSquares of the difference between the output of a predictor and the target are wildly used loss function especially in regression problems:\n$$ \\ell(\\hat{\\boldsymbol{y}}_i,\\boldsymbol{y}_i)=(\\hat{\\boldsymbol{y}}_i-\\boldsymbol{y}_i)^T(\\hat{\\boldsymbol{y}}_i-\\boldsymbol{y}_i) \\tag{1} $$\nLinear Regression1 Linear regression is a good problem to begin our machine learning career. Not only because of its easiest form and logic but also it\u0026rsquo;s a good basis from which we can extend to other more complicated methods like nonlinear regression and kernel functions. Linear regression has been used for more than 200 years and it is still a good model to solve new problems we might come across nowadays.\nIts form is: $$ y=\\boldsymbol{w}^T\\boldsymbol{x}+\\boldsymbol{b}\\tag{2} $$\nwhere the vectors $\\boldsymbol{x}$ , the number $y$ are input and output respectively, $\\boldsymbol{w}$, $\\boldsymbol{b}$ are the parameter vectors of the model.\nFrom a general view to a machine learning problem, the parameters can be viewed as two different objects:\n An unknown constant A random variable  Thie first opinion comes from the frequentist statistics, while the second one is the basis of Bayesian statistician.\nWhat we do in this post is estimating the parameters by least-squares methods. And the data we used here is the weights of a newborn baby by days from WHO:\n   Day Male(kg) Female(kg)     0 3.5 3.4   15 4.0 3.8   45 4.9 4.5   75 5.7 5.2   105 6.4 5.9   135 7.0 6.4   165 7.6 7.0   195 8.2 7.5   225 8.6 7.9   255 9.1 8.3   285 9.5 8.7   315 9.8 9.0   345 10.2 9.4   375 10.5 9.7    View of algebra Our task is predicting the weights of a newborn baby on a certain day after his birth. From equation (1), (2) and the task, we can get the error of the $i$th training point:\n$$ e_i=(y_i-wx_i-b)^2\\tag{3} $$\nwhere $y_i$ is the target according to the $i$th input $x_i$ from the training set. In our task, the output and target are a real number.\nThen the total error(Notation: loss function is a function of one pair of input and target), sum of squares of the whole training set become:\n$$ e_{\\text{total}} = \\sum_i(y_i-wx_i-b)^2\\tag{4} $$\nOur mission now is to minimize equation (4). Be careful: in the training phase, the unknown variables in equation (4) are $w$ and $b$. Because it is a quadric function, so there exists one and only one minimum point. And the necessary condition of the minimum points is:\n Its gradient should be equal to $0$\n Assuming we have $N$ training points in the sample, so we can calculate its gradient $\\begin{bmatrix}\\frac{\\partial e_{\\text{total}}}{\\partial b} \\ \\frac{\\partial e_{\\text{total}}}{\\partial w} \\end{bmatrix}$:\n$$ \\begin{aligned} \\frac{\\partial e_{\\text{total}}}{\\partial b} \u0026amp;= \\sum_i^N (2b-2(y_i-wx_i))\\ \u0026amp;=2Nb-2\\sum_i^N y_i+2w\\sum_i^Nx_i \\end{aligned}\\tag{5} $$\nand the second component:\n$$ \\begin{aligned} \\frac{\\partial e_{\\text{total}}}{\\partial w}\u0026amp;= \\sum_i^N (2 x_i^2w-2(y_i-b)x_i)\\ \u0026amp;=2w\\sum_i^Nx_i^2-2\\sum_i^Nx_iy_i+2b\\sum_i^Nx_i \\end{aligned}\\tag{6} $$\nwe combine them and set both of them to 0: $$ \\begin{aligned} Nb-\\sum_i^N y_i+w\\sum_i^Nx_i\u0026amp;=0\\ w\\sum_i^Nx_i^2-\\sum_i^Nx_iy_i+b\\sum_i^Nx_i\u0026amp;=0 \\end{aligned}\\tag{7} $$\nto solve these complex equations, we use a little trick here. Summations of $x_i$ , $y_i$, $x_i^2$ and $x_iy_i$ is the obstacle in front of us. We prefer multiplications to summations in equations. So we would like to substitute the summation with multiplications. We have\n$$ N\\bar{x}=\\sum_i^N x_i\\tag{8} $$\nand\n$$ N\\bar{y}=\\sum_i^N y_i\\tag{9} $$\nthen we substitute equation (8) and (9) into equation (7) we get:\n$$ b=\\bar{y}-w\\bar{x}\\tag{10} $$\nfor the first part in equation (7). And substitute equation (10) to the second part in equation (7):\n$$ \\begin{aligned} w\\sum_i^Nx_i^2-\\sum_i^Nx_iy_i+(\\bar{y}-w\\bar{x})\\sum_i^Nx_i\u0026amp;=0\\ w\\sum_i^Nx_i^2-\\sum_i^Nx_iy_i+\\sum_i^Nx_i\\bar{y}-w\\sum_i^N\\bar{x}x_i\u0026amp;=0\\ (w\\sum_i^Nx_i^2-w\\sum_i^N\\bar{x}x_i)-(\\sum_i^Nx_iy_i-\\sum_i^Nx_i\\bar{y})\u0026amp;=0\\ w\\sum_i^Nx_i(x_i-\\bar{x})-\\sum_i^Nx_i(y_i-\\bar{y})\u0026amp;=0\\ w\u0026amp;=\\frac{\\sum_i^Nx_i(y_i-\\bar{y})}{\\sum_i^Nx_i(x_i-\\bar{x})} \\end{aligned}\\tag{11} $$\nThe result of equation (11) can also be written as\n$$ w=\\frac{\\sum_i^N(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_i^N(x_i-\\bar{x})^2}\\tag{12} $$\nsuch as in the book \u0026lsquo;An introduction to statistical learning\u0026rsquo;2. They are equivalent because if we do a little change in the third step of equation (11), it\u0026rsquo;s gonna be like:\n$$ \\begin{aligned} (w\\sum_i^Nx_i^2-w\\sum_i^N\\bar{x}x_i-w\\sum_i^N\\bar{x}x_i+w\\sum_i^N\\bar{x}^2)\u0026amp;-\\(\\sum_i^Nx_iy_i-\\sum_i^Nx_i\\bar{y}-\\sum_i^N\\bar{x}y_i+\\sum_i^N\\bar{x}\\bar{y})\u0026amp;=0\\ w\\sum_i^N(x_i-\\bar{x})(x_i-\\bar{x})\u0026amp;-\\ \\sum_i^N(x_i-\\bar{x})(y_i-\\bar{y})\u0026amp;=0\\ w\u0026amp;=\\frac{\\sum_i^N(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_i^N(x_i-\\bar{x})^2} \\end{aligned}\\tag{13} $$\nfor\n$$ w\\sum_i^N\\bar{x}x_i=w\\sum_i^N\\bar{x}^2\\tag{14} $$\nand $$ \\sum_i^N\\bar{x}y_i=\\sum_i^N\\bar{x}\\bar{y}\\tag{15} $$\nCode of Linear Regression (Algebra Form) import pandas as pds import numpy as np import matplotlib.pyplot as plt data_file = pds.read_csv(\u0026#39;./data/babys_weights_by_months.csv\u0026#39;) data_x = np.array(data_file[\u0026#39;day\u0026#39;]) data_y_male = np.array(data_file[\u0026#39;male\u0026#39;]) data_y_female = np.array(data_file[\u0026#39;female\u0026#39;]) # calculate mean of x and y data_x_bar = np.mean(data_x) data_y_male_bar = np.mean(data_y_male) # calculate w using equation 11 sum_1 = 0 sum_2 = 0 for i in range(len(data_x)): sum_1 += data_x[i]*(data_y_male[i]-data_y_male_bar) sum_2 += data_x[i]*(data_x[i]-data_x_bar) w = sum_1/sum_2 # calculate b using equation 10 b = data_y_male_bar - w* data_x_bar # plot the line day_0 = data_x[0] day_end = data_x[-1] days = np.array([day_0,day_end]) plt.plot(days,days*w+b, c=\u0026#39;r\u0026#39;) plt.scatter(data_file[\u0026#39;day\u0026#39;], data_file[\u0026#39;male\u0026#39;], c=\u0026#39;r\u0026#39;, label=\u0026#39;male\u0026#39;, alpha=0.5) plt.xlabel(\u0026#39;days\u0026#39;) plt.ylabel(\u0026#39;weight(kg)\u0026#39;) plt.legend() plt.show() And its plot is like: whose weight is $0.018442166$ and the intercept is $4.160650577$.\nView of Geometric To such a simple example with just two parameters above, the calculation of parameter could mess us up. However, the practical task always has more parameters, say hundreds or even thousand parameters. It seems impossible for us to solve that.\nNow let\u0026rsquo;s review the linear relation in equation (2) and when we have a training sample consisted of $m$ points : $$ {(\\boldsymbol{x}_1,y_1),(\\boldsymbol{x}_2,y_2),\\dots,(\\boldsymbol{x}_m,y_m)}\\tag{16} $$\nand they are under the same linear relation. Then they can be combined as:\n$$ \\begin{bmatrix} y_1\\ y_2\\ \\vdots\\ y_m \\end{bmatrix}=\\begin{bmatrix} -\u0026amp;\\boldsymbol{x}_1^T\u0026amp;-\\ -\u0026amp;\\boldsymbol{x}_2^T\u0026amp;-\\ \u0026amp;\\vdots\u0026amp;\\ -\u0026amp;\\boldsymbol{x}_m^T\u0026amp;- \\end{bmatrix}\\boldsymbol{w}+I_m\\boldsymbol{b}\\tag{17} $$\nwhere $I_m$ is an identical matirx whose column and row is $m$ and $\\boldsymbol{b}$ is $b$ repeating $m$ times. To make the equation shorter and easy to operate, we can put $b$ into the vectore $\\boldsymbol{w}$ like:\n$$ \\begin{bmatrix} y_1\\ y_2\\ \\vdots\\ y_m \\end{bmatrix}=\\begin{bmatrix} 1\u0026amp;-\u0026amp;\\boldsymbol{x}_1^T\u0026amp;-\\ 1\u0026amp;-\u0026amp;\\boldsymbol{x}_2^T\u0026amp;-\\ 1\u0026amp;\u0026amp;\\vdots\u0026amp;\\ 1\u0026amp;-\u0026amp;\\boldsymbol{x}_m^T\u0026amp;- \\end{bmatrix} \\begin{bmatrix} b\\ \\boldsymbol{w} \\end{bmatrix} \\tag{18} $$\nWe use a simplified equation to represent the relation in equation 18: $$ \\boldsymbol{y} = X\\boldsymbol{w}\\tag{19} $$\nFrom the linear algebra points, equation 19 represents that $\\boldsymbol{y}$ is in the column space of $X$. This is corresponding to the phenomena that all the points of the set (16) stand in a line. When the points are not in a line, the equation (19) does not hold and what we need to do is find a vector $\\boldsymbol{\\hat{y}}$ in the column space which is the closest one to the vector $\\boldsymbol{y}$:\n$$ \\argmin_{\\boldsymbol{\\hat{y}}=X\\boldsymbol{w}} ||\\boldsymbol{y}-\\boldsymbol{\\hat{y}}||\\tag{20} $$\nAnd as we have known, the projection of $\\boldsymbol{y}$ to the column space of $X$ has the shortest distance to $\\boldsymbol{y}$\nOur mission now is to find $\\boldsymbol{w}$ to make:\n$$ \\boldsymbol{\\hat{y}} = X\\boldsymbol{w}\\tag{21} $$\nwhere $\\boldsymbol{\\hat{y}}$ is the projection of $\\boldsymbol{y}$ in the column space of $X$.\nAccording to the projection equation in linear algebra:\n$$ \\boldsymbol{\\hat{y}}=X(X^TX)^{-1}X^T\\boldsymbol{y}\\tag{22} $$\nThen substitute equation (21) into equation (22) and assuming $(X^TX)^{-1}$ exists:\n$$ \\begin{aligned} X\\boldsymbol{w}\u0026amp;=X(X^TX)^{-1}X^T\\boldsymbol{y}\\ X^TX\\boldsymbol{w}\u0026amp;=X^TX(X^TX)^{-1}X^T\\boldsymbol{y}\\ X^TX\\boldsymbol{w}\u0026amp;=X^T\\boldsymbol{y}\\ \\boldsymbol{w}\u0026amp;=(X^TX)^{-1}X^T\\boldsymbol{y} \\end{aligned}\\tag{23} $$\nTo a thin and tall matrix, $X$ which means here the number of sample points in the sample is far more than the dimension of a sample point, $(X^TX)^{-1}$ exists.\nCode of Linear Regression (Matrix Form) import pandas as pds import numpy as np import matplotlib.pyplot as plt class LeastSquaresEstimation(): def __init__(self, method=\u0026#39;OLS\u0026#39;): self.method = method def fit(self, x, y): x = np.array(x).reshape(-1, 1) # add a column which is all 1s to calculate bias of linear function x = np.c_[np.ones(x.size).reshape(-1, 1), x] y = np.array(y).reshape(-1, 1) if self.method == \u0026#39;OLS\u0026#39;: w = np.linalg.inv(x.transpose().dot(x)).dot(x.transpose()).dot(y) b = w[0][0] w = w[1][0] return w, b if __name__ == \u0026#39;__main__\u0026#39;: data_file = pds.read_csv(\u0026#39;./data/babys_weights_by_months.csv\u0026#39;) lse = LeastSquaresEstimation() weight_male, bias_male = lse.fit(data_file[\u0026#39;day\u0026#39;],data_file[\u0026#39;male\u0026#39;]) day_0 = data_file[\u0026#39;day\u0026#39;][0] day_end = list(data_file[\u0026#39;day\u0026#39;])[-1] days = np.array([day_0,day_end]) plt.scatter(data_file[\u0026#39;day\u0026#39;], data_file[\u0026#39;male\u0026#39;], c=\u0026#39;r\u0026#39;, label=\u0026#39;male\u0026#39;, alpha=0.5) plt.scatter(data_file[\u0026#39;day\u0026#39;], data_file[\u0026#39;female\u0026#39;], c=\u0026#39;b\u0026#39;, label=\u0026#39;female\u0026#39;, alpha=0.5) plt.xlabel(\u0026#39;days\u0026#39;) plt.ylabel(\u0026#39;weight(kg)\u0026#39;) plt.legend() plt.show() the entire project can be found at: https://github.com/Tony-Tan/ML and please star me.\nIts output is also like:\nReference   Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://anthony-tan.com/deep_learning/ml-linear-regression-least-squares-estimation/","summary":"Keywords: least squares estimation\nSquares of the difference between the output of a predictor and the target are wildly used loss function especially in regression problems:\n$$ \\ell(\\hat{\\boldsymbol{y}}_i,\\boldsymbol{y}_i)=(\\hat{\\boldsymbol{y}}_i-\\boldsymbol{y}_i)^T(\\hat{\\boldsymbol{y}}_i-\\boldsymbol{y}_i) \\tag{1} $$\nLinear Regression1 Linear regression is a good problem to begin our machine learning career. Not only because of its easiest form and logic but also it\u0026rsquo;s a good basis from which we can extend to other more complicated methods like nonlinear regression and kernel functions.","title":"[Linear Classification] Least Squares Estimation"},{"content":"About Me Hi, I\u0026rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years. So far, I\u0026rsquo;ve learned and reviewed some papers that are necessary for reinforcement learning research, and some mathematics, like calculus, linear algebra, probability, and so on.\nHowever the bigger one in the picture is me, Tony, and the smaller one is my dog, Potato(He was too young to take a shower when we take the picture, and now he is no more a dirty puppy😀)\nWhy the blogs These blogs here are used to simply explain what I have learned, according to the Feyman Technique. And blogging what I\u0026rsquo;ve just learned is the most important part of learning. The whole process is:\n Choosing a concept or theory I would like to know(collecting necessary materials )  Outlining what prior knowledge of this concept or theory Taking note of this prior knowledge   Try to explain the new theory to readers of the website without any new words and concepts in the theory (draft) Go back to the source and fill in the gap in understanding Simplify the explaining(rewrite and post)  What in blogs These blogs contain:\n Mathematics Neuroscience Algorithms  Deep Learning Reinforcement Learning    And some of these posts might also be represented by videos on my YouTube channel.\n","permalink":"https://anthony-tan.com/about/","summary":"About Me Hi, I\u0026rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years.","title":""}]